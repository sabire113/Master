{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae0ac86",
      "metadata": {
        "scrolled": false,
        "id": "2ae0ac86"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load and prepare the data\n",
        "df = pd.read_csv(\"Cleaned_OSEFX_Market_Macro_Data.csv\")\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values(by=['Instrument', 'Date'])\n",
        "\n",
        "# Step 1: Calculate returns, risk-free rate (monthly), and excess returns\n",
        "df['Return'] = df.groupby('Instrument')['ClosePrice'].pct_change()\n",
        "df['RiskFreeRate'] = (1 + df['NorgesBank10Y']) ** (1/12) - 1\n",
        "df['ExcessReturn'] = df['Return'] - df['RiskFreeRate']\n",
        "\n",
        "# Step 2: Lag firm characteristics (excluding identifiers and macro variables)\n",
        "macro_vars = ['BrentOil', 'USDNOK', 'EURNOK', 'US10Y', 'USCPI',\n",
        "              'USGDPGrowth', 'NorwegianCPI', 'NorgesBank10Y']\n",
        "id_vars = ['Date', 'Instrument', 'EconomicSector', 'ExcessReturn']\n",
        "firm_features = [col for col in df.columns if col not in id_vars + macro_vars + ['Return', 'RiskFreeRate']]\n",
        "\n",
        "# Lag all firm-specific features by 1 month per stock\n",
        "for feature in firm_features:\n",
        "    df[feature + '_lag'] = df.groupby('Instrument')[feature].shift(1)\n",
        "\n",
        "# Step 3: Rank-transform firm features to [-1, 1] each month\n",
        "for feature in firm_features:\n",
        "    lagged_feature = feature + '_lag'\n",
        "    df[lagged_feature + '_rank'] = df.groupby('Date')[lagged_feature].transform(\n",
        "        lambda x: 2 * (x.rank(method='first') - 1) / (len(x) - 1) - 1 if len(x) > 1 else 0\n",
        "    )\n",
        "\n",
        "# Step 4: One-hot encode industry (EconomicSector)\n",
        "df = pd.get_dummies(df, columns=['EconomicSector'], prefix='Sector')\n",
        "\n",
        "# Step 4.1: Convert booleans to 0/1 integers (IMPORTANT!)\n",
        "industry_dummies_cols = [col for col in df.columns if col.startswith('Sector_')]\n",
        "df[industry_dummies_cols] = df[industry_dummies_cols].astype(int)\n",
        "\n",
        "\n",
        "# Step 5: Multiply each firm feature by each macro variable to create interaction terms\n",
        "interaction_features = []\n",
        "for feature in firm_features:\n",
        "    f_lag_rank = feature + '_lag_rank'\n",
        "    for macro in macro_vars:\n",
        "        col_name = f\"{feature}_x_{macro}\"\n",
        "        df[col_name] = df[f_lag_rank] * df[macro]\n",
        "        interaction_features.append(col_name)\n",
        "\n",
        "# Step 6: Define final feature set\n",
        "ranked_features = [f + '_lag_rank' for f in firm_features]\n",
        "industry_dummies = [col for col in df.columns if col.startswith('Sector_')]\n",
        "final_features = ranked_features + interaction_features + industry_dummies\n",
        "\n",
        "# Step 7: Drop rows with missing values (from lagging and returns)\n",
        "df_model = df.dropna(subset=final_features + ['ExcessReturn'])\n",
        "\n",
        "# Show the prepared DataFrame structure\n",
        "df_model[['Date', 'Instrument', 'ExcessReturn'] + final_features].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "65qFvNZa5j--"
      },
      "id": "65qFvNZa5j--",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a17a5fd3",
      "metadata": {
        "id": "a17a5fd3"
      },
      "source": [
        "# hyperparameter tuning for GLM with group Lasso using the static split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e324731",
      "metadata": {
        "scrolled": false,
        "id": "4e324731"
      },
      "outputs": [],
      "source": [
        "# Step 8: Create static split for initial hyperparameter tuning\n",
        "# Train: 1995–2003, Validation: 2004–2009\n",
        "\n",
        "df_model['Year'] = df_model['Date'].dt.year\n",
        "\n",
        "train_df = df_model[(df_model['Year'] >= 1995) & (df_model['Year'] <= 2003)]\n",
        "val_df = df_model[(df_model['Year'] >= 2004) & (df_model['Year'] <= 2009)]\n",
        "\n",
        "X_train = train_df[final_features]\n",
        "y_train = train_df['ExcessReturn']\n",
        "X_val = val_df[final_features]\n",
        "y_val = val_df['ExcessReturn']\n",
        "\n",
        "# Show dimensions of the split sets\n",
        "X_train.shape, X_val.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05f960c3",
      "metadata": {
        "scrolled": false,
        "id": "05f960c3"
      },
      "outputs": [],
      "source": [
        "import patsy\n",
        "\n",
        "# Step 9: Build quadratic spline basis for each feature in the static train set\n",
        "# We'll use 3 knots at the 25th, 50th, and 75th percentiles, which mirrors a flexible but controlled spline expansion.\n",
        "\n",
        "# Function to create quadratic spline basis (order=2) with 3 knots\n",
        "def spline_basis(x, knots):\n",
        "    \"\"\"Generates a quadratic spline basis with 3 knots for a single feature.\"\"\"\n",
        "    basis = pd.DataFrame()\n",
        "    basis['x'] = x\n",
        "    basis['x2'] = x**2\n",
        "    for i, knot in enumerate(knots):\n",
        "        basis[f'knot_{i+1}'] = np.maximum(0, (x - knot))**2\n",
        "    return basis\n",
        "\n",
        "# Create spline-expanded features and group IDs\n",
        "X_train_spline = pd.DataFrame(index=X_train.index)\n",
        "X_val_spline = pd.DataFrame(index=X_val.index)\n",
        "group_ids = []\n",
        "group_names = []\n",
        "\n",
        "group_id_counter = 0\n",
        "\n",
        "for feature in X_train.columns:\n",
        "    # Determine knots based on training data\n",
        "    knots = np.percentile(X_train[feature].dropna(), [25, 50, 75])\n",
        "\n",
        "    # Build spline basis for train and val\n",
        "    spline_train = spline_basis(X_train[feature], knots)\n",
        "    spline_val = spline_basis(X_val[feature], knots)\n",
        "\n",
        "    # Assign column names to reflect feature source\n",
        "    spline_train.columns = [f\"{feature}_spline_{col}\" for col in spline_train.columns]\n",
        "    spline_val.columns = spline_train.columns\n",
        "\n",
        "    # Append to final design matrices\n",
        "    X_train_spline = pd.concat([X_train_spline, spline_train], axis=1)\n",
        "    X_val_spline = pd.concat([X_val_spline, spline_val], axis=1)\n",
        "\n",
        "    # Track group ids for group lasso\n",
        "    group_ids.extend([group_id_counter] * spline_train.shape[1])\n",
        "    group_names.append(feature)\n",
        "    group_id_counter += 1\n",
        "\n",
        "# Confirm shape and group structure\n",
        "X_train_spline.shape, X_val_spline.shape, len(group_ids), len(group_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "380662a2",
      "metadata": {
        "scrolled": true,
        "id": "380662a2"
      },
      "outputs": [],
      "source": [
        "! pip install group-lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "843f110e",
      "metadata": {
        "id": "843f110e"
      },
      "outputs": [],
      "source": [
        "from group_lasso import GroupLasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "# Step 10: Standardize features before applying group lasso\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_spline)\n",
        "X_val_scaled = scaler.transform(X_val_spline)\n",
        "\n",
        "# Step 11: Train a group lasso model\n",
        "# We'll scan a small set of lambda (alpha) values to mimic hyperparameter tuning\n",
        "\n",
        "alphas = np.logspace(-4, -1, 5)  # Regularization strengths\n",
        "results = []\n",
        "\n",
        "for alpha in alphas:\n",
        "    model = GroupLasso(\n",
        "        groups=np.array(group_ids),\n",
        "        group_reg=alpha,\n",
        "        l1_reg=0,  # No individual L1 penalty\n",
        "        n_iter=1000,\n",
        "        scale_reg=\"group_size\",\n",
        "        subsampling_scheme=1,\n",
        "        supress_warning=True,\n",
        "        tol=1e-3,\n",
        "        fit_intercept=True,\n",
        "        random_state=0\n",
        "    )\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_val_pred = model.predict(X_val_scaled)\n",
        "    mse_val = mean_squared_error(y_val, y_val_pred)\n",
        "    results.append((alpha, mse_val))\n",
        "\n",
        "# Sort and pick best lambda\n",
        "best_alpha, best_mse = sorted(results, key=lambda x: x[1])[0]\n",
        "best_alpha, best_mse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90807823",
      "metadata": {
        "scrolled": false,
        "id": "90807823"
      },
      "outputs": [],
      "source": [
        "from group_lasso import GroupLasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Clear any previous results\n",
        "predictions = []\n",
        "true_returns = []\n",
        "prediction_dates = []\n",
        "\n",
        "# Define rolling evaluation window\n",
        "start_test_year = 2010  # Based on adjusted split for 30 years of data\n",
        "end_test_year = df_model['Year'].max()\n",
        "\n",
        "# Rolling loop with expanding train (starting at 1995), fixed 6-year validation, 1-year test\n",
        "for test_year in range(start_test_year, end_test_year + 1):\n",
        "    train_end = test_year - 7\n",
        "    val_start = test_year - 6\n",
        "    val_end = test_year - 1\n",
        "\n",
        "    # Subset data\n",
        "    train_df = df_model[df_model['Year'] <= train_end]\n",
        "    val_df = df_model[(df_model['Year'] >= val_start) & (df_model['Year'] <= val_end)]\n",
        "    test_df = df_model[df_model['Year'] == test_year]\n",
        "\n",
        "    if len(train_df) < 100 or len(val_df) < 10 or len(test_df) < 10:\n",
        "        continue\n",
        "\n",
        "    # Features and targets\n",
        "    X_train = train_df[final_features]\n",
        "    y_train = train_df['ExcessReturn']\n",
        "    X_val = val_df[final_features]\n",
        "    y_val = val_df['ExcessReturn']\n",
        "    X_test = test_df[final_features]\n",
        "    y_test = test_df['ExcessReturn']\n",
        "\n",
        "    # Build spline basis\n",
        "    X_train_spline, X_val_spline, X_test_spline = pd.DataFrame(index=X_train.index), pd.DataFrame(index=X_val.index), pd.DataFrame(index=X_test.index)\n",
        "    temp_group_ids = []\n",
        "    group_id_counter = 0\n",
        "\n",
        "    for feature in X_train.columns:\n",
        "        try:\n",
        "            knots = np.percentile(X_train[feature].dropna(), [25, 50, 75])\n",
        "            spline_train = spline_basis(X_train[feature], knots)\n",
        "            spline_val = spline_basis(X_val[feature], knots)\n",
        "            spline_test = spline_basis(X_test[feature], knots)\n",
        "\n",
        "            spline_train.columns = [f\"{feature}_spline_{col}\" for col in spline_train.columns]\n",
        "            spline_val.columns = spline_train.columns\n",
        "            spline_test.columns = spline_train.columns\n",
        "\n",
        "            X_train_spline = pd.concat([X_train_spline, spline_train], axis=1)\n",
        "            X_val_spline = pd.concat([X_val_spline, spline_val], axis=1)\n",
        "            X_test_spline = pd.concat([X_test_spline, spline_test], axis=1)\n",
        "\n",
        "            temp_group_ids.extend([group_id_counter] * spline_train.shape[1])\n",
        "            group_id_counter += 1\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_spline)\n",
        "    X_val_scaled = scaler.transform(X_val_spline)\n",
        "    X_test_scaled = scaler.transform(X_test_spline)\n",
        "\n",
        "    # Refit GLM using best_alpha (found from static tuning earlier)\n",
        "    model = GroupLasso(\n",
        "        groups=np.array(temp_group_ids),\n",
        "        group_reg=best_alpha,\n",
        "        l1_reg=0,\n",
        "        n_iter=1000,\n",
        "        scale_reg=\"group_size\",\n",
        "        subsampling_scheme=1,\n",
        "        supress_warning=True,\n",
        "        tol=1e-3,\n",
        "        fit_intercept=True,\n",
        "        random_state=0\n",
        "    )\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Store results\n",
        "    predictions.extend(y_pred)\n",
        "    true_returns.extend(y_test.values)\n",
        "    prediction_dates.extend(test_df['Date'].values)\n",
        "\n",
        "# Final out-of-sample R²\n",
        "y_true = np.array(true_returns)\n",
        "y_pred = np.array(predictions)\n",
        "oos_r2 = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "oos_r2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e850cf8",
      "metadata": {
        "id": "0e850cf8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(prediction_dates, y_true, label=\"Actual Excess Return\")\n",
        "plt.plot(prediction_dates, y_pred, label=\"Predicted\", alpha=0.7)\n",
        "plt.legend()\n",
        "plt.title(\"GLM Out-of-Sample Predictions (2010–2024)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Excess Return\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd4e34b3",
      "metadata": {
        "id": "cd4e34b3"
      },
      "source": [
        "# GLM+H Replication Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d05405",
      "metadata": {
        "scrolled": false,
        "id": "18d05405",
        "outputId": "68f7f7df-c2aa-4591-ec8d-58498e347172"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/3145007406.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col_name] = df[f_lag_rank] * df[macro]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Instrument</th>\n",
              "      <th>ExcessReturn</th>\n",
              "      <th>ClosePrice_lag_rank</th>\n",
              "      <th>OpenPrice_lag_rank</th>\n",
              "      <th>Volume_lag_rank</th>\n",
              "      <th>BidPrice_lag_rank</th>\n",
              "      <th>AskPrice_lag_rank</th>\n",
              "      <th>DividendYield_lag_rank</th>\n",
              "      <th>BookValuePerShare_lag_rank</th>\n",
              "      <th>...</th>\n",
              "      <th>Sector_Basic Materials</th>\n",
              "      <th>Sector_Consumer Cyclicals</th>\n",
              "      <th>Sector_Consumer Non-Cyclicals</th>\n",
              "      <th>Sector_Energy</th>\n",
              "      <th>Sector_Financials</th>\n",
              "      <th>Sector_Healthcare</th>\n",
              "      <th>Sector_Industrials</th>\n",
              "      <th>Sector_Real Estate</th>\n",
              "      <th>Sector_Technology</th>\n",
              "      <th>Sector_Utilities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20689</th>\n",
              "      <td>2018-01-31</td>\n",
              "      <td>20202.OL</td>\n",
              "      <td>0.120161</td>\n",
              "      <td>-0.106061</td>\n",
              "      <td>-0.106061</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.090909</td>\n",
              "      <td>-0.121212</td>\n",
              "      <td>-0.060606</td>\n",
              "      <td>-0.030303</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20809</th>\n",
              "      <td>2018-02-28</td>\n",
              "      <td>20202.OL</td>\n",
              "      <td>-0.154103</td>\n",
              "      <td>-0.030303</td>\n",
              "      <td>-0.030303</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.075758</td>\n",
              "      <td>-0.030303</td>\n",
              "      <td>-0.075758</td>\n",
              "      <td>-0.015152</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20975</th>\n",
              "      <td>2018-03-31</td>\n",
              "      <td>20202.OL</td>\n",
              "      <td>0.075083</td>\n",
              "      <td>-0.119403</td>\n",
              "      <td>-0.119403</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.089552</td>\n",
              "      <td>-0.074627</td>\n",
              "      <td>-0.044776</td>\n",
              "      <td>-0.014925</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21000</th>\n",
              "      <td>2018-04-30</td>\n",
              "      <td>20202.OL</td>\n",
              "      <td>0.117184</td>\n",
              "      <td>-0.014925</td>\n",
              "      <td>-0.014925</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.014925</td>\n",
              "      <td>0.044776</td>\n",
              "      <td>-0.044776</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21170</th>\n",
              "      <td>2018-05-31</td>\n",
              "      <td>20202.OL</td>\n",
              "      <td>0.040790</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.044776</td>\n",
              "      <td>0.194030</td>\n",
              "      <td>-0.044776</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 211 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date Instrument  ExcessReturn  ClosePrice_lag_rank  \\\n",
              "20689 2018-01-31   20202.OL      0.120161            -0.106061   \n",
              "20809 2018-02-28   20202.OL     -0.154103            -0.030303   \n",
              "20975 2018-03-31   20202.OL      0.075083            -0.119403   \n",
              "21000 2018-04-30   20202.OL      0.117184            -0.014925   \n",
              "21170 2018-05-31   20202.OL      0.040790             0.014925   \n",
              "\n",
              "       OpenPrice_lag_rank  Volume_lag_rank  BidPrice_lag_rank  \\\n",
              "20689           -0.106061             -1.0          -0.090909   \n",
              "20809           -0.030303             -1.0          -0.075758   \n",
              "20975           -0.119403             -1.0          -0.089552   \n",
              "21000           -0.014925             -1.0          -0.014925   \n",
              "21170            0.014925             -1.0           0.044776   \n",
              "\n",
              "       AskPrice_lag_rank  DividendYield_lag_rank  BookValuePerShare_lag_rank  \\\n",
              "20689          -0.121212               -0.060606                   -0.030303   \n",
              "20809          -0.030303               -0.075758                   -0.015152   \n",
              "20975          -0.074627               -0.044776                   -0.014925   \n",
              "21000           0.044776               -0.044776                    0.000000   \n",
              "21170           0.194030               -0.044776                    0.000000   \n",
              "\n",
              "       ...  Sector_Basic Materials  Sector_Consumer Cyclicals  \\\n",
              "20689  ...                       0                          0   \n",
              "20809  ...                       0                          0   \n",
              "20975  ...                       0                          0   \n",
              "21000  ...                       0                          0   \n",
              "21170  ...                       0                          0   \n",
              "\n",
              "       Sector_Consumer Non-Cyclicals  Sector_Energy  Sector_Financials  \\\n",
              "20689                              0              0                  0   \n",
              "20809                              0              0                  0   \n",
              "20975                              0              0                  0   \n",
              "21000                              0              0                  0   \n",
              "21170                              0              0                  0   \n",
              "\n",
              "       Sector_Healthcare  Sector_Industrials  Sector_Real Estate  \\\n",
              "20689                  0                   1                   0   \n",
              "20809                  0                   1                   0   \n",
              "20975                  0                   1                   0   \n",
              "21000                  0                   1                   0   \n",
              "21170                  0                   1                   0   \n",
              "\n",
              "       Sector_Technology  Sector_Utilities  \n",
              "20689                  0                 0  \n",
              "20809                  0                 0  \n",
              "20975                  0                 0  \n",
              "21000                  0                 0  \n",
              "21170                  0                 0  \n",
              "\n",
              "[5 rows x 211 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load and prepare the data\n",
        "df = pd.read_csv(\"Cleaned_OSEFX_Market_Macro_Data.csv\")\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values(by=['Instrument', 'Date'])\n",
        "\n",
        "# Step 1: Calculate returns, risk-free rate (monthly), and excess returns\n",
        "df['Return'] = df.groupby('Instrument')['ClosePrice'].pct_change()\n",
        "df['RiskFreeRate'] = (1 + df['NorgesBank10Y']) ** (1/12) - 1\n",
        "df['ExcessReturn'] = df['Return'] - df['RiskFreeRate']\n",
        "\n",
        "# Step 2: Lag firm characteristics (excluding identifiers and macro variables)\n",
        "macro_vars = ['BrentOil', 'USDNOK', 'EURNOK', 'US10Y', 'USCPI',\n",
        "              'USGDPGrowth', 'NorwegianCPI', 'NorgesBank10Y']\n",
        "id_vars = ['Date', 'Instrument', 'EconomicSector', 'ExcessReturn']\n",
        "firm_features = [col for col in df.columns if col not in id_vars + macro_vars + ['Return', 'RiskFreeRate']]\n",
        "\n",
        "# Lag all firm-specific features by 1 month per stock\n",
        "for feature in firm_features:\n",
        "    df[feature + '_lag'] = df.groupby('Instrument')[feature].shift(1)\n",
        "\n",
        "# Step 3: Rank-transform firm features to [-1, 1] each month\n",
        "for feature in firm_features:\n",
        "    lagged_feature = feature + '_lag'\n",
        "    df[lagged_feature + '_rank'] = df.groupby('Date')[lagged_feature].transform(\n",
        "        lambda x: 2 * (x.rank(method='first') - 1) / (len(x) - 1) - 1 if len(x) > 1 else 0\n",
        "    )\n",
        "\n",
        "# Step 4: One-hot encode industry (EconomicSector)\n",
        "df = pd.get_dummies(df, columns=['EconomicSector'], prefix='Sector')\n",
        "\n",
        "# Step 4.1: Convert booleans to 0/1 integers (IMPORTANT!)\n",
        "industry_dummies_cols = [col for col in df.columns if col.startswith('Sector_')]\n",
        "df[industry_dummies_cols] = df[industry_dummies_cols].astype(int)\n",
        "\n",
        "\n",
        "# Step 5: Multiply each firm feature by each macro variable to create interaction terms\n",
        "interaction_features = []\n",
        "for feature in firm_features:\n",
        "    f_lag_rank = feature + '_lag_rank'\n",
        "    for macro in macro_vars:\n",
        "        col_name = f\"{feature}_x_{macro}\"\n",
        "        df[col_name] = df[f_lag_rank] * df[macro]\n",
        "        interaction_features.append(col_name)\n",
        "\n",
        "# Step 6: Define final feature set\n",
        "ranked_features = [f + '_lag_rank' for f in firm_features]\n",
        "industry_dummies = [col for col in df.columns if col.startswith('Sector_')]\n",
        "final_features = ranked_features + interaction_features + industry_dummies\n",
        "\n",
        "# Step 7: Drop rows with missing values (from lagging and returns)\n",
        "df_model = df.dropna(subset=final_features + ['ExcessReturn'])\n",
        "\n",
        "# Show the prepared DataFrame structure\n",
        "df_model[['Date', 'Instrument', 'ExcessReturn'] + final_features].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1eb2b56",
      "metadata": {
        "id": "a1eb2b56",
        "outputId": "0ce10596-407d-415b-f0ba-515ca5134651"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/302544595.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_model['Year'] = df_model['Date'].dt.year\n",
            "/var/folders/qy/dzbss6wj77xg01cwhw5q2kn80000gn/T/ipykernel_1660/302544595.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_model['Year'] = df_model['Date'].dt.year\n"
          ]
        }
      ],
      "source": [
        "df_model['Year'] = df_model['Date'].dt.year\n",
        "\n",
        "# Static split: train = 1995–2003, validation = 2004–2009\n",
        "train_df = df_model[(df_model['Year'] >= 1995) & (df_model['Year'] <= 2003)]\n",
        "val_df = df_model[(df_model['Year'] >= 2004) & (df_model['Year'] <= 2009)]\n",
        "\n",
        "X_train = train_df[final_features]\n",
        "y_train = train_df['ExcessReturn']\n",
        "X_val = val_df[final_features]\n",
        "y_val = val_df['ExcessReturn']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a59c40",
      "metadata": {
        "id": "68a59c40"
      },
      "source": [
        "###  STEP 2 – Build Spline Basis and Group IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e08dfcfa",
      "metadata": {
        "id": "e08dfcfa"
      },
      "outputs": [],
      "source": [
        "def build_spline_basis(X):\n",
        "    X_spline = pd.DataFrame(index=X.index)\n",
        "    group_ids = []\n",
        "    group_counter = 0\n",
        "    for feature in X.columns:\n",
        "        try:\n",
        "            knots = np.percentile(X[feature].dropna(), [25, 50, 75])\n",
        "            x = X[feature]\n",
        "            basis = pd.DataFrame({\n",
        "                f\"{feature}_x\": x,\n",
        "                f\"{feature}_x2\": x**2,\n",
        "                f\"{feature}_knot1\": np.maximum(0, (x - knots[0])**2),\n",
        "                f\"{feature}_knot2\": np.maximum(0, (x - knots[1])**2),\n",
        "                f\"{feature}_knot3\": np.maximum(0, (x - knots[2])**2),\n",
        "            }, index=X.index)\n",
        "            X_spline = pd.concat([X_spline, basis], axis=1)\n",
        "            group_ids.extend([group_counter] * 5)\n",
        "            group_counter += 1\n",
        "        except:\n",
        "            continue\n",
        "    return X_spline, group_ids\n",
        "\n",
        "X_train_spline, group_ids = build_spline_basis(X_train)\n",
        "X_val_spline, _ = build_spline_basis(X_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037bfcd9",
      "metadata": {
        "id": "037bfcd9"
      },
      "source": [
        "### STEP 3 – Hyperparameter Tuning with Huber Loss (GLM+H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f502497f",
      "metadata": {
        "id": "f502497f"
      },
      "outputs": [],
      "source": [
        "! pip install cvxpy osqp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98bb9ce7",
      "metadata": {
        "id": "98bb9ce7"
      },
      "outputs": [],
      "source": [
        "import cvxpy as cp\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def tune_glm_huber(X_train, y_train, X_val, y_val, group_ids, lambda_grid, delta=1.0):\n",
        "    best_lambda = None\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_train_std = scaler.fit_transform(X_train)\n",
        "    X_val_std = scaler.transform(X_val)\n",
        "    y_train = y_train.values\n",
        "    y_val = y_val.values\n",
        "\n",
        "    print(\"Tuning λ using Huber loss...\")\n",
        "\n",
        "    for lam in lambda_grid:\n",
        "        beta = cp.Variable(X_train.shape[1])\n",
        "        intercept = cp.Variable()\n",
        "        residuals = y_train - X_train_std @ beta - intercept\n",
        "        huber_loss = cp.sum(cp.huber(residuals, M=delta))\n",
        "\n",
        "        group_penalty = 0\n",
        "        for g in np.unique(group_ids):\n",
        "            idx = np.where(np.array(group_ids) == g)[0]\n",
        "            if len(idx) > 0:\n",
        "                group_penalty += cp.norm2(beta[idx])\n",
        "\n",
        "        problem = cp.Problem(cp.Minimize(huber_loss + lam * group_penalty))\n",
        "\n",
        "        try:\n",
        "            problem.solve(solver=cp.OSQP, verbose=False)\n",
        "            if beta.value is None or intercept.value is None:\n",
        "                raise ValueError(\"Solution not found\")\n",
        "\n",
        "            y_val_pred = X_val_std @ beta.value + intercept.value\n",
        "            val_loss = np.mean(np.where(\n",
        "                np.abs(y_val - y_val_pred) <= delta,\n",
        "                0.5 * (y_val - y_val_pred)**2,\n",
        "                delta * (np.abs(y_val - y_val_pred) - 0.5 * delta)\n",
        "            ))\n",
        "\n",
        "            print(f\"λ = {lam:.5f} → Val Huber Loss = {val_loss:.6f}\")\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_lambda = lam\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Failed for λ = {lam:.5f}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if best_lambda is None:\n",
        "        print(\"❌ No valid lambda found during tuning! Returning default λ = 0.01\")\n",
        "        best_lambda = 0.01\n",
        "\n",
        "    print(f\"✅ Best lambda (Huber): {best_lambda:.5f}\")\n",
        "    return best_lambda\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3cf2cb",
      "metadata": {
        "id": "7a3cf2cb"
      },
      "source": [
        "### STEP 4 – Rolling Expanding Window Evaluation with GLM+H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7371fc8d",
      "metadata": {
        "id": "7371fc8d",
        "outputId": "67fc4eeb-8792-4743-d9f5-addb025f0823"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'best_lambda_h' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(idx) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     42\u001b[0m         group_penalty \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mnorm2(beta[idx])\n\u001b[0;32m---> 44\u001b[0m problem \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mProblem(cp\u001b[38;5;241m.\u001b[39mMinimize(huber_loss \u001b[38;5;241m+\u001b[39m \u001b[43mbest_lambda_h\u001b[49m \u001b[38;5;241m*\u001b[39m group_penalty))\n\u001b[1;32m     45\u001b[0m problem\u001b[38;5;241m.\u001b[39msolve(solver\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39mOSQP)\n\u001b[1;32m     47\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m X_test_scaled \u001b[38;5;241m@\u001b[39m beta\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m+\u001b[39m intercept\u001b[38;5;241m.\u001b[39mvalue\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_lambda_h' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "predictions, true_returns, prediction_dates = [], [], []\n",
        "\n",
        "for test_year in range(2010, df_model['Year'].max() + 1):\n",
        "    train_end = test_year - 7\n",
        "    val_start = test_year - 6\n",
        "    val_end = test_year - 1\n",
        "\n",
        "    train_df = df_model[df_model['Year'] <= train_end]\n",
        "    val_df = df_model[(df_model['Year'] >= val_start) & (df_model['Year'] <= val_end)]\n",
        "    test_df = df_model[df_model['Year'] == test_year]\n",
        "\n",
        "    if len(train_df) < 100 or len(val_df) < 10 or len(test_df) < 10:\n",
        "        continue\n",
        "\n",
        "    X_train = train_df[final_features]\n",
        "    y_train = train_df['ExcessReturn']\n",
        "    X_val = val_df[final_features]\n",
        "    y_val = val_df['ExcessReturn']\n",
        "    X_test = test_df[final_features]\n",
        "    y_test = test_df['ExcessReturn']\n",
        "\n",
        "    X_train_spline, group_ids = build_spline_basis(X_train)\n",
        "    X_val_spline, _ = build_spline_basis(X_val)\n",
        "    X_test_spline, _ = build_spline_basis(X_test)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_spline)\n",
        "    X_val_scaled = scaler.transform(X_val_spline)\n",
        "    X_test_scaled = scaler.transform(X_test_spline)\n",
        "\n",
        "    beta = cp.Variable(X_train_scaled.shape[1])\n",
        "    intercept = cp.Variable()\n",
        "    residuals = y_train.values - X_train_scaled @ beta - intercept\n",
        "    huber_loss = cp.sum(cp.huber(residuals, M=1.0))\n",
        "\n",
        "    group_penalty = 0\n",
        "    for g in np.unique(group_ids):\n",
        "        idx = np.where(np.array(group_ids) == g)[0]\n",
        "        if len(idx) > 0:\n",
        "            group_penalty += cp.norm2(beta[idx])\n",
        "\n",
        "    problem = cp.Problem(cp.Minimize(huber_loss + best_lambda_h * group_penalty))\n",
        "    problem.solve(solver=cp.OSQP)\n",
        "\n",
        "    y_pred = X_test_scaled @ beta.value + intercept.value\n",
        "    predictions.extend(y_pred)\n",
        "    true_returns.extend(y_test.values)\n",
        "    prediction_dates.extend(test_df['Date'].values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57137b58",
      "metadata": {
        "id": "57137b58"
      },
      "outputs": [],
      "source": [
        "### STEP 5 – Evaluate Out-of-Sample R²"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b91709b",
      "metadata": {
        "id": "5b91709b"
      },
      "outputs": [],
      "source": [
        "y_true = np.array(true_returns)\n",
        "y_pred = np.array(predictions)\n",
        "oos_r2 = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "print(f\"Out-of-sample R² (GLM+H): {oos_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e04ca43",
      "metadata": {
        "id": "6e04ca43",
        "outputId": "a75e6486-3632-4deb-fd13-728fa74539b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.01"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Re-import everything after code state reset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"Cleaned_OSEFX_Market_Macro_Data.csv\")\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values(by=['Instrument', 'Date'])\n",
        "\n",
        "# Step 1: Compute monthly return and excess return\n",
        "df['Return'] = df.groupby('Instrument')['ClosePrice'].pct_change()\n",
        "df['RiskFreeRate'] = (1 + df['NorgesBank10Y']) ** (1/12) - 1\n",
        "df['ExcessReturn'] = df['Return'] - df['RiskFreeRate']\n",
        "\n",
        "# Step 2: Lag firm features\n",
        "macro_vars = ['BrentOil', 'USDNOK', 'EURNOK', 'US10Y', 'USCPI', 'USGDPGrowth', 'NorwegianCPI', 'NorgesBank10Y']\n",
        "id_vars = ['Date', 'Instrument', 'EconomicSector', 'ExcessReturn']\n",
        "firm_features = [col for col in df.columns if col not in id_vars + macro_vars + ['Return', 'RiskFreeRate']]\n",
        "\n",
        "for feature in firm_features:\n",
        "    df[feature + '_lag'] = df.groupby('Instrument')[feature].shift(1)\n",
        "\n",
        "# Step 3: Rank-transform lagged features to [-1, 1]\n",
        "for feature in firm_features:\n",
        "    lagged_feature = feature + '_lag'\n",
        "    df[lagged_feature + '_rank'] = df.groupby('Date')[lagged_feature].transform(\n",
        "        lambda x: 2 * (x.rank(method='first') - 1) / (len(x) - 1) - 1 if len(x) > 1 else 0\n",
        "    )\n",
        "\n",
        "# Step 4: One-hot encode sectors and convert bools to int\n",
        "df = pd.get_dummies(df, columns=['EconomicSector'], prefix='Sector')\n",
        "sector_cols = [col for col in df.columns if col.startswith('Sector_')]\n",
        "df[sector_cols] = df[sector_cols].astype(int)\n",
        "\n",
        "# Step 5: Interact macro vars with firm features\n",
        "interaction_features = []\n",
        "for feature in firm_features:\n",
        "    f_lag_rank = feature + '_lag_rank'\n",
        "    for macro in macro_vars:\n",
        "        col_name = f\"{feature}_x_{macro}\"\n",
        "        df[col_name] = df[f_lag_rank] * df[macro]\n",
        "        interaction_features.append(col_name)\n",
        "\n",
        "# Step 6: Final features\n",
        "ranked_features = [f + '_lag_rank' for f in firm_features]\n",
        "final_features = ranked_features + interaction_features + sector_cols\n",
        "\n",
        "# Step 7: Drop NA and extract Year\n",
        "df_model = df.dropna(subset=final_features + ['ExcessReturn']).copy()\n",
        "df_model['Year'] = df_model['Date'].dt.year\n",
        "\n",
        "# Step 8: Static split for lambda tuning\n",
        "train_df = df_model[(df_model['Year'] >= 1995) & (df_model['Year'] <= 2003)]\n",
        "val_df = df_model[(df_model['Year'] >= 2004) & (df_model['Year'] <= 2009)]\n",
        "\n",
        "X_train = train_df[final_features]\n",
        "y_train = train_df['ExcessReturn']\n",
        "X_val = val_df[final_features]\n",
        "y_val = val_df['ExcessReturn']\n",
        "\n",
        "# Step 9: Build spline basis and group ids\n",
        "def build_spline_basis(X):\n",
        "    X_spline = pd.DataFrame(index=X.index)\n",
        "    group_ids = []\n",
        "    group_counter = 0\n",
        "    for feature in X.columns:\n",
        "        try:\n",
        "            knots = np.percentile(X[feature].dropna(), [25, 50, 75])\n",
        "            x = X[feature]\n",
        "            basis = pd.DataFrame({\n",
        "                f\"{feature}_x\": x,\n",
        "                f\"{feature}_x2\": x**2,\n",
        "                f\"{feature}_knot1\": np.maximum(0, (x - knots[0])**2),\n",
        "                f\"{feature}_knot2\": np.maximum(0, (x - knots[1])**2),\n",
        "                f\"{feature}_knot3\": np.maximum(0, (x - knots[2])**2),\n",
        "            }, index=X.index)\n",
        "            X_spline = pd.concat([X_spline, basis], axis=1)\n",
        "            group_ids.extend([group_counter] * 5)\n",
        "            group_counter += 1\n",
        "        except:\n",
        "            continue\n",
        "    return X_spline, group_ids\n",
        "\n",
        "X_train_spline, group_ids = build_spline_basis(X_train)\n",
        "X_val_spline, _ = build_spline_basis(X_val)\n",
        "\n",
        "# Step 10: Lambda tuning using Huber loss\n",
        "def tune_glm_huber(X_train, y_train, X_val, y_val, group_ids, lambda_grid, delta=1.0):\n",
        "    best_lambda = None\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_std = scaler.fit_transform(X_train)\n",
        "    X_val_std = scaler.transform(X_val)\n",
        "    y_train = y_train.values\n",
        "    y_val = y_val.values\n",
        "\n",
        "    for lam in lambda_grid:\n",
        "        beta = cp.Variable(X_train.shape[1])\n",
        "        intercept = cp.Variable()\n",
        "        residuals = y_train - X_train_std @ beta - intercept\n",
        "        huber_loss = cp.sum(cp.huber(residuals, M=delta))\n",
        "\n",
        "        group_penalty = 0\n",
        "        for g in np.unique(group_ids):\n",
        "            idx = np.where(np.array(group_ids) == g)[0]\n",
        "            if len(idx) > 0:\n",
        "                group_penalty += cp.norm2(beta[idx])\n",
        "\n",
        "        problem = cp.Problem(cp.Minimize(huber_loss + lam * group_penalty))\n",
        "        try:\n",
        "            problem.solve(solver=cp.OSQP)\n",
        "            if beta.value is None or intercept.value is None:\n",
        "                continue\n",
        "            y_val_pred = X_val_std @ beta.value + intercept.value\n",
        "            val_loss = np.mean(np.where(\n",
        "                np.abs(y_val - y_val_pred) <= delta,\n",
        "                0.5 * (y_val - y_val_pred)**2,\n",
        "                delta * (np.abs(y_val - y_val_pred) - 0.5 * delta)\n",
        "            ))\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_lambda = lam\n",
        "        except:\n",
        "            continue\n",
        "    return best_lambda if best_lambda is not None else 0.01\n",
        "\n",
        "# Tune best lambda\n",
        "lambda_grid = np.logspace(-4, -1, 5)\n",
        "best_lambda_h = tune_glm_huber(X_train_spline, y_train, X_val_spline, y_val, group_ids, lambda_grid)\n",
        "best_lambda_h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "999f381a",
      "metadata": {
        "id": "999f381a"
      },
      "outputs": [],
      "source": [
        "# Step 11: Rolling out-of-sample prediction using GLM+H with fixed best_lambda_h\n",
        "\n",
        "predictions, true_returns, prediction_dates = [], [], []\n",
        "\n",
        "for test_year in range(2010, df_model['Year'].max() + 1):\n",
        "    train_end = test_year - 7\n",
        "    val_start = test_year - 6\n",
        "    val_end = test_year - 1\n",
        "\n",
        "    train_df = df_model[df_model['Year'] <= train_end]\n",
        "    val_df = df_model[(df_model['Year'] >= val_start) & (df_model['Year'] <= val_end)]\n",
        "    test_df = df_model[df_model['Year'] == test_year]\n",
        "\n",
        "    if len(train_df) < 100 or len(val_df) < 10 or len(test_df) < 10:\n",
        "        continue\n",
        "\n",
        "    X_train = train_df[final_features]\n",
        "    y_train = train_df['ExcessReturn']\n",
        "    X_val = val_df[final_features]\n",
        "    y_val = val_df['ExcessReturn']\n",
        "    X_test = test_df[final_features]\n",
        "    y_test = test_df['ExcessReturn']\n",
        "\n",
        "    X_train_spline, group_ids = build_spline_basis(X_train)\n",
        "    X_val_spline, _ = build_spline_basis(X_val)\n",
        "    X_test_spline, _ = build_spline_basis(X_test)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_spline)\n",
        "    X_val_scaled = scaler.transform(X_val_spline)\n",
        "    X_test_scaled = scaler.transform(X_test_spline)\n",
        "\n",
        "    beta = cp.Variable(X_train_scaled.shape[1])\n",
        "    intercept = cp.Variable()\n",
        "    residuals = y_train.values - X_train_scaled @ beta - intercept\n",
        "    huber_loss = cp.sum(cp.huber(residuals, M=1.0))\n",
        "\n",
        "    group_penalty = 0\n",
        "    for g in np.unique(group_ids):\n",
        "        idx = np.where(np.array(group_ids) == g)[0]\n",
        "        if len(idx) > 0:\n",
        "            group_penalty += cp.norm2(beta[idx])\n",
        "\n",
        "    problem = cp.Problem(cp.Minimize(huber_loss + best_lambda_h * group_penalty))\n",
        "\n",
        "    try:\n",
        "        problem.solve(solver=cp.OSQP)\n",
        "        if beta.value is not None and intercept.value is not None:\n",
        "            y_pred = X_test_scaled @ beta.value + intercept.value\n",
        "            predictions.extend(y_pred)\n",
        "            true_returns.extend(y_test.values)\n",
        "            prediction_dates.extend(test_df['Date'].values)\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "# Step 12: Evaluate R² and plot predictions\n",
        "y_true = np.array(true_returns)\n",
        "y_pred = np.array(predictions)\n",
        "oos_r2 = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "\n",
        "# Plot predictions\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(prediction_dates, y_true, label=\"Actual Excess Return\")\n",
        "plt.plot(prediction_dates, y_pred, label=\"Predicted\", alpha=0.8)\n",
        "plt.title(\"GLM+H Out-of-Sample Predictions (2010–2024)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Excess Return\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "oos_r2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Upload the file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "0U4wyA4-8SU6",
        "outputId": "349af05c-c677-481b-d774-76dba5130d89"
      },
      "id": "0U4wyA4-8SU6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8d11e3a1-1613-4046-8459-e789918cb32a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8d11e3a1-1613-4046-8459-e789918cb32a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ML_Standardized_Ready_OSEFX.csv to ML_Standardized_Ready_OSEFX.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cvxpy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import cvxpy as cp\n",
        "\n",
        "# Load standardized ML-ready data\n",
        "df = pd.read_csv(\"ML_Standardized_Ready_OSEFX.csv\")\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "# Extract feature columns (exclude 'Target', 'Year', etc.)\n",
        "feature_cols = [col for col in df.columns if col not in [\"Target\", \"Year\"]]\n",
        "\n",
        "# Static split based on the paper\n",
        "train_df = df[(df[\"Year\"] >= 1995) & (df[\"Year\"] <= 2003)]\n",
        "val_df = df[(df[\"Year\"] >= 2004) & (df[\"Year\"] <= 2009)]\n",
        "\n",
        "X_train = train_df[feature_cols]\n",
        "y_train = train_df[\"Target\"]\n",
        "X_val = val_df[feature_cols]\n",
        "y_val = val_df[\"Target\"]\n",
        "\n",
        "# Build spline basis (GLM+H paper design)\n",
        "def build_spline_basis(X):\n",
        "    X_spline = pd.DataFrame(index=X.index)\n",
        "    group_ids = []\n",
        "    group_counter = 0\n",
        "    for feature in X.columns:\n",
        "        try:\n",
        "            knots = np.percentile(X[feature].dropna(), [25, 50, 75])\n",
        "            x = X[feature]\n",
        "            basis = pd.DataFrame({\n",
        "                f\"{feature}_x\": x,\n",
        "                f\"{feature}_x2\": x**2,\n",
        "                f\"{feature}_knot1\": np.maximum(0, (x - knots[0])**2),\n",
        "                f\"{feature}_knot2\": np.maximum(0, (x - knots[1])**2),\n",
        "                f\"{feature}_knot3\": np.maximum(0, (x - knots[2])**2),\n",
        "            }, index=X.index)\n",
        "            X_spline = pd.concat([X_spline, basis], axis=1)\n",
        "            group_ids.extend([group_counter] * 5)\n",
        "            group_counter += 1\n",
        "        except:\n",
        "            continue\n",
        "    return X_spline, group_ids\n",
        "\n",
        "X_train_spline, group_ids = build_spline_basis(X_train)\n",
        "X_val_spline, _ = build_spline_basis(X_val)\n",
        "\n",
        "# Tune GLM + H (Huber loss)\n",
        "def tune_glm_huber(X_train, y_train, X_val, y_val, group_ids, lambda_grid, delta=1.0):\n",
        "    best_lambda = None\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_std = scaler.fit_transform(X_train)\n",
        "    X_val_std = scaler.transform(X_val)\n",
        "    y_train = y_train.values\n",
        "    y_val = y_val.values\n",
        "\n",
        "    for lam in lambda_grid:\n",
        "        beta = cp.Variable(X_train.shape[1])\n",
        "        intercept = cp.Variable()\n",
        "        residuals = y_train - X_train_std @ beta - intercept\n",
        "        huber_loss = cp.sum(cp.huber(residuals, M=delta))\n",
        "\n",
        "        group_penalty = 0\n",
        "        for g in np.unique(group_ids):\n",
        "            idx = np.where(np.array(group_ids) == g)[0]\n",
        "            if len(idx) > 0:\n",
        "                group_penalty += cp.norm2(beta[idx])\n",
        "\n",
        "        problem = cp.Problem(cp.Minimize(huber_loss + lam * group_penalty))\n",
        "        try:\n",
        "            problem.solve(solver=cp.OSQP)\n",
        "            if beta.value is None or intercept.value is None:\n",
        "                continue\n",
        "            y_val_pred = X_val_std @ beta.value + intercept.value\n",
        "            val_loss = np.mean(np.where(\n",
        "                np.abs(y_val - y_val_pred) <= delta,\n",
        "                0.5 * (y_val - y_val_pred)**2,\n",
        "                delta * (np.abs(y_val - y_val_pred) - 0.5 * delta)\n",
        "            ))\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_lambda = lam\n",
        "        except:\n",
        "            continue\n",
        "    return best_lambda if best_lambda is not None else 0.01\n",
        "\n",
        "# Define lambda grid and tune\n",
        "lambda_grid = np.logspace(-4, -1, 5)\n",
        "best_lambda_h = tune_glm_huber(X_train_spline, y_train, X_val_spline, y_val, group_ids, lambda_grid)\n",
        "best_lambda_h\n"
      ],
      "metadata": {
        "id": "GajZ1SHf5lSg"
      },
      "id": "GajZ1SHf5lSg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "import warnings\n",
        "\n",
        "# Suppress solver warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load standardized dataset\n",
        "df = pd.read_csv(\"ML_Standardized_Ready_OSEFX.csv\")\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df = df.sort_values(by=[\"Instrument\", \"Date\"])\n",
        "\n",
        "# Static training/validation split for lambda tuning (1995–2003 and 2004–2009)\n",
        "train_df = df[(df[\"Year\"] >= 1995) & (df[\"Year\"] <= 2003)]\n",
        "val_df = df[(df[\"Year\"] >= 2004) & (df[\"Year\"] <= 2009)]\n",
        "\n",
        "# Exclude metadata columns\n",
        "exclude_cols = [\"Instrument\", \"Date\", \"Year\", \"Target\"]\n",
        "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train_df[feature_cols]\n",
        "y_train = train_df[\"Target\"]\n",
        "X_val = val_df[feature_cols]\n",
        "y_val = val_df[\"Target\"]\n",
        "\n",
        "# Function to build spline basis and group IDs\n",
        "def build_spline_basis(X):\n",
        "    X_spline = pd.DataFrame(index=X.index)\n",
        "    group_ids = []\n",
        "    group_counter = 0\n",
        "    for feature in X.columns:\n",
        "        try:\n",
        "            knots = np.percentile(X[feature].dropna(), [25, 50, 75])\n",
        "            x = X[feature]\n",
        "            basis = pd.DataFrame({\n",
        "                f\"{feature}_x\": x,\n",
        "                f\"{feature}_x2\": x**2,\n",
        "                f\"{feature}_knot1\": np.maximum(0, (x - knots[0])**2),\n",
        "                f\"{feature}_knot2\": np.maximum(0, (x - knots[1])**2),\n",
        "                f\"{feature}_knot3\": np.maximum(0, (x - knots[2])**2),\n",
        "            }, index=X.index)\n",
        "            X_spline = pd.concat([X_spline, basis], axis=1)\n",
        "            group_ids.extend([group_counter] * 5)\n",
        "            group_counter += 1\n",
        "        except:\n",
        "            continue\n",
        "    return X_spline, group_ids\n",
        "\n",
        "# Build spline features and group ids\n",
        "X_train_spline, group_ids = build_spline_basis(X_train)\n",
        "X_val_spline, _ = build_spline_basis(X_val)\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train_spline)\n",
        "X_val_std = scaler.transform(X_val_spline)\n",
        "y_train = y_train.values\n",
        "y_val = y_val.values\n",
        "\n",
        "# Lambda grid\n",
        "lambda_grid = np.logspace(-4, -1, 5)\n",
        "\n",
        "# Fit GLM + Huber (GLM+H) with group lasso penalty\n",
        "best_lambda = None\n",
        "best_val_loss = float(\"inf\")\n",
        "best_beta = None\n",
        "best_intercept = None\n",
        "delta = 1.0  # Huber loss threshold\n",
        "\n",
        "for lam in lambda_grid:\n",
        "    beta = cp.Variable(X_train_std.shape[1])\n",
        "    intercept = cp.Variable()\n",
        "    residuals = y_train - X_train_std @ beta - intercept\n",
        "    huber_loss = cp.sum(cp.huber(residuals, M=delta))\n",
        "\n",
        "    group_penalty = 0\n",
        "    for g in np.unique(group_ids):\n",
        "        idx = np.where(np.array(group_ids) == g)[0]\n",
        "        if len(idx) > 0:\n",
        "            group_penalty += cp.norm2(beta[idx])\n",
        "\n",
        "    problem = cp.Problem(cp.Minimize(huber_loss + lam * group_penalty))\n",
        "    try:\n",
        "        problem.solve(solver=cp.OSQP)\n",
        "        if beta.value is not None and intercept.value is not None:\n",
        "            y_val_pred = X_val_std @ beta.value + intercept.value\n",
        "            val_loss = np.mean(np.where(\n",
        "                np.abs(y_val - y_val_pred) <= delta,\n",
        "                0.5 * (y_val - y_val_pred)**2,\n",
        "                delta * (np.abs(y_val - y_val_pred) - 0.5 * delta)\n",
        "            ))\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_lambda = lam\n",
        "                best_beta = beta.value\n",
        "                best_intercept = intercept.value\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Final R^2 on validation set for GLM+H\n",
        "y_val_pred_final = X_val_std @ best_beta + best_intercept\n",
        "glm_h_r2 = r2_score(y_val, y_val_pred_final)\n",
        "\n",
        "glm_h_r2, best_lambda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "HbRVkEa-5mba",
        "outputId": "ad68ad7d-872b-47f6-af18-e7c1d0ce23ef"
      },
      "id": "HbRVkEa-5mba",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d98fa420acf4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# Final R^2 on validation set for GLM+H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0my_val_pred_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val_std\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mbest_beta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbest_intercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0mglm_h_r2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_pred_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cvxpy\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import cvxpy as cp\n",
        "\n",
        "# Load standardized ML-ready data\n",
        "df = pd.read_csv(\"ML_Standardized_Ready_OSEFX.csv\")\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "# Extract feature columns (exclude 'Target', 'Year', etc.)\n",
        "feature_cols = [col for col in df.columns if col not in [\"Target\", \"Year\"]]\n",
        "\n",
        "# Static split based on the paper\n",
        "train_df = df[(df[\"Year\"] >= 1995) & (df[\"Year\"] <= 2003)]\n",
        "val_df = df[(df[\"Year\"] >= 2004) & (df[\"Year\"] <= 2009)]\n",
        "\n",
        "X_train = train_df[feature_cols]\n",
        "y_train = train_df[\"Target\"]\n",
        "X_val = val_df[feature_cols]\n",
        "y_val = val_df[\"Target\"]\n",
        "\n",
        "# Build spline basis (GLM+H paper design)\n",
        "def build_spline_basis(X):\n",
        "    X_spline = pd.DataFrame(index=X.index)\n",
        "    group_ids = []\n",
        "    group_counter = 0\n",
        "    for feature in X.columns:\n",
        "        try:\n",
        "            knots = np.percentile(X[feature].dropna(), [25, 50, 75])\n",
        "            x = X[feature]\n",
        "            basis = pd.DataFrame({\n",
        "                f\"{feature}_x\": x,\n",
        "                f\"{feature}_x2\": x**2,\n",
        "                f\"{feature}_knot1\": np.maximum(0, (x - knots[0])**2),\n",
        "                f\"{feature}_knot2\": np.maximum(0, (x - knots[1])**2),\n",
        "                f\"{feature}_knot3\": np.maximum(0, (x - knots[2])**2),\n",
        "            }, index=X.index)\n",
        "            X_spline = pd.concat([X_spline, basis], axis=1)\n",
        "            group_ids.extend([group_counter] * 5)\n",
        "            group_counter += 1\n",
        "        except:\n",
        "            continue\n",
        "    return X_spline, group_ids\n",
        "\n",
        "X_train_spline, group_ids = build_spline_basis(X_train)\n",
        "X_val_spline, _ = build_spline_basis(X_val)\n",
        "\n",
        "# Tune GLM + H (Huber loss)\n",
        "def tune_glm_huber(X_train, y_train, X_val, y_val, group_ids, lambda_grid, delta=1.0):\n",
        "    best_lambda = None\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_std = scaler.fit_transform(X_train)\n",
        "    X_val_std = scaler.transform(X_val)\n",
        "    y_train = y_train.values\n",
        "    y_val = y_val.values\n",
        "\n",
        "    for lam in lambda_grid:\n",
        "        beta = cp.Variable(X_train.shape[1])\n",
        "        intercept = cp.Variable()\n",
        "        residuals = y_train - X_train_std @ beta - intercept\n",
        "        huber_loss = cp.sum(cp.huber(residuals, M=delta))\n",
        "\n",
        "        group_penalty = 0\n",
        "        for g in np.unique(group_ids):\n",
        "            idx = np.where(np.array(group_ids) == g)[0]\n",
        "            if len(idx) > 0:\n",
        "                group_penalty += cp.norm2(beta[idx])\n",
        "\n",
        "        problem = cp.Problem(cp.Minimize(huber_loss + lam * group_penalty))\n",
        "        try:\n",
        "            problem.solve(solver=cp.OSQP)\n",
        "            if beta.value is None or intercept.value is None:\n",
        "                continue\n",
        "            y_val_pred = X_val_std @ beta.value + intercept.value\n",
        "            val_loss = np.mean(np.where(\n",
        "                np.abs(y_val - y_val_pred) <= delta,\n",
        "                0.5 * (y_val - y_val_pred)**2,\n",
        "                delta * (np.abs(y_val - y_val_pred) - 0.5 * delta)\n",
        "            ))\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_lambda = lam\n",
        "        except:\n",
        "            continue\n",
        "    return best_lambda if best_lambda is not None else 0.01\n",
        "\n",
        "# Define lambda grid and tune\n",
        "lambda_grid = np.logspace(-4, -1, 5)\n",
        "best_lambda_h = tune_glm_huber(X_train_spline, y_train, X_val_spline, y_val, group_ids, lambda_grid)\n",
        "best_lambda_h\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su6eE0O3-HgC",
        "outputId": "3796af51-b222-4b16-c6b7-3253e51461ed"
      },
      "id": "su6eE0O3-HgC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.11/dist-packages (1.6.4)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (0.6.7.post3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (3.2.7.post2)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (1.14.1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy) (0.1.7.post5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pjtbb0-nBOVf"
      },
      "id": "Pjtbb0-nBOVf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare containers\n",
        "glm_h_preds, glm_h_true, glm_h_dates = [], [], []\n",
        "\n",
        "# Define rolling years\n",
        "start_year = 2010\n",
        "end_year = df_merged[\"Year\"].max()\n",
        "best_lambda = 0.01\n",
        "delta = 1.0\n",
        "\n",
        "# Spline builder\n",
        "def build_spline_basis(X):\n",
        "    X_spline = pd.DataFrame(index=X.index)\n",
        "    group_ids = []\n",
        "    group_counter = 0\n",
        "    for feature in X.columns:\n",
        "        knots = np.percentile(X[feature].dropna(), [25, 50, 75])\n",
        "        x = X[feature]\n",
        "        basis = pd.DataFrame({\n",
        "            f\"{feature}_x\": x,\n",
        "            f\"{feature}_x2\": x**2,\n",
        "            f\"{feature}_knot1\": np.maximum(0, (x - knots[0])**2),\n",
        "            f\"{feature}_knot2\": np.maximum(0, (x - knots[1])**2),\n",
        "            f\"{feature}_knot3\": np.maximum(0, (x - knots[2])**2),\n",
        "        }, index=X.index)\n",
        "        X_spline = pd.concat([X_spline, basis], axis=1)\n",
        "        group_ids.extend([group_counter] * 5)\n",
        "        group_counter += 1\n",
        "    return X_spline, group_ids\n",
        "\n",
        "import cvxpy as cp\n",
        "for test_year in range(start_year, end_year + 1):\n",
        "    train_df = df_merged[df_merged[\"Year\"] <= test_year - 1]\n",
        "    test_df = df_merged[df_merged[\"Year\"] == test_year]\n",
        "\n",
        "    if train_df.empty or test_df.empty:\n",
        "        continue\n",
        "\n",
        "    X_train = train_df[final_features]\n",
        "    y_train = train_df[\"Target\"]\n",
        "    X_test = test_df[final_features]\n",
        "    y_test = test_df[\"Target\"]\n",
        "\n",
        "    X_train_spline, group_ids = build_spline_basis(X_train)\n",
        "    X_test_spline, _ = build_spline_basis(X_test)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_std = scaler.fit_transform(X_train_spline)\n",
        "    X_test_std = scaler.transform(X_test_spline)\n",
        "\n",
        "    beta = cp.Variable(X_train_std.shape[1])\n",
        "    intercept = cp.Variable()\n",
        "    residuals = y_train.values - X_train_std @ beta - intercept\n",
        "    huber_loss = cp.sum(cp.huber(residuals, M=delta))\n",
        "\n",
        "    group_penalty = 0\n",
        "    for g in np.unique(group_ids):\n",
        "        idx = np.where(np.array(group_ids) == g)[0]\n",
        "        if len(idx) > 0:\n",
        "            group_penalty += cp.norm2(beta[idx])\n",
        "\n",
        "    problem = cp.Problem(cp.Minimize(huber_loss + best_lambda * group_penalty))\n",
        "    try:\n",
        "        problem.solve(solver=cp.OSQP)\n",
        "        if beta.value is not None and intercept.value is not None:\n",
        "            y_pred = X_test_std @ beta.value + intercept.value\n",
        "            glm_h_preds.extend(y_pred)\n",
        "            glm_h_true.extend(y_test.values)\n",
        "            glm_h_dates.extend(test_df[\"Date\"].values)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Calculate out-of-sample R²\n",
        "r2_score(glm_h_true, glm_h_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "EjefXF98-WJO",
        "outputId": "93e957a9-e5ef-4d03-98d8-c22a3b3534be"
      },
      "id": "EjefXF98-WJO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_merged' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ede13404a8ea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define rolling years\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstart_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2010\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mend_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Year\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mbest_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression, HuberRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "\n",
        "# 📌 Load dataset\n",
        "file_path = \"Cleaned_OSEFX_Market_Macro_Data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 📌 Convert Date column to datetime and sort\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df = df.sort_values(by=[\"Date\", \"Instrument\"]).reset_index(drop=True)\n",
        "\n",
        "# 📌 Adjusted target variable: Monthly Stock Return - (Norges Bank 10Y / 12)\n",
        "df[\"AdjustedReturn\"] = df[\"MonthlyReturn\"] - (df[\"NorgesBank10Y\"] / 12)\n",
        "\n",
        "# 📌 Define Features and Target\n",
        "features = [\n",
        "    \"ClosePrice\", \"OpenPrice\", \"Volume\", \"BidPrice\", \"AskPrice\", \"DividendYield\",\n",
        "    \"BookValuePerShare\", \"Beta\", \"MarketCap\", \"CommonSharesOutstanding\",\n",
        "    \"Momentum_3M\", \"Momentum_6M\", \"Momentum_12M\", \"Volatility_3M\", \"Volatility_6M\",\n",
        "    \"Volatility_12M\", \"BidAskSpread\", \"TurnoverRatio\", \"BM\", \"BrentOil\", \"USDNOK\",\n",
        "    \"EURNOK\", \"US10Y\", \"USCPI\", \"USGDPGrowth\", \"NorwegianCPI\", \"NorgesBank10Y\"\n",
        "]\n",
        "target = \"AdjustedReturn\"\n",
        "\n",
        "# 📌 Drop missing values in target\n",
        "df = df.dropna(subset=[target])\n",
        "\n",
        "# 📌 Replace infinities with NaN and fill NaNs with median values\n",
        "df[features] = df[features].replace([np.inf, -np.inf], np.nan)\n",
        "df[features] = df[features].fillna(df[features].median())\n",
        "\n",
        "# 📌 Prepare feature matrix and target variable\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# 📌 Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X = pd.DataFrame(X_scaled, columns=features, index=df.index)\n",
        "\n",
        "# 📌 Define years for rolling window approach\n",
        "years = sorted(df[\"Date\"].dt.year.unique())\n",
        "min_training_years = 9  # Initial 9 years for training\n",
        "\n",
        "# Store results\n",
        "glm_is_r2_scores, glm_val_r2_scores, glm_test_r2_scores = [], [], []\n",
        "glmh_is_r2_scores, glmh_val_r2_scores, glmh_test_r2_scores = [], [], []\n",
        "\n",
        "# Hybrid Scheme Implementation: Expanding Training Period, Fixed Validation and Testing Periods\n",
        "for i in range(min_training_years, len(years) - 1):  # Start from year 10 (first time training can happen)\n",
        "    train_years = years[:i]  # Expanding training set (starts with 9 years, then add 1 each iteration)\n",
        "    val_years = years[i:i+5]  # Fixed 5-year validation set\n",
        "    test_years = years[i+5:i+10]  # Fixed 5-year test set (adjusted to 5 years)\n",
        "\n",
        "    # Ensure we don't exceed the available years (to avoid index error)\n",
        "    if len(test_years) < 5:\n",
        "        break\n",
        "\n",
        "    # Define the indices for the training, validation, and test sets\n",
        "    train_idx = df[df[\"Date\"].dt.year.isin(train_years)].index\n",
        "    val_idx = df[df[\"Date\"].dt.year.isin(val_years)].index\n",
        "    test_idx = df[df[\"Date\"].dt.year.isin(test_years)].index\n",
        "\n",
        "    X_train, X_val, X_test = X.loc[train_idx], X.loc[val_idx], X.loc[test_idx]\n",
        "    y_train, y_val, y_test = y.loc[train_idx], y.loc[val_idx], y.loc[test_idx]\n",
        "\n",
        "    # Instantiate models\n",
        "    glm_model = LinearRegression()  # GLM\n",
        "    glmh_model = HuberRegressor(epsilon=1.0, max_iter=1000)  # GLM + Huber loss with increased max_iter\n",
        "\n",
        "    # Fit GLM and GLM + Huber models\n",
        "    glm_model.fit(X_train, y_train)\n",
        "    glmh_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on training, validation, and test sets for both models\n",
        "    glm_train_preds = glm_model.predict(X_train)\n",
        "    glm_val_preds = glm_model.predict(X_val)\n",
        "    glm_test_preds = glm_model.predict(X_test)\n",
        "\n",
        "    glmh_train_preds = glmh_model.predict(X_train)\n",
        "    glmh_val_preds = glmh_model.predict(X_val)\n",
        "    glmh_test_preds = glmh_model.predict(X_test)\n",
        "\n",
        "    # Calculate R^2 for GLM\n",
        "    glm_train_r2 = r2_score(y_train, glm_train_preds)\n",
        "    glm_val_r2 = r2_score(y_val, glm_val_preds)\n",
        "    glm_test_r2 = r2_score(y_test, glm_test_preds)\n",
        "\n",
        "    # Calculate R^2 for GLM + Huber\n",
        "    glmh_train_r2 = r2_score(y_train, glmh_train_preds)\n",
        "    glmh_val_r2 = r2_score(y_val, glmh_val_preds)\n",
        "    glmh_test_r2 = r2_score(y_test, glmh_test_preds)\n",
        "\n",
        "    # Store results\n",
        "    glm_is_r2_scores.append(glm_train_r2)\n",
        "    glm_val_r2_scores.append(glm_val_r2)\n",
        "    glm_test_r2_scores.append(glm_test_r2)\n",
        "\n",
        "    glmh_is_r2_scores.append(glmh_train_r2)\n",
        "    glmh_val_r2_scores.append(glmh_val_r2)\n",
        "    glmh_test_r2_scores.append(glmh_test_r2)\n",
        "\n",
        "# 📌 Display results\n",
        "print(\"GLM Results:\")\n",
        "print(f\"Train R²: {np.mean(glm_is_r2_scores):.4f}, Validation R²: {np.mean(glm_val_r2_scores):.4f}, Test R²: {np.mean(glm_test_r2_scores):.4f}\")\n",
        "\n",
        "print(\"\\nGLM + Huber Results:\")\n",
        "print(f\"Train R²: {np.mean(glmh_is_r2_scores):.4f}, Validation R²: {np.mean(glmh_val_r2_scores):.4f}, Test R²: {np.mean(glmh_test_r2_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "PZt2VamWBPVu"
      },
      "id": "PZt2VamWBPVu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install cvxpy scs\n",
        "\n",
        "# Re-import required modules due to kernel reset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"ML_Standardized_Ready_OSEFX.csv\")\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df = df.sort_values(by=[\"Instrument\", \"Date\"])\n",
        "\n",
        "# Define features\n",
        "final_features = [col for col in df.columns if col not in [\"Target\", \"Year\", \"Date\", \"Instrument\"]]\n",
        "\n",
        "# Define spline basis function\n",
        "def spline_basis(x, knots):\n",
        "    basis = pd.DataFrame(index=x.index)\n",
        "    basis[\"x\"] = x\n",
        "    basis[\"x2\"] = x**2\n",
        "    for i, knot in enumerate(knots):\n",
        "        basis[f\"knot_{i+1}\"] = np.maximum(0, (x - knot))**2\n",
        "    return basis\n",
        "\n",
        "def build_spline_basis(X):\n",
        "    X_spline = pd.DataFrame(index=X.index)\n",
        "    group_ids = []\n",
        "    group_counter = 0\n",
        "    for feature in X.columns:\n",
        "        try:\n",
        "            knots = np.percentile(X[feature].dropna(), [25, 50, 75])\n",
        "            train_basis = spline_basis(X[feature], knots)\n",
        "            colnames = [f\"{feature}_{c}\" for c in train_basis.columns]\n",
        "            train_basis.columns = colnames\n",
        "            X_spline = pd.concat([X_spline, train_basis], axis=1)\n",
        "            group_ids.extend([group_counter] * train_basis.shape[1])\n",
        "            group_counter += 1\n",
        "        except:\n",
        "            continue\n",
        "    return X_spline, group_ids\n",
        "\n",
        "# Lambda tuning\n",
        "train_df = df[(df[\"Year\"] >= 1995) & (df[\"Year\"] <= 2003)]\n",
        "val_df = df[(df[\"Year\"] >= 2004) & (df[\"Year\"] <= 2009)]\n",
        "\n",
        "X_train = train_df[final_features]\n",
        "y_train = train_df[\"Target\"]\n",
        "X_val = val_df[final_features]\n",
        "y_val = val_df[\"Target\"]\n",
        "\n",
        "X_train_spline, group_ids = build_spline_basis(X_train)\n",
        "X_val_spline, _ = build_spline_basis(X_val)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train_spline)\n",
        "X_val_std = scaler.transform(X_val_spline)\n",
        "y_train_np = y_train.values\n",
        "y_val_np = y_val.values\n",
        "\n",
        "best_lambda = None\n",
        "best_loss = np.inf\n",
        "lambda_grid = np.logspace(-4, -1, 5)\n",
        "\n",
        "for lam in lambda_grid:\n",
        "    beta = cp.Variable(X_train_std.shape[1])\n",
        "    intercept = cp.Variable()\n",
        "    residuals = y_train_np - X_train_std @ beta - intercept\n",
        "    huber_loss = cp.sum(cp.huber(residuals, M=1.0))\n",
        "    group_penalty = sum(cp.norm2(beta[np.where(np.array(group_ids) == g)[0]])\n",
        "                        for g in np.unique(group_ids))\n",
        "    problem = cp.Problem(cp.Minimize(huber_loss + lam * group_penalty))\n",
        "    try:\n",
        "        problem.solve(solver=cp.SCS, verbose=False)\n",
        "        if beta.value is None or intercept.value is None:\n",
        "            continue\n",
        "        y_pred_val = X_val_std @ beta.value + intercept.value\n",
        "        loss = np.mean(np.where(\n",
        "            np.abs(y_val_np - y_pred_val) <= 1.0,\n",
        "            0.5 * (y_val_np - y_pred_val)**2,\n",
        "            1.0 * (np.abs(y_val_np - y_pred_val) - 0.5)\n",
        "        ))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_lambda = lam\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Save best lambda and continue\n",
        "best_lambda_to_use = best_lambda if best_lambda is not None else 0.01\n",
        "best_lambda_to_use\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# STEP 2: Rolling Out-of-Sample Evaluation\n",
        "predictions, true_returns, prediction_dates = [], [], []\n",
        "\n",
        "start_test_year = 2010\n",
        "end_test_year = df[\"Year\"].max()\n",
        "\n",
        "for test_year in range(start_test_year, end_test_year + 1):\n",
        "    train_end = test_year - 7\n",
        "    val_start = test_year - 6\n",
        "    val_end = test_year - 1\n",
        "\n",
        "    train_df = df[df[\"Year\"] <= train_end]\n",
        "    val_df = df[(df[\"Year\"] >= val_start) & (df[\"Year\"] <= val_end)]\n",
        "    test_df = df[df[\"Year\"] == test_year]\n",
        "\n",
        "    if len(train_df) < 100 or len(val_df) < 10 or len(test_df) < 10:\n",
        "        continue\n",
        "\n",
        "    X_train = train_df[final_features]\n",
        "    y_train = train_df[\"Target\"]\n",
        "    X_test = test_df[final_features]\n",
        "    y_test = test_df[\"Target\"]\n",
        "\n",
        "    # Build spline basis\n",
        "    X_train_spline, group_ids = build_spline_basis(X_train)\n",
        "    X_test_spline, _ = build_spline_basis(X_test)\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_train_std = scaler.fit_transform(X_train_spline)\n",
        "    X_test_std = scaler.transform(X_test_spline)\n",
        "    y_train_np = y_train.values\n",
        "\n",
        "    # Define variables\n",
        "    beta = cp.Variable(X_train_std.shape[1])\n",
        "    intercept = cp.Variable()\n",
        "    residuals = y_train_np - X_train_std @ beta - intercept\n",
        "    huber_loss = cp.sum(cp.huber(residuals, M=1.0))\n",
        "\n",
        "    group_penalty = sum(cp.norm2(beta[np.where(np.array(group_ids) == g)[0]])\n",
        "                        for g in np.unique(group_ids))\n",
        "\n",
        "    problem = cp.Problem(cp.Minimize(huber_loss + best_lambda_to_use * group_penalty))\n",
        "\n",
        "    try:\n",
        "        problem.solve(solver=cp.SCS, verbose=False)\n",
        "        if beta.value is None or intercept.value is None:\n",
        "            continue\n",
        "        y_pred = X_test_std @ beta.value + intercept.value\n",
        "        predictions.extend(y_pred)\n",
        "        true_returns.extend(y_test.values)\n",
        "        prediction_dates.extend(test_df[\"Date\"].values)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# STEP 3: Evaluation\n",
        "y_true = np.array(true_returns)\n",
        "y_pred = np.array(predictions)\n",
        "oos_r2 = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "print(f\"📈 Out-of-Sample R² (GLM+H): {oos_r2:.4f}\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(prediction_dates, y_true, label=\"Actual\", alpha=0.7)\n",
        "plt.plot(prediction_dates, y_pred, label=\"Predicted\", alpha=0.7)\n",
        "plt.title(\"GLM+H Out-of-Sample Excess Return Prediction (2010–Present)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Excess Return\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optional: Create and display results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    \"Date\": prediction_dates,\n",
        "    \"Actual\": y_true,\n",
        "    \"Predicted\": y_pred\n",
        "})\n",
        "\n",
        "# If you want to save it:\n",
        "# results_df.to_csv(\"GLMH_Predictions_OSEFX.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "KtQGXgZ4C9Fj",
        "outputId": "c1d2ad77-1fb2-4461-ad93-55f81710d7b6"
      },
      "id": "KtQGXgZ4C9Fj",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.11/dist-packages (1.6.4)\n",
            "Requirement already satisfied: scs in /usr/local/lib/python3.11/dist-packages (3.2.7.post2)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (0.6.7.post3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy) (1.14.1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy) (0.1.7.post5)\n",
            "📈 Out-of-Sample R² (GLM+H): -582174.3648\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fYL5S7mRDDxQ"
      },
      "id": "fYL5S7mRDDxQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}