{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc0a257",
   "metadata": {},
   "source": [
    "# README_pipeline_guide.md"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00c4fa8a",
   "metadata": {},
   "source": [
    "This structure consists of four main files:\n",
    "\n",
    "1. preprocess_data.py\n",
    "Prepares raw data for modeling. It calculates monthly and excess returns, creates the prediction target (TargetReturn_t+1), performs winsorization, feature engineering (e.g., term spread, log transforms), handles missing values, and saves the cleaned dataset for the pipeline.\n",
    "\n",
    "2. config.py\n",
    "Holds all configuration settings (file paths, model parameters, flags). We primarily edit this file to change datasets, models, or hyperparameters.\n",
    "\n",
    "3. pipeline_utils.py\n",
    "Contains shared, model-agnostic functions‚Äîsuch as data loading, standardization, cleaning, feature engineering, splitting, portfolio evaluation, plotting, and result saving. We generally don‚Äôt need to edit this unless the methodology changes.\n",
    "\n",
    "4. main_runner.py\n",
    "The main script that orchestrates the full pipeline. It imports from the other files, runs the ML models (based on what is enabled in config.py), evaluates results, and saves everything. This is where we can plug in new models if needed.\n",
    "\n",
    "üîÅ Prediction Target\n",
    "Based on Gu et al. (2020) [(Et(ri,t+1) = g*(zi,t))], the pipeline is designed to predict next month‚Äôs excess return using information available at the end of month t.\n",
    "\n",
    "To align with this framework, the pipeline uses the target variable TargetReturn_t+1 rather than contemporaneous excess returns.\n",
    "\n",
    "‚úÖ How to Use / Run the Pipeline:\n",
    "\n",
    "Each file should be saved separately as a .py file. We follow this order:\n",
    "\n",
    "Run File 1: preprocess_data.py\n",
    "‚Üí This generates Cleaned_OSEFX_Market_Macro_Data_PREPROCESSED.csv\n",
    "\n",
    "Run File 2: config.py\n",
    "‚Üí Loads all settings (this is lightweight and quick)\n",
    "\n",
    "Run File 3: pipeline_utils.py\n",
    "‚Üí Loads shared functions (also quick)\n",
    "\n",
    "Run File 4: main_runner.py\n",
    "‚Üí This is the main file that runs the entire ML pipeline\n",
    "\n",
    "üß™ Testing Workflow (Very Useful!)\n",
    "We don‚Äôt need to run the full pipeline every time. Instead, we can test just one or two models to verify changes before running the full suite (which may take hours, especially for non-linear models).\n",
    "\n",
    "How?\n",
    "\n",
    "Open config.py and set which models to run:\n",
    "\n",
    "# <<< MODEL CONFIGURATION >>>\n",
    "RUN_MODELS = {\n",
    "    'OLS': True,        'OLS3H': True,       'PLS': False,        'PCR': False,\n",
    "    'ENET': False,      'GLM_H': False,      'RF': False,         'GBRT_H': False,\n",
    "    'NN1': False,       'NN2': False,        'NN3': False,        'NN4': False,\n",
    "    'NN5': False,\n",
    "}\n",
    "\n",
    "We simply set True for the models we want to run, and False for those we want to skip. This flexibility makes debugging and experimentation easier.\n",
    "\n",
    "‚ö†Ô∏è Very Important!\n",
    "Any time we make a change in one of the files‚Äîespecially preprocess_data.py, config.py, or pipeline_utils.py‚Äîwe must remember to:\n",
    "\n",
    "Save the updated file and re-run from File 1 ‚Üí File 2 ‚Üí File 3 ‚Üí File 4\n",
    "This ensures consistency across the pipeline and accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e96085",
   "metadata": {},
   "source": [
    "# File 1: preprocess_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fa813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- preprocess_data.py ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import datetime\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# *** DEFINE HELPER FUNCTION HERE ***\n",
    "def find_col(df, potential_names, default=None):\n",
    "    \"\"\"Helper to find the first matching column name from a list.\"\"\"\n",
    "    for name in potential_names:\n",
    "        if name in df.columns: return name\n",
    "    print(f\"  Warning: Could not find column using names: {potential_names}\") # Added warning\n",
    "    return default\n",
    "# *** END HELPER FUNCTION DEFINITION ***\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = \"Cleaned_OSEFX_Market_Macro_Data.csv\"\n",
    "OUTPUT_FILE = \"Cleaned_OSEFX_Market_Macro_Data_PREPROCESSED.csv\" # Output for the pipeline\n",
    "TARGET_COL_NAME = \"TargetReturn_t+1\"         # Standard name for the pipeline\n",
    "NEXT_RAW_RET_COL_NAME = \"NextMonthlyReturn_t+1\" # Raw return for portfolio eval\n",
    "MKT_CAP_ORIG_COL_NAME = \"MarketCap_orig\"        # Original market cap for portfolio eval\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Loading raw data from: {INPUT_FILE}\")\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    print(f\"Raw data loaded. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found: {INPUT_FILE}\"); exit()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading data: {e}\"); exit()\n",
    "\n",
    "# --- Data Preparation ---\n",
    "print(\"Sorting and preparing columns...\")\n",
    "df = df.sort_values(by=[\"Instrument\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "# *** COLUMN CLEANING STEP ***\n",
    "print(\"  Cleaning original column names...\")\n",
    "df.columns = df.columns.str.replace(\"[^A-Za-z0-9_]+\", \"_\", regex=True).str.strip('_').str.replace('__', '_')\n",
    "# Find the potentially renamed columns AFTER cleaning\n",
    "norges_bank_10y_col = find_col(df, ['NorgesBank10Y', 'norgesbank10y']) # Helper function is now defined\n",
    "if not norges_bank_10y_col:\n",
    "    print(\"ERROR: Cannot find NorgesBank10Y column after cleaning.\")\n",
    "    exit()\n",
    "close_price_col = find_col(df, ['ClosePrice', 'closeprice'])\n",
    "common_shares_col = find_col(df, ['CommonSharesOutstanding', 'commonsharesoutstanding'])\n",
    "if not close_price_col or not common_shares_col:\n",
    "     print(f\"ERROR: Cannot find ClosePrice/CommonSharesOutstanding after cleaning.\")\n",
    "     exit()\n",
    "nibor3m_col = find_col(df, ['NIBOR3M', 'nibor3m'])\n",
    "if not nibor3m_col:\n",
    "    print(\"ERROR: Cannot find NIBOR3M column after cleaning.\")\n",
    "    exit()\n",
    "print(\"  Original column names cleaned.\")\n",
    "# *** END OF CLEANING AND FINDING RENAMED COLS ***\n",
    "\n",
    "# --- Continue with Calculations using found column names ---\n",
    "# Calculate Monthly Return (t) and Winsorize EARLY\n",
    "print(\"Calculating returns...\")\n",
    "df[\"MonthlyReturn_t\"] = df.groupby(\"Instrument\")[close_price_col].pct_change() # Use found name\n",
    "# ... (rest of return calculation as before) ...\n",
    "df[\"MonthlyReturn_t\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df[\"MonthlyReturn_t\"].fillna(0, inplace=True)\n",
    "df[\"MonthlyReturn_t\"] = winsorize(df[\"MonthlyReturn_t\"].values, limits=[0.01, 0.01])\n",
    "print(\"  MonthlyReturn_t calculated and winsorized.\")\n",
    "\n",
    "# Calculate Risk-Free Rate (t) using the cleaned column name\n",
    "df.loc[:, \"MonthlyRiskFreeRate_t\"] = df[norges_bank_10y_col] / 12 / 100 # Use found name\n",
    "print(\"  MonthlyRiskFreeRate_t calculated.\")\n",
    "\n",
    "# Calculate Adjusted Return (Excess Return for month t)\n",
    "df[\"AdjustedReturn_t\"] = df[\"MonthlyReturn_t\"] - df[\"MonthlyRiskFreeRate_t\"]\n",
    "print(\"  AdjustedReturn_t (Excess Return t) calculated.\")\n",
    "\n",
    "# --- Create Target and Necessary Lead Variables ---\n",
    "# *** NOW THESE COLUMNS WILL BE CREATED WITH '+' ***\n",
    "print(\"Calculating lead variables (Target and Next Raw Return)...\")\n",
    "df[TARGET_COL_NAME] = df.groupby(\"Instrument\")[\"AdjustedReturn_t\"].shift(-1)\n",
    "print(f\"  Target variable '{TARGET_COL_NAME}' created.\")\n",
    "df[NEXT_RAW_RET_COL_NAME] = df.groupby(\"Instrument\")[\"MonthlyReturn_t\"].shift(-1)\n",
    "print(f\"  Next raw return variable '{NEXT_RAW_RET_COL_NAME}' created.\")\n",
    "# *** END OF TARGET CREATION ***\n",
    "\n",
    "# Drop rows where the TARGET variable is NaN (essential!)\n",
    "initial_rows = len(df)\n",
    "df.dropna(subset=[TARGET_COL_NAME], inplace=True)\n",
    "print(f\"  Dropped {initial_rows - len(df)} rows with missing target '{TARGET_COL_NAME}'.\")\n",
    "if df.empty: print(\"ERROR: DataFrame empty after dropping missing target.\"); exit()\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "print(\"Performing feature engineering (Market Cap, Term Spread, Log Transforms)...\")\n",
    "# Recalculate MarketCap (t) using cleaned column names\n",
    "df[\"MarketCap\"] = df[close_price_col] * df[common_shares_col] # Use found names\n",
    "df.loc[df[\"MarketCap\"] <= 0, \"MarketCap\"] = np.nan\n",
    "df[MKT_CAP_ORIG_COL_NAME] = df[\"MarketCap\"].copy()\n",
    "print(f\"  MarketCap calculated and original stored in '{MKT_CAP_ORIG_COL_NAME}'.\")\n",
    "\n",
    "# Create term spread (t) using cleaned column names\n",
    "df[\"TermSpread\"] = df[norges_bank_10y_col] - df[nibor3m_col] # Use found names\n",
    "print(\"  TermSpread calculated.\")\n",
    "\n",
    "# Log-transform relevant variables (using potentially cleaned names)\n",
    "vars_to_log = [\"MarketCap\", \"BM\", \"ClosePrice\", \"Volume\", \"CommonSharesOutstanding\"]\n",
    "print(f\"  Log-transforming: {vars_to_log}\")\n",
    "for var in vars_to_log:\n",
    "    # Find the potentially cleaned column name - use more robust check now\n",
    "    potential_names = [var, var.lower(), var.replace(\"_\",\"\")] # Add different variations if needed\n",
    "    cleaned_var_name = find_col(df, potential_names)\n",
    "    if cleaned_var_name:\n",
    "        df[cleaned_var_name] = pd.to_numeric(df[cleaned_var_name], errors='coerce')\n",
    "        original_nan_mask = df[cleaned_var_name].isna()\n",
    "        log_col = f\"log_{cleaned_var_name}\" # Create log name based on found name\n",
    "        df[log_col] = np.nan\n",
    "        positive_mask = (~original_nan_mask) & (df[cleaned_var_name] > 1e-9)\n",
    "        df.loc[positive_mask, log_col] = np.log(df.loc[positive_mask, cleaned_var_name])\n",
    "        print(f\"    - Logged {positive_mask.sum()} positive values for {cleaned_var_name} -> {log_col}.\")\n",
    "    else:\n",
    "        print(f\"    - Warning: Column for '{var}' not found for log transform (using {potential_names}).\")\n",
    "\n",
    "\n",
    "# --- Final Checks and Save ---\n",
    "print(\"Final checks and saving preprocessed data...\")\n",
    "# Ensure essential ID/Date columns have standard names if possible\n",
    "if 'Instrument' not in df.columns and 'instrument' in df.columns:\n",
    "    df = df.rename(columns={'instrument': 'Instrument'})\n",
    "if 'Date' not in df.columns and 'date' in df.columns:\n",
    "    df = df.rename(columns={'date': 'Date'})\n",
    "\n",
    "# Verify essential columns exist before saving\n",
    "essential_final = ['Date', 'Instrument', TARGET_COL_NAME, NEXT_RAW_RET_COL_NAME, MKT_CAP_ORIG_COL_NAME]\n",
    "missing = [c for c in essential_final if c not in df.columns]\n",
    "if missing:\n",
    "    print(f\"ERROR: Essential columns missing before saving: {missing}\")\n",
    "    print(f\"Available columns: {df.columns.tolist()}\")\n",
    "    exit()\n",
    "\n",
    "# Drop rows where original market cap is non-positive or NaN\n",
    "initial_rows = len(df)\n",
    "df = df.dropna(subset=[MKT_CAP_ORIG_COL_NAME])\n",
    "df = df[df[MKT_CAP_ORIG_COL_NAME] > 0]\n",
    "rows_removed = initial_rows - len(df)\n",
    "if rows_removed > 0:\n",
    "    print(f\"  Dropped {rows_removed} rows with missing or non-positive '{MKT_CAP_ORIG_COL_NAME}'.\")\n",
    "\n",
    "if df.empty: print(\"ERROR: DataFrame empty after final checks.\"); exit()\n",
    "\n",
    "# Convert numeric columns to float32\n",
    "numeric_cols_final = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols_final] = df[numeric_cols_final].astype(\"float32\")\n",
    "\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Preprocessing complete. Final shape: {df.shape}\")\n",
    "print(f\"Preprocessed data saved to: {OUTPUT_FILE}\")\n",
    "print(\"\\nFinal Data Info:\")\n",
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c19d7",
   "metadata": {},
   "source": [
    "# File 2: config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8514d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- config.py ---\n",
    "# Central configuration file for the ML Asset Pricing Pipeline.\n",
    "# Edit the settings below to match your data, desired models, and analysis parameters.\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# <<< FILE PATHS >>>\n",
    "# --------------------------------------------------------------------------\n",
    "# *** POINT TO THE PREPROCESSED FILE ***\n",
    "DATA_FILE = \"Cleaned_OSEFX_Market_Macro_Data_PREPROCESSED.csv\"\n",
    "BENCHMARK_FILE = None\n",
    "FF_FACTOR_FILE = \"Europe_4_Factors_Monthly.csv\" # <--- Path to Fama-French factor CSV (optional)\n",
    "PORTFOLIO_DEFS_FILE = None\n",
    "OUTPUT_DIR = \"ML_Pipeline_Results_Yearly_Percentile_Preprocessed\" # Changed output dir name\n",
    "\n",
    "# <<< DATA PREPARATION CONFIG >>>\n",
    "# --------------------------------------------------------------------------\n",
    "# --- Column Names (These should match names in the *PREPROCESSED* CSV) ---\n",
    "# *** SIMPLIFIED: Only map ID/Date if they aren't standard, target is defined below ***\n",
    "COLUMN_CONFIG = {\n",
    "    'date': ['Date', 'date'],\n",
    "    'id': ['Instrument', 'instrument'],\n",
    "    # Other columns should now have clean names from preprocess_data.py\n",
    "    # We don't need mappings for price, shares, rf, book_market etc. here\n",
    "    # as they are either used in preprocessing or already logged/transformed.\n",
    "    'EconomicSector': ['EconomicSector'] # Keep if sector dummies are needed\n",
    "}\n",
    "\n",
    "# --- Feature Engineering (Now done externally) ---\n",
    "# VARS_TO_LOG = [] # Logging is done in preprocess_data.py\n",
    "TARGET_VARIABLE = \"TargetReturn_t+1\"         # *** MATCHES PREPROCESSING SCRIPT OUTPUT ***\n",
    "NEXT_RETURN_VARIABLE = \"NextMonthlyReturn_t+1\" # *** MATCHES PREPROCESSING SCRIPT OUTPUT ***\n",
    "MARKET_CAP_ORIG_VARIABLE = \"MarketCap_orig\"    # *** MATCHES PREPROCESSING SCRIPT OUTPUT ***\n",
    "\n",
    "# --- Data Cleaning & Filtering (Within Pipeline) ---\n",
    "# WINSORIZE_LIMITS = [] # Winsorizing of raw returns done externally\n",
    "# Imputation will happen in clean_data on features\n",
    "# Dropping NaNs focuses on target/ID/next_ret/mkt_cap_orig (essentials for modeling/analysis)\n",
    "ESSENTIAL_COLS_FOR_DROPNA = ['Date', 'Instrument', TARGET_VARIABLE, NEXT_RETURN_VARIABLE, MARKET_CAP_ORIG_VARIABLE]\n",
    "\n",
    "# <<< ROLLING WINDOW CONFIG >>>\n",
    "# --------------------------------------------------------------------------\n",
    "INITIAL_TRAIN_YEARS = 9\n",
    "VALIDATION_YEARS = 6\n",
    "TEST_YEARS_PER_WINDOW = 1\n",
    "\n",
    "# <<< MODEL CONFIGURATION >>>\n",
    "# --------------------------------------------------------------------------\n",
    "# --- Model Selection ---\n",
    "RUN_MODELS = {\n",
    "    'OLS': True,        'OLS3H': True,      'PLS': True,        'PCR': True,\n",
    "    'ENET': True,       'GLM_H': False,      'RF': False,         'GBRT_H': False,\n",
    "    'NN1': False,        'NN2': False,        'NN3': False,         'NN4': False,\n",
    "    'NN5': False,\n",
    "}\n",
    "\n",
    "# --- Feature Sets ---\n",
    "# *** THESE MUST MATCH THE COLUMN NAMES IN THE *PREPROCESSED* CSV ***\n",
    "OLS3_FEATURE_NAMES = [\"log_BM\", \"Momentum_12M\", \"log_MarketCap\"] # Verify these exist in the preprocessed file\n",
    "MODEL_FEATURE_MAP = { # Which feature set does each model use?\n",
    "    'OLS': 'all_numeric', 'OLS3H': 'ols3_features', 'PLS': 'all_numeric',\n",
    "    'PCR': 'all_numeric', 'ENET': 'all_numeric', 'GLM_H': 'all_numeric',\n",
    "    'RF': 'all_numeric', 'GBRT_H': 'all_numeric',\n",
    "    'NN1': 'all_numeric', 'NN2': 'all_numeric', 'NN3': 'all_numeric',\n",
    "    'NN4': 'all_numeric', 'NN5': 'all_numeric',\n",
    "}\n",
    "\n",
    "# --- Model Hyperparameters (Keep as is, or adjust) ---\n",
    "MODEL_PARAMS = {\n",
    "    'OLS': {},\n",
    "    'OLS3H': {'maxiter': 100, 'tol': 1e-6},\n",
    "    'PLS': {'n_components_grid': [1, 3, 5, 8, 10, 15]},\n",
    "    'PCR': {'n_components_grid': [1, 5, 10, 15, 20, 25]},\n",
    "    'ENET': {'alphas': np.logspace(-6, 1, 8), 'l1_ratio': [0.1, 0.5, 0.9, 0.99, 1.0], 'cv_folds': 3, 'max_iter': 1000, 'tol': 0.001, 'n_jobs': -1},\n",
    "    'GLM_H': {'param_grid': {'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0], 'epsilon': [1.1, 1.35, 1.5, 2.0]}, 'max_iter': 300},\n",
    "    'RF': {'param_grid': {'n_estimators': [100], 'max_depth': [3, 6, 10], 'min_samples_leaf': [50, 100], 'max_features': ['sqrt', 0.33]}, 'n_jobs': -1, 'random_state': 42},\n",
    "    'GBRT_H': {'param_grid': {'n_estimators': [100], 'learning_rate': [0.1], 'max_depth': [3, 5], 'min_samples_leaf': [50, 100], 'alpha': [0.9]}, 'loss': 'huber', 'random_state': 42},\n",
    "    'NN_SHARED': {'param_grid': {'lambda1': [1e-5, 1e-4, 1e-3], 'learning_rate': [0.001, 0.01]}, 'epochs': 100, 'batch_size': 10000, 'patience': 5, 'ensemble_size': 10, 'random_seed_base': 42},\n",
    "    'NN1': {'name': 'NN1', 'hidden_units': [32]},\n",
    "    'NN2': {'name': 'NN2', 'hidden_units': [64, 32]},\n",
    "    'NN3': {'name': 'NN3', 'hidden_units': [96, 64, 32]},\n",
    "    'NN4': {'name': 'NN4', 'hidden_units': [128, 96, 64, 32]},\n",
    "    'NN5': {'name': 'NN5', 'hidden_units': [128, 96, 64, 32, 16]},\n",
    "}\n",
    "\n",
    "# <<< ANALYSIS & REPORTING CONFIG >>>\n",
    "# --------------------------------------------------------------------------\n",
    "SUBSETS_TO_RUN = ['all', 'big', 'small']\n",
    "BIG_FIRM_TOP_PERCENT = 30\n",
    "SMALL_FIRM_BOTTOM_PERCENT = 30\n",
    "# Use the specific market cap column saved for this purpose\n",
    "FILTER_SMALL_CAPS_PORTFOLIO = False\n",
    "ANNUALIZATION_FACTOR = 12\n",
    "\n",
    "# --- Variable Importance ---\n",
    "CALCULATE_VI = True\n",
    "VI_METHOD = 'permutation_zero'\n",
    "VI_PLOT_TOP_N = 20\n",
    "MODEL_VI_STRATEGY = {\n",
    "    'OLS': 'per_window', 'OLS3H': 'per_window', 'PLS': 'per_window',\n",
    "    'PCR': 'per_window', 'ENET': 'per_window', 'GLM_H': 'per_window',\n",
    "    'RF': 'last_window', 'GBRT_H': 'last_window',\n",
    "    'NN1': 'last_window', 'NN2': 'last_window', 'NN3': 'last_window',\n",
    "    'NN4': 'last_window', 'NN5': 'last_window',\n",
    "}\n",
    "\n",
    "# --- Complexity Plotting ---\n",
    "COMPLEXITY_PARAMS_TO_PLOT = {\n",
    "    'PLS': ['optim_n_components'], 'PCR': ['optim_n_components'],\n",
    "    'ENET': ['optim_alpha', 'optim_l1_ratio'], 'GLM_H': ['optim_alpha', 'optim_epsilon'],\n",
    "    'RF': ['optim_max_depth'],\n",
    "    'GBRT_H': ['optim_max_depth'],\n",
    "    'NN1': ['optim_lambda1', 'optim_learning_rate'], 'NN2': ['optim_lambda1', 'optim_learning_rate'],\n",
    "    'NN3': ['optim_lambda1', 'optim_learning_rate'], 'NN4': ['optim_lambda1', 'optim_learning_rate'],\n",
    "    'NN5': ['optim_lambda1', 'optim_learning_rate'],\n",
    "}\n",
    "\n",
    "# --- Seeds ---\n",
    "GENERAL_SEED = 42\n",
    "TF_SEED = 42\n",
    "\n",
    "# --- Create Output Directory ---\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Created output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"Configuration loaded from config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7074ec",
   "metadata": {},
   "source": [
    "# File 3: pipeline_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d660bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- pipeline_utils.py ---\n",
    "# Shared utility functions for the ML Asset Pricing Pipeline.\n",
    "# Contains functions for data loading/prep, feature definition,\n",
    "# standardization, cleaning, splitting, portfolio analysis, VI, plotting, saving.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from scipy.stats.mstats import winsorize # No longer needed here\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "# Import config AFTER it's defined\n",
    "import config\n",
    "\n",
    "# --- Suppress specific warnings ---\n",
    "# (Keep suppression settings as they are)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Mean of empty slice\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"invalid value encountered in log\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Maximum number of iterations reached.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"divide by zero encountered.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"invalid value encountered.*divide\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Set general random seed\n",
    "random.seed(config.GENERAL_SEED)\n",
    "np.random.seed(config.GENERAL_SEED)\n",
    "\n",
    "\n",
    "# === Stage 1: Data Loading and Preparation (SIMPLIFIED) ===\n",
    "def find_col(df, potential_names, default=None):\n",
    "    \"\"\"Helper to find the first matching column name from a list.\"\"\"\n",
    "    for name in potential_names:\n",
    "        if name in df.columns: return name\n",
    "    return default\n",
    "\n",
    "def load_prepare_data(file_path, column_config, target_var_name, next_ret_var_name, mkt_cap_orig_var_name):\n",
    "    \"\"\"\n",
    "    Loads the PREPROCESSED data file.\n",
    "    Performs minimal checks: date conversion, finds essential columns (Date, ID, Target, NextReturn, MktCapOrig).\n",
    "    Does NOT calculate returns, targets, log transforms, etc. as this is assumed done externally.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 1. Laster Forh√•ndsbehandlet Data ---\")\n",
    "    print(f\"Laster data fra: {file_path}\")\n",
    "    try:\n",
    "        # Load data, ensuring Date is parsed\n",
    "        df = pd.read_csv(file_path, parse_dates=['Date']) # Assume 'Date' is the date column\n",
    "        print(f\"Forh√•ndsbehandlet data lastet inn. Form: {df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FEIL: Fil '{file_path}' ikke funnet.\"); return None\n",
    "    except Exception as e:\n",
    "        print(f\"FEIL under lasting av forh√•ndsbehandlet data: {e}\"); return None\n",
    "\n",
    "    # --- 1. Find and Standardize Essential Column Names ---\n",
    "    std_names_map = {\n",
    "        'date': 'Date',\n",
    "        'id': 'Instrument',\n",
    "        # Add other potential mappings from config if needed, but keep it minimal\n",
    "    }\n",
    "    rename_dict = {}\n",
    "    found_cols = {}\n",
    "\n",
    "    # Find Date column\n",
    "    date_col_found = find_col(df, column_config.get('date', ['Date', 'date']))\n",
    "    if not date_col_found: print(\"FEIL: Datokolonne ikke funnet.\"); return None\n",
    "    if date_col_found != 'Date': rename_dict[date_col_found] = 'Date'\n",
    "    found_cols['date'] = 'Date'\n",
    "\n",
    "    # Find ID column\n",
    "    id_col_found = find_col(df, column_config.get('id', ['Instrument', 'instrument']))\n",
    "    if not id_col_found: print(\"FEIL: Instrument ID kolonne ikke funnet.\"); return None\n",
    "    if id_col_found != 'Instrument': rename_dict[id_col_found] = 'Instrument'\n",
    "    found_cols['id'] = 'Instrument'\n",
    "\n",
    "    # Apply renames if necessary\n",
    "    if rename_dict:\n",
    "        df = df.rename(columns=rename_dict)\n",
    "        print(f\"Standardiserte essensielle kolonner: {list(rename_dict.values())}\")\n",
    "\n",
    "    # --- 2. Verify Essential Columns Exist ---\n",
    "    essential_cols = ['Date', 'Instrument', target_var_name, next_ret_var_name, mkt_cap_orig_var_name]\n",
    "    # Check if sector column exists IF it's needed later (e.g., for dummies, although dummies should be pre-created now)\n",
    "    if 'EconomicSector' in column_config:\n",
    "         sector_col_cand = find_col(df, column_config['EconomicSector'])\n",
    "         if sector_col_cand: essential_cols.append(sector_col_cand)\n",
    "\n",
    "    missing_essential = [col for col in essential_cols if col not in df.columns]\n",
    "    if missing_essential:\n",
    "        print(f\"FEIL: Essensielle kolonner mangler i forh√•ndsbehandlet fil: {missing_essential}\")\n",
    "        print(f\"Tilgjengelige kolonner: {df.columns.tolist()}\")\n",
    "        return None\n",
    "    print(\"Essensielle kolonner funnet.\")\n",
    "\n",
    "    # --- 3. Ensure Date is Datetime and Sort ---\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
    "        try:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "        except Exception as e:\n",
    "            print(f\"FEIL: Kunne ikke konvertere 'Date' kolonne til datetime: {e}\"); return None\n",
    "    df = df.sort_values(by=['Instrument', 'Date']).reset_index(drop=True)\n",
    "    print(\"Data sortert etter Instrument og Dato.\")\n",
    "\n",
    "    # --- 4. Optional: Create Sector Dummies (if not already done in preprocessing) ---\n",
    "    # Check if sector column exists and if dummy columns DON'T already exist\n",
    "    sector_col_std = find_col(df, column_config.get('EconomicSector', []))\n",
    "    if sector_col_std and not any(col.startswith(\"Sector_\") for col in df.columns):\n",
    "         print(f\"  INFO: Oppretter Sektor dummy-variabler fra '{sector_col_std}'...\")\n",
    "         df = pd.get_dummies(df, columns=[sector_col_std], prefix=\"Sector\", dtype=int)\n",
    "         print(\"  Sektor dummy-variabler opprettet.\")\n",
    "\n",
    "    print(f\"Lasting og grunnleggende sjekk fullf√∏rt. Form: {df.shape}\")\n",
    "    # print(f\"Final Columns: {df.columns.tolist()}\") # Uncomment for detailed debug\n",
    "    return df\n",
    "\n",
    "\n",
    "# === Stage 2: Feature Definition ===\n",
    "def define_features(df, ols3_feature_names, base_exclusions):\n",
    "    \"\"\"\n",
    "    Identifies numeric features from the PREPROCESSED data,\n",
    "    excluding specified base columns (target, IDs, intermediate calcs that might remain).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 2. Definerer Features (fra forh√•ndsbehandlet data) ---\")\n",
    "    if df is None or df.empty: print(\" FEIL: DataFrame tom.\"); return [], [], []\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    print(f\"  Funnet {len(numeric_cols)} num. kolonner.\")\n",
    "\n",
    "    # Define columns to exclude from features\n",
    "    cols_to_exclude = set()\n",
    "\n",
    "    # Add columns explicitly passed for exclusion (target, next_ret, mkt_cap_orig, etc.)\n",
    "    base_exclusions_present = [col for col in base_exclusions if col in df.columns]\n",
    "    cols_to_exclude.update(base_exclusions_present)\n",
    "\n",
    "    # Add standard identifiers and intermediate calculation columns that might *still* be present\n",
    "    # (AdjustedReturn_t is likely still there from the preprocessor script)\n",
    "    date_col = 'Date' # Should be standardized now\n",
    "    id_col = 'Instrument' # Should be standardized now\n",
    "    std_exclusions = [date_col, id_col, 'level_0','index','Year','MonthYear',\n",
    "                      'AdjustedReturn_t', # Keep this exclusion\n",
    "                      'MonthlyReturn_t', 'MonthlyRiskFreeRate_t'] # Keep these\n",
    "    cols_to_exclude.update([col for col in std_exclusions if col in df.columns])\n",
    "\n",
    "    # Add portfolio helper columns if they might exist (unlikely now)\n",
    "    pf_cols=['Rank','DecileRank','Decile','ew_weights','vw_weights','rank']\n",
    "    cols_to_exclude.update([c for c in pf_cols if c in df.columns])\n",
    "\n",
    "    # Exclude original versions of logged variables IF the log version exists\n",
    "    # (This check remains useful, e.g., excludes MarketCap if log_MarketCap exists)\n",
    "    log_cols = {c for c in numeric_cols if c.startswith('log_')}\n",
    "    # List potential raw names based on how log names are created (log_VARNAME)\n",
    "    raw_names_from_logs = {c.replace('log_', '') for c in log_cols}\n",
    "    for raw_name in raw_names_from_logs:\n",
    "        if raw_name in df.columns and f\"log_{raw_name}\" in log_cols:\n",
    "            cols_to_exclude.add(raw_name)\n",
    "            # Specific exclusions if MarketCap or ClosePrice were logged\n",
    "            if raw_name in ['MarketCap', 'ClosePrice'] and 'ClosePrice' in df.columns:\n",
    "                 cols_to_exclude.add('ClosePrice') # Exclude if log exists\n",
    "            if raw_name == 'MarketCap' and 'CommonSharesOutstanding' in df.columns:\n",
    "                 cols_to_exclude.add('CommonSharesOutstanding') # Exclude if log exists\n",
    "\n",
    "    # Exclude raw NorgesBank10Y, NIBOR3M if TermSpread exists\n",
    "    if 'TermSpread' in df.columns:\n",
    "         if 'NorgesBank10Y' in df.columns: cols_to_exclude.add('NorgesBank10Y')\n",
    "         if 'NIBOR3M' in df.columns: cols_to_exclude.add('NIBOR3M')\n",
    "\n",
    "    # Identify final numeric features\n",
    "    potential_features = [c for c in numeric_cols if c not in cols_to_exclude]\n",
    "    final_features = []\n",
    "    for col in potential_features:\n",
    "        if col not in df.columns: continue\n",
    "        valid = df[col].dropna()\n",
    "        # Check for variance and multiple unique values\n",
    "        # Use a slightly larger tolerance for std dev check with float32\n",
    "        if len(valid) > 1 and valid.nunique() > 1 and valid.std() > 1e-7:\n",
    "            final_features.append(col)\n",
    "        # else: print(f\"  -> Dropping potential feature '{col}' due to no variance or <=1 unique value.\")\n",
    "\n",
    "    final_features = sorted(list(set(final_features)))\n",
    "    print(f\"  Identifisert {len(final_features)} features totalt etter ekskludering og validitetssjekk.\")\n",
    "    print(f\"  Features sample: {final_features[:5]}...{final_features[-5:]}\") # Show sample\n",
    "    # print(f\"  Ekskluderte kolonner: {sorted(list(cols_to_exclude.intersection(df.columns)))}\") # Optional Debug\n",
    "\n",
    "    # Check OLS3 features against the *final* feature list\n",
    "    # Use the names specified in config.OLS3_FEATURE_NAMES directly\n",
    "    ols3_features_final = [f for f in ols3_feature_names if f in final_features]\n",
    "    missing_ols3 = [f for f in ols3_feature_names if f not in ols3_features_final]\n",
    "\n",
    "    if missing_ols3: print(f\"  ADVARSEL: OLS3 mangler features fra config: {missing_ols3}\")\n",
    "    if not ols3_features_final: print(\"  ADVARSEL: Ingen av de spesifiserte OLS3 features er gyldige endelige features.\")\n",
    "    elif len(ols3_features_final) < len(ols3_feature_names): print(f\"  ADVARSEL: Kunne finne deler av OLS3 features: {ols3_features_final}\")\n",
    "    else: print(f\"  Valide OLS3 features funnet: {ols3_features_final}\")\n",
    "\n",
    "    # Return all valid features, valid OLS3 features, and all valid features again\n",
    "    all_needed_final = sorted(list(set(final_features)))\n",
    "    return all_needed_final, ols3_features_final, all_needed_final\n",
    "\n",
    "\n",
    "# === Stage 3: Standardization ===\n",
    "# (Keep rank_standardize_features as is - it operates on the defined features)\n",
    "def rank_standardize_features(df, features_to_standardize):\n",
    "    print(\"\\n--- 3. Rank Standardiserer Features ---\");\n",
    "    date_col = 'Date' # Assumes Date column exists and is named 'Date'\n",
    "    if date_col not in df.columns: print(f\"FEIL: Datokolonne ('{date_col}') mangler for standardisering.\"); return df\n",
    "    features=[f for f in features_to_standardize if f in df.columns];\n",
    "    if not features: print(\"  Ingen features funnet √• standardisere.\"); return df\n",
    "    print(f\"Standardiserer {len(features)} features...\")\n",
    "    def rank_transform(x):\n",
    "        x_num=pd.to_numeric(x,errors='coerce')\n",
    "        if x_num.isnull().all(): return x_num # Return NaNs if all are NaN\n",
    "        # Rank, converting ranks to [-1, 1] range\n",
    "        r=x_num.rank(pct=True, na_option='keep')\n",
    "        # Fill remaining NaNs (e.g., from single non-NaN value) with 0 AFTER scaling\n",
    "        return (r * 2 - 1).fillna(0)\n",
    "\n",
    "    try:\n",
    "        # Group by Date and apply the rank transform to each feature column\n",
    "        # Using transform should be efficient\n",
    "        df[features] = df.groupby(date_col)[features].transform(rank_transform)\n",
    "    except Exception as e:\n",
    "        print(f\" ADVARSEL under transform (pr√∏ver apply): {e}. Standardisering kan v√¶re ufullstendig.\");\n",
    "        # Fallback might be needed for complex cases, but transform is preferred\n",
    "        try:\n",
    "            df_s = df.set_index(date_col)\n",
    "            for col in features:\n",
    "                 df_s[col] = df_s.groupby(level=0)[col].apply(rank_transform)\n",
    "            df = df_s.reset_index() # Bring Date back as a column\n",
    "        except Exception as e2:\n",
    "             print(f\" FEIL under apply: {e2}. Standardisering kan v√¶re ufullstendig.\"); return df # Return potentially partially processed df\n",
    "\n",
    "    print(\"Rank standardisering fullf√∏rt.\"); return df\n",
    "\n",
    "\n",
    "# === Stage 4: Data Cleaning (Post-Standardization) ===\n",
    "def clean_data(df, numeric_features_to_impute, essential_cols_for_dropna, mkt_cap_orig_var):\n",
    "    \"\"\"\n",
    "    Cleans data AFTER standardization.\n",
    "    1. Replaces inf with NaN in features.\n",
    "    2. Imputes NaN in FEATURE columns using the overall median.\n",
    "    3. Drops rows with NaN in ESSENTIAL columns (target, IDs, next_ret, mkt_cap_orig).\n",
    "    4. Drops rows with non-positive original market cap.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 4. Renser Data (Post-Standardisering) ---\"); initial_rows=len(df)\n",
    "    features=[f for f in numeric_features_to_impute if f in df.columns];\n",
    "\n",
    "    if features:\n",
    "        # Replace inf values first within feature columns\n",
    "        inf_mask = df[features].isin([np.inf, -np.inf])\n",
    "        if inf_mask.any().any():\n",
    "            inf_cols = df[features].columns[inf_mask.any(axis=0)].tolist()\n",
    "            print(f\"  Erstatter +/-inf med NaN i features: {inf_cols}...\")\n",
    "            df[features] = df[features].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # Impute NaNs in FEATURES with OVERALL MEDIAN (robust to outliers after standardization)\n",
    "        # Calculate medians ONLY for the feature columns that have NaNs\n",
    "        cols_with_nan = df[features].isnull().any()\n",
    "        features_to_impute_now = cols_with_nan[cols_with_nan].index.tolist()\n",
    "\n",
    "        if features_to_impute_now:\n",
    "            print(f\"  Imputerer NaNs i {len(features_to_impute_now)} features med overall median...\")\n",
    "            medians = df[features_to_impute_now].median(skipna=True) # Calculate median for each feature column\n",
    "\n",
    "            # Fill NaNs using the calculated medians\n",
    "            df[features_to_impute_now] = df[features_to_impute_now].fillna(medians)\n",
    "\n",
    "            # If median itself is NaN (e.g., all NaNs in a column), fill remaining NaNs with 0\n",
    "            if medians.isnull().any():\n",
    "                cols_nan_median = medians[medians.isnull()].index.tolist()\n",
    "                print(f\"  ADVARSEL: Median var NaN for features: {cols_nan_median}. Fyller resterende NaNs i disse kolonnene med 0.\")\n",
    "                df[cols_nan_median] = df[cols_nan_median].fillna(0)\n",
    "            print(f\"  NaNs i features imputert.\")\n",
    "        else:\n",
    "            print(\"  Ingen NaNs funnet i features som trenger imputering.\")\n",
    "\n",
    "    # Drop rows with NaNs in ESSENTIAL columns (target, IDs, next return, original market cap)\n",
    "    # These columns should have been created by the preprocessing script.\n",
    "    essentials_present = [c for c in essential_cols_for_dropna if c in df.columns]\n",
    "    if essentials_present:\n",
    "        rows0 = len(df)\n",
    "        df = df.dropna(subset=essentials_present)\n",
    "        dropped_count = rows0 - len(df)\n",
    "        if dropped_count > 0:\n",
    "            print(f\"  Fjernet {dropped_count} rader pga NaN i essensielle kolonner: {essentials_present}.\")\n",
    "        else:\n",
    "            print(f\"  Ingen rader fjernet pga NaN i essensielle kolonner ({essentials_present}).\")\n",
    "    else:\n",
    "         print(f\"  ADVARSEL: Kunne ikke sjekke NaN i essensielle kolonner (mangler): {[c for c in essential_cols_for_dropna if c not in df.columns]}\")\n",
    "\n",
    "\n",
    "    # Drop rows where original market cap is non-positive (already done in preprocessor, but good safety check)\n",
    "    if mkt_cap_orig_var in df.columns:\n",
    "        rows0 = len(df)\n",
    "        df = df[df[mkt_cap_orig_var] > 0]\n",
    "        dropped_count = rows0 - len(df)\n",
    "        if dropped_count > 0:\n",
    "            print(f\"  Fjernet {dropped_count} rader der '{mkt_cap_orig_var}' <= 0 (sikkerhetssjekk).\")\n",
    "    else:\n",
    "        print(f\"ADVARSEL: Kolonne '{mkt_cap_orig_var}' ikke funnet for sjekk av positiv verdi.\")\n",
    "\n",
    "    print(f\"Datarensing ferdig. Form: {df.shape}. Totalt fjernet {initial_rows-len(df)} rader i dette steget.\");\n",
    "    if df.empty: print(\"FEIL: DataFrame er tom etter rensing.\"); return None\n",
    "    return df\n",
    "\n",
    "\n",
    "# === Stage 5: Data Splitting ===\n",
    "# (Keep get_yearly_rolling_splits as is)\n",
    "def get_yearly_rolling_splits(df, initial_train_years, val_years, test_years):\n",
    "    print(\"\\n--- 5. Setter opp √Örlige Rullerende Vinduer ---\")\n",
    "    date_col = 'Date' # Assumes standard name\n",
    "    if date_col not in df.columns: raise ValueError(f\"'{date_col}' kolonnen mangler for splitting.\")\n",
    "\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
    "         try: df[date_col] = pd.to_datetime(df[date_col])\n",
    "         except Exception as e: raise ValueError(f\"Kunne ikke konvertere '{date_col}' til datetime: {e}\")\n",
    "\n",
    "    df['Year']=df[date_col].dt.year; unique_years=sorted(df[\"Year\"].unique()); n_years=len(unique_years)\n",
    "    print(f\"Funnet {n_years} unike √•r i data ({unique_years[0]}-{unique_years[-1]})\")\n",
    "\n",
    "    min_years_needed = initial_train_years + val_years + test_years\n",
    "    if n_years < min_years_needed:\n",
    "        df.drop(columns=['Year'],inplace=True,errors='ignore'); # Clean up temp column\n",
    "        raise ValueError(f\"Ikke nok √•r ({n_years}) for den spesifiserte splitten (trenger minst {min_years_needed}).\")\n",
    "\n",
    "    first_test_year_idx = initial_train_years + val_years\n",
    "    if first_test_year_idx >= n_years:\n",
    "        df.drop(columns=['Year'],inplace=True,errors='ignore');\n",
    "        raise ValueError(f\"Kombinasjonen av initial_train ({initial_train_years}) og validation ({val_years}) dekker alle ({n_years}) eller flere √•r. Ingen test√•r igjen.\")\n",
    "\n",
    "    first_test_year = unique_years[first_test_year_idx]\n",
    "    last_test_start_year = unique_years[n_years - test_years]\n",
    "    num_windows = last_test_start_year - first_test_year + 1\n",
    "\n",
    "    if num_windows <= 0:\n",
    "        df.drop(columns=['Year'],inplace=True,errors='ignore');\n",
    "        raise ValueError(f\"Negativt eller null antall vinduer beregnet ({num_windows}). Sjekk √•rskonfigurasjon. First test year: {first_test_year}, Last possible test start year: {last_test_start_year}\")\n",
    "\n",
    "    print(f\"Genererer {num_windows} rullerende vinduer.\")\n",
    "    print(f\"  F√∏rste vindu test√•r: {first_test_year} (slutter {first_test_year+test_years-1})\")\n",
    "    print(f\"  Siste vindu test√•r: {last_test_start_year} (slutter {last_test_start_year+test_years-1})\")\n",
    "\n",
    "    splits_info=[] # Store tuples of (train_idx, val_idx, test_idx, train_dates, val_dates, test_dates)\n",
    "    for i in range(num_windows):\n",
    "        test_start_year = first_test_year + i\n",
    "        test_end_year = test_start_year + test_years - 1\n",
    "        val_end_year = test_start_year - 1\n",
    "        val_start_year = val_end_year - val_years + 1\n",
    "        train_end_year = val_start_year - 1\n",
    "        train_start_year = unique_years[0] # Train from the beginning\n",
    "\n",
    "        train_indices = df[(df['Year'] >= train_start_year) & (df['Year'] <= train_end_year)].index\n",
    "        val_indices = df[(df['Year'] >= val_start_year) & (df['Year'] <= val_end_year)].index\n",
    "        test_indices = df[(df['Year'] >= test_start_year) & (df['Year'] <= test_end_year)].index\n",
    "\n",
    "        train_dates = df.loc[train_indices, date_col].agg(['min','max']) if not train_indices.empty else None\n",
    "        val_dates = df.loc[val_indices, date_col].agg(['min','max']) if not val_indices.empty else None\n",
    "        test_dates = df.loc[test_indices, date_col].agg(['min','max']) if not test_indices.empty else None\n",
    "\n",
    "        splits_info.append((\n",
    "            train_indices, val_indices, test_indices,\n",
    "            train_dates, val_dates, test_dates,\n",
    "            train_start_year, train_end_year, val_start_year, val_end_year, test_start_year, test_end_year\n",
    "        ))\n",
    "\n",
    "    print(\"\\n--- Split Detaljer per Vindu ---\")\n",
    "    for i,split_data in enumerate(splits_info):\n",
    "        tr_idx, v_idx, te_idx, tr_d, v_d, t_d, tr_s, tr_e, v_s, v_e, t_s, t_e = split_data\n",
    "        print(f\"  Vindu {i+1}/{num_windows}:\")\n",
    "        print(f\"    Train: {tr_s}-{tr_e} ({len(tr_idx)} obs) [{tr_d['min'].date() if tr_d is not None else 'N/A'} -> {tr_d['max'].date() if tr_d is not None else 'N/A'}]\")\n",
    "        print(f\"    Val:   {v_s}-{v_e} ({len(v_idx)} obs) [{v_d['min'].date() if v_d is not None else 'N/A'} -> {v_d['max'].date() if v_d is not None else 'N/A'}]\")\n",
    "        print(f\"    Test:  {t_s}-{t_e} ({len(te_idx)} obs) [{t_d['min'].date() if t_d is not None else 'N/A'} -> {t_d['max'].date() if t_d is not None else 'N/A'}]\")\n",
    "        yield tr_idx, v_idx, te_idx, tr_d, v_d, t_d # Yield indices and date ranges\n",
    "\n",
    "    df.drop(columns=['Year'],inplace=True,errors='ignore')\n",
    "\n",
    "\n",
    "# === Stage 6: Model Evaluation Metrics ===\n",
    "# (Keep calculate_oos_r2 and calculate_sharpe_of_predictions as is)\n",
    "def calculate_oos_r2(y_true, y_pred):\n",
    "    \"\"\" Calculates OOS R2 based on Gu, Kelly, Xiu (2020) definition: 1 - SSR/SST0. \"\"\"\n",
    "    if y_true is None or y_pred is None: return np.nan\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    if len(y_true) < 2 or len(y_pred) < 2 or len(y_true) != len(y_pred): return np.nan\n",
    "\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    y_t = y_true[mask]; y_p = y_pred[mask]\n",
    "\n",
    "    if len(y_t) < 2: return np.nan\n",
    "    ss_res = np.sum((y_t - y_p)**2)\n",
    "    ss_tot = np.sum(y_t**2) # SST0\n",
    "\n",
    "    if ss_tot < 1e-15:\n",
    "        return 1.0 if ss_res < 1e-15 else np.nan\n",
    "    return 1.0 - (ss_res / ss_tot)\n",
    "\n",
    "def calculate_sharpe_of_predictions(y_pred, annualization_factor=12):\n",
    "    \"\"\" Calculates annualized Sharpe ratio of the *predictions* themselves. \"\"\"\n",
    "    if y_pred is None: return np.nan\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if len(y_pred) < 2: return np.nan\n",
    "\n",
    "    mask = np.isfinite(y_pred)\n",
    "    y_p = y_pred[mask]\n",
    "    if len(y_p) < 2: return np.nan\n",
    "    mean_pred = np.mean(y_p)\n",
    "    std_pred = np.std(y_p)\n",
    "    if std_pred < 1e-9: return np.nan\n",
    "    return (mean_pred / std_pred) * np.sqrt(annualization_factor)\n",
    "\n",
    "\n",
    "# === Stage 7: Portfolio Analysis ===\n",
    "# (Keep perform_detailed_portfolio_analysis mostly as is, but ensure column names passed are correct)\n",
    "# --> Key change: The `original_df_subset` argument will now be the main `df_clean` DataFrame\n",
    "#     loaded by the pipeline, which contains the preprocessed data including\n",
    "#     NEXT_RETURN_VARIABLE and MARKET_CAP_ORIG_VARIABLE created by `preprocess_data.py`.\n",
    "def MDD(returns):\n",
    "    \"\"\" Calculates Maximum Drawdown from a pandas Series of returns. \"\"\"\n",
    "    returns = pd.Series(returns).fillna(0) # Ensure it's a series and fill NaNs with 0\n",
    "    if returns.empty or len(returns) < 2: return np.nan\n",
    "\n",
    "    cumulative_returns = (1 + returns).cumprod()\n",
    "    peak = cumulative_returns.cummax()\n",
    "    drawdown = (cumulative_returns / peak) - 1\n",
    "    max_drawdown = drawdown.min() # MDD is the minimum value in the drawdown series\n",
    "\n",
    "    return max_drawdown * 100 if pd.notna(max_drawdown) else np.nan\n",
    "\n",
    "def perform_detailed_portfolio_analysis(results_df, # Contains Date, Instrument, yhat_*\n",
    "                                        full_preprocessed_df, # Contains Date, Instrument, Target, NextRawReturn, MktCqpOrig etc.\n",
    "                                        prediction_cols,\n",
    "                                        mkt_cap_orig_var, # Name of the original market cap column\n",
    "                                        next_ret_var,     # Name of the NEXT month's RAW return column\n",
    "                                        # monthly_rf_var, # Not strictly needed if using excess returns from results_df\n",
    "                                        filter_small_caps=False, annualization_factor=12,\n",
    "                                        benchmark_file=None, ff_factor_file=None):\n",
    "    print(\"\\n--- 7. Detaljert Portef√∏ljeanalyse (Desiler) ---\")\n",
    "    if filter_small_caps: print(\">>> Filtrering av sm√• selskaper (basert p√• market cap ved t) er AKTIVERT for portef√∏ljedanning <<<\")\n",
    "\n",
    "    # --- Input Data Validation ---\n",
    "    date_col = 'Date' # Standard name\n",
    "    id_col = 'Instrument' # Standard name\n",
    "    target_var = config.TARGET_VARIABLE # From config, should match results_df\n",
    "\n",
    "    required_orig = [date_col, id_col, mkt_cap_orig_var, next_ret_var] # Core needs from preprocessed data\n",
    "    missing_orig = [c for c in required_orig if c not in full_preprocessed_df.columns]\n",
    "    if missing_orig: print(f\"FEIL: Mangler p√•krevde kolonner i full_preprocessed_df: {missing_orig}.\"); return {},{},{}\n",
    "\n",
    "    required_res = [date_col, id_col, target_var] + prediction_cols\n",
    "    missing_res = [c for c in required_res if c not in results_df.columns]\n",
    "    if missing_res: print(f\"FEIL: Mangler p√•krevde kolonner i results_df: {missing_res}.\"); return {},{},{}\n",
    "\n",
    "    # --- Data Preparation & Merging ---\n",
    "    print(\"Forbereder data for portef√∏ljeanalyse...\")\n",
    "    # Ensure correct dtypes and standard columns before merge\n",
    "    results_df[date_col] = pd.to_datetime(results_df[date_col])\n",
    "    results_df[id_col] = results_df[id_col].astype(str)\n",
    "    full_preprocessed_df[date_col] = pd.to_datetime(full_preprocessed_df[date_col])\n",
    "    full_preprocessed_df[id_col] = full_preprocessed_df[id_col].astype(str)\n",
    "\n",
    "    # Select necessary columns from the full preprocessed data\n",
    "    # We need Date, Instrument, the original market cap (t), and the next raw return (t+1)\n",
    "    df_orig_sub = full_preprocessed_df[required_orig].drop_duplicates(subset=[date_col, id_col], keep='first')\n",
    "\n",
    "    # Select necessary columns from the results (predictions at t for portfolio formed at t)\n",
    "    results_sub = results_df[[date_col, id_col] + prediction_cols + [target_var]].drop_duplicates(subset=[date_col, id_col], keep='first')\n",
    "\n",
    "    # Merge predictions (at time t) with market cap (at t) and NEXT month's return (t+1)\n",
    "    portfolio_data = pd.merge(results_sub, df_orig_sub, on=[date_col, id_col], how='inner')\n",
    "    print(f\"Data for analyse etter merge: {portfolio_data.shape}\")\n",
    "\n",
    "    # Rename columns for clarity in portfolio context\n",
    "    # me = market equity (market cap) at time t\n",
    "    # ret_t+1 = raw return realized in month t+1 (obtained from preprocessed data)\n",
    "    # target = target variable (excess return t+1, used for sorting checks maybe, but ret_t+1 used for perf)\n",
    "    portfolio_data = portfolio_data.rename(columns={\n",
    "        mkt_cap_orig_var: 'me',\n",
    "        next_ret_var: 'ret_t+1',\n",
    "        target_var: 'target_ret_t+1' # Rename target for clarity\n",
    "    })\n",
    "\n",
    "    # --- Calculate Excess Returns (t+1) for analysis ---\n",
    "    # We need the risk-free rate corresponding to the ret_t+1 period.\n",
    "    # The easiest way is to get it from the target_ret_t+1 calculation:\n",
    "    # target_ret_t+1 = ret_t+1 - rf_t+1  =>  rf_t+1 = ret_t+1 - target_ret_t+1\n",
    "    if 'ret_t+1' in portfolio_data.columns and 'target_ret_t+1' in portfolio_data.columns:\n",
    "         portfolio_data['rf_t+1'] = portfolio_data['ret_t+1'] - portfolio_data['target_ret_t+1']\n",
    "         # Calculate excess return for t+1 (should be very close to target_ret_t+1, good check)\n",
    "         portfolio_data['excess_ret_t+1'] = portfolio_data['ret_t+1'] - portfolio_data['rf_t+1']\n",
    "         print(\"  Beregnet excess_ret_t+1 for portef√∏ljeanalyse.\")\n",
    "    else:\n",
    "         print(\"FEIL: Kan ikke beregne excess_ret_t+1 - mangler ret_t+1 eller target_ret_t+1.\")\n",
    "         return {}, {}, {}\n",
    "\n",
    "\n",
    "    # --- Data Cleaning Post-Merge ---\n",
    "    # Critical columns needed for decile sorts and return calculations\n",
    "    crit_cols = prediction_cols + ['excess_ret_t+1', 'ret_t+1', 'me'] # rf_t+1 not strictly needed if excess_ret exists\n",
    "    initial_rows = len(portfolio_data)\n",
    "    portfolio_data = portfolio_data.dropna(subset=crit_cols)\n",
    "    # Ensure valid market cap for weighting and potential filtering\n",
    "    portfolio_data = portfolio_data[portfolio_data['me'] > 0]\n",
    "    rows_removed = initial_rows - len(portfolio_data)\n",
    "    if rows_removed > 0:\n",
    "        print(f\"  Fjernet {rows_removed} rader pga NaNs i kritiske kolonner ({crit_cols}) eller me <= 0.\")\n",
    "    if portfolio_data.empty: print(\"FEIL: Ingen gyldige data igjen etter sammensl√•ing og rensing.\"); return {},{},{}\n",
    "\n",
    "    # Use Month-Year Period for grouping\n",
    "    portfolio_data['MonthYear'] = portfolio_data[date_col].dt.to_period('M')\n",
    "\n",
    "    # --- Decile Sorting and Monthly Returns Calculation ---\n",
    "    # (The rest of this function remains largely the same, as it operates on the prepared portfolio_data)\n",
    "    all_monthly_results = []\n",
    "    monthly_weights_all = [] # To store weights for turnover calculation\n",
    "    model_names_processed = [] # Track models actually processed\n",
    "    hl_monthly_dfs_plotting = {} # Store H-L returns for plotting\n",
    "    long_monthly_dfs_plotting = {} # Store Long-only (D10) returns for plotting\n",
    "\n",
    "    unique_months = sorted(portfolio_data['MonthYear'].unique())\n",
    "    print(f\"Itererer gjennom {len(unique_months)} m√•neder for desil-sortering...\")\n",
    "\n",
    "    for month in unique_months:\n",
    "        monthly_data_full = portfolio_data[portfolio_data['MonthYear'] == month].copy()\n",
    "\n",
    "        if filter_small_caps:\n",
    "            mc_cutoff = monthly_data_full['me'].quantile(config.SMALL_FIRM_BOTTOM_PERCENT / 100.0)\n",
    "            monthly_data_filtered = monthly_data_full[monthly_data_full['me'] > mc_cutoff].copy()\n",
    "            if monthly_data_filtered.empty and not monthly_data_full.empty: continue\n",
    "            elif len(monthly_data_filtered) < 10: continue\n",
    "            else: monthly_data = monthly_data_filtered\n",
    "        else:\n",
    "            monthly_data = monthly_data_full\n",
    "\n",
    "        if len(monthly_data) < 10: continue\n",
    "\n",
    "        for model_pred_col in prediction_cols:\n",
    "            model_name = model_pred_col.replace('yhat_', '').upper().replace('_', '-')\n",
    "            if model_name not in model_names_processed: model_names_processed.append(model_name)\n",
    "\n",
    "            monthly_data_model = monthly_data.dropna(subset=[model_pred_col]).copy()\n",
    "            if len(monthly_data_model) < 10: continue\n",
    "\n",
    "            monthly_data_model['Rank'] = monthly_data_model[model_pred_col].rank(method='first')\n",
    "            try:\n",
    "                monthly_data_model['Decile'] = pd.qcut(monthly_data_model['Rank'], 10, labels=False, duplicates='drop') + 1\n",
    "            except ValueError: continue\n",
    "\n",
    "            if monthly_data_model['Decile'].nunique() < 2: continue\n",
    "\n",
    "            monthly_data_model['ew_weights'] = 1 / monthly_data_model.groupby('Decile')[id_col].transform('size')\n",
    "            mc_sum_decile = monthly_data_model.groupby('Decile')['me'].transform('sum')\n",
    "            monthly_data_model['vw_weights'] = np.where(mc_sum_decile > 1e-9, monthly_data_model['me'] / mc_sum_decile, 0)\n",
    "\n",
    "            # Use 'excess_ret_t+1' and 'ret_t+1' directly now\n",
    "            monthly_data_model['ew_excess_ret'] = monthly_data_model['excess_ret_t+1'] * monthly_data_model['ew_weights']\n",
    "            monthly_data_model['vw_excess_ret'] = monthly_data_model['excess_ret_t+1'] * monthly_data_model['vw_weights']\n",
    "            monthly_data_model['ew_raw_ret'] = monthly_data_model['ret_t+1'] * monthly_data_model['ew_weights']\n",
    "            monthly_data_model['vw_raw_ret'] = monthly_data_model['ret_t+1'] * monthly_data_model['vw_weights']\n",
    "            monthly_data_model['ew_pred_ret'] = monthly_data_model[model_pred_col] * monthly_data_model['ew_weights']\n",
    "            monthly_data_model['vw_pred_ret'] = monthly_data_model[model_pred_col] * monthly_data_model['vw_weights']\n",
    "\n",
    "            weights_m = monthly_data_model[[id_col, 'Decile', 'ew_weights', 'vw_weights']].copy()\n",
    "            weights_m['Model'] = model_name\n",
    "            weights_m['MonthYear'] = month\n",
    "            monthly_weights_all.append(weights_m)\n",
    "\n",
    "            agg_results = monthly_data_model.groupby('Decile').agg(\n",
    "                ew_excess_ret = ('ew_excess_ret', 'sum'),\n",
    "                vw_excess_ret = ('vw_excess_ret', 'sum'),\n",
    "                ew_raw_ret = ('ew_raw_ret', 'sum'),\n",
    "                vw_raw_ret = ('vw_raw_ret', 'sum'),\n",
    "                ew_pred_ret = ('ew_pred_ret', 'sum'),\n",
    "                vw_pred_ret = ('vw_pred_ret', 'sum'),\n",
    "                n_stocks = (id_col, 'size')\n",
    "            ).reset_index()\n",
    "\n",
    "            agg_results['MonthYear'] = month\n",
    "            agg_results['Model'] = model_name\n",
    "            all_monthly_results.append(agg_results)\n",
    "\n",
    "    if not all_monthly_results: print(\"FEIL: Ingen m√•nedsresultater ble generert.\"); return {},{},{}\n",
    "\n",
    "    # --- Combine Monthly Results and Calculate Turnover ---\n",
    "    # (Turnover calculation logic remains the same)\n",
    "    combined_results_df = pd.concat(all_monthly_results).reset_index(drop=True)\n",
    "    turnover_results = defaultdict(lambda: {'ew': np.nan, 'vw': np.nan, 'long_ew': np.nan, 'long_vw': np.nan})\n",
    "\n",
    "    if monthly_weights_all:\n",
    "        all_weights_df = pd.concat(monthly_weights_all).sort_values(['Model', 'MonthYear', id_col])\n",
    "        print(f\"\\nBeregner portef√∏ljeomsetning (turnover) for {len(model_names_processed)} modeller...\")\n",
    "        for mn in model_names_processed:\n",
    "            model_weights = all_weights_df[all_weights_df['Model'] == mn].copy()\n",
    "            if model_weights.empty: continue\n",
    "\n",
    "            # H-L Turnover\n",
    "            long_weights = model_weights[model_weights['Decile'] == 10]\n",
    "            short_weights = model_weights[model_weights['Decile'] == 1].assign(ew_weights=lambda x: -x.ew_weights, vw_weights=lambda x: -x.vw_weights)\n",
    "            hl_weights = pd.concat([long_weights, short_weights]).sort_values([id_col, 'MonthYear'])\n",
    "            hl_weights['ew_w_next'] = hl_weights.groupby(id_col)['ew_weights'].shift(-1).fillna(0)\n",
    "            hl_weights['vw_w_next'] = hl_weights.groupby(id_col)['vw_weights'].shift(-1).fillna(0)\n",
    "            hl_weights['trade_ew'] = abs(hl_weights['ew_w_next'] - hl_weights['ew_weights'])\n",
    "            hl_weights['trade_vw'] = abs(hl_weights['vw_w_next'] - hl_weights['vw_weights'])\n",
    "            last_month_hl = hl_weights['MonthYear'].max()\n",
    "            monthly_turnover_hl = hl_weights[hl_weights['MonthYear'] != last_month_hl].groupby('MonthYear').agg(sum_trade_ew=('trade_ew', 'sum'), sum_trade_vw=('trade_vw', 'sum'))\n",
    "            if not monthly_turnover_hl.empty:\n",
    "                turnover_results[mn]['ew'] = monthly_turnover_hl['sum_trade_ew'].mean() / 2\n",
    "                turnover_results[mn]['vw'] = monthly_turnover_hl['sum_trade_vw'].mean() / 2\n",
    "\n",
    "            # Long-Only Turnover\n",
    "            long_only_weights = model_weights[model_weights['Decile'] == 10].sort_values([id_col, 'MonthYear'])\n",
    "            if not long_only_weights.empty:\n",
    "                 long_only_weights['ew_w_next'] = long_only_weights.groupby(id_col)['ew_weights'].shift(-1).fillna(0)\n",
    "                 long_only_weights['vw_w_next'] = long_only_weights.groupby(id_col)['vw_weights'].shift(-1).fillna(0)\n",
    "                 long_only_weights['trade_ew'] = abs(long_only_weights['ew_w_next'] - long_only_weights['ew_weights'])\n",
    "                 long_only_weights['trade_vw'] = abs(long_only_weights['vw_w_next'] - long_only_weights['vw_weights'])\n",
    "                 last_month_lo = long_only_weights['MonthYear'].max()\n",
    "                 monthly_turnover_lo = long_only_weights[long_only_weights['MonthYear'] != last_month_lo].groupby('MonthYear').agg(sum_trade_ew=('trade_ew', 'sum'), sum_trade_vw=('trade_vw', 'sum'))\n",
    "                 if not monthly_turnover_lo.empty:\n",
    "                     turnover_results[mn]['long_ew'] = monthly_turnover_lo['sum_trade_ew'].mean() / 2\n",
    "                     turnover_results[mn]['long_vw'] = monthly_turnover_lo['sum_trade_vw'].mean() / 2\n",
    "        print(\"Omsetningsberegning fullf√∏rt.\")\n",
    "    else: print(\"Advarsel: Ingen vektdata funnet, kan ikke beregne omsetning.\")\n",
    "\n",
    "    # --- Aggregate Performance and Generate Tables/Plots ---\n",
    "    # (Calculation and formatting logic remains the same)\n",
    "    decile_tables = {}\n",
    "    hl_risk_tables = {}\n",
    "    long_risk_tables = {}\n",
    "    performance_summary_list = []\n",
    "\n",
    "    print(f\"\\nGenererer ytelsestabeller for {len(model_names_processed)} modeller...\")\n",
    "    for model_name in model_names_processed:\n",
    "        model_results = combined_results_df[combined_results_df['Model'] == model_name].copy()\n",
    "        if model_results.empty: continue\n",
    "\n",
    "        # Decile Performance\n",
    "        decile_perf = model_results.groupby('Decile').agg(\n",
    "            ew_pred_mean=('ew_pred_ret', 'mean'), vw_pred_mean=('vw_pred_ret', 'mean'),\n",
    "            ew_excess_mean=('ew_excess_ret', 'mean'), vw_excess_mean=('vw_excess_ret', 'mean'),\n",
    "            ew_raw_std=('ew_raw_ret', 'std'), vw_raw_std=('vw_raw_ret', 'std'), # Use raw std for SR\n",
    "            n_months=('MonthYear', 'nunique'), avg_stocks=('n_stocks','mean')\n",
    "        ).reset_index()\n",
    "        decile_perf['ew_sharpe'] = (decile_perf['ew_excess_mean'] / decile_perf['ew_raw_std']) * np.sqrt(annualization_factor)\n",
    "        decile_perf['vw_sharpe'] = (decile_perf['vw_excess_mean'] / decile_perf['vw_raw_std']) * np.sqrt(annualization_factor)\n",
    "\n",
    "        # H-L Performance\n",
    "        hl_stats_df = pd.DataFrame(); hl_monthly = pd.DataFrame()\n",
    "        if 1 in model_results['Decile'].values and 10 in model_results['Decile'].values:\n",
    "            long_monthly = model_results[model_results['Decile'] == 10].set_index('MonthYear')\n",
    "            short_monthly = model_results[model_results['Decile'] == 1].set_index('MonthYear')\n",
    "            common_index = long_monthly.index.intersection(short_monthly.index)\n",
    "            if not common_index.empty:\n",
    "                hl_monthly = pd.DataFrame({\n",
    "                    'ew_excess_ret_HL': long_monthly.loc[common_index, 'ew_excess_ret'].sub(short_monthly.loc[common_index, 'ew_excess_ret'], fill_value=0),\n",
    "                    'vw_excess_ret_HL': long_monthly.loc[common_index, 'vw_excess_ret'].sub(short_monthly.loc[common_index, 'vw_excess_ret'], fill_value=0),\n",
    "                    'ew_raw_ret_HL': long_monthly.loc[common_index, 'ew_raw_ret'].sub(short_monthly.loc[common_index, 'ew_raw_ret'], fill_value=0),\n",
    "                    'vw_raw_ret_HL': long_monthly.loc[common_index, 'vw_raw_ret'].sub(short_monthly.loc[common_index, 'vw_raw_ret'], fill_value=0),\n",
    "                    'ew_pred_ret_HL': long_monthly.loc[common_index, 'ew_pred_ret'].sub(short_monthly.loc[common_index, 'ew_pred_ret'], fill_value=0),\n",
    "                    'vw_pred_ret_HL': long_monthly.loc[common_index, 'vw_pred_ret'].sub(short_monthly.loc[common_index, 'vw_pred_ret'], fill_value=0)\n",
    "                }).reset_index()\n",
    "                hl_monthly_dfs_plotting[model_name] = hl_monthly.copy()\n",
    "\n",
    "                ew_excess_mean_hl = hl_monthly['ew_excess_ret_HL'].mean(); vw_excess_mean_hl = hl_monthly['vw_excess_ret_HL'].mean()\n",
    "                ew_raw_std_hl = hl_monthly['ew_raw_ret_HL'].std(); vw_raw_std_hl = hl_monthly['vw_raw_ret_HL'].std()\n",
    "                ew_sharpe_hl = (ew_excess_mean_hl / ew_raw_std_hl) * np.sqrt(annualization_factor) if ew_raw_std_hl > 1e-9 else np.nan\n",
    "                vw_sharpe_hl = (vw_excess_mean_hl / vw_raw_std_hl) * np.sqrt(annualization_factor) if vw_raw_std_hl > 1e-9 else np.nan\n",
    "                mdd_ew_hl = MDD(hl_monthly['ew_excess_ret_HL']); mdd_vw_hl = MDD(hl_monthly['vw_excess_ret_HL'])\n",
    "                # Factor model placeholders\n",
    "                alpha_ew_hl, tstat_ew_hl, r2_ew_hl = np.nan, np.nan, np.nan\n",
    "                alpha_vw_hl, tstat_vw_hl, r2_vw_hl = np.nan, np.nan, np.nan\n",
    "\n",
    "                hl_stats_df = pd.DataFrame({\n",
    "                    'ew_pred_mean': [hl_monthly['ew_pred_ret_HL'].mean()],'vw_pred_mean': [hl_monthly['vw_pred_ret_HL'].mean()],\n",
    "                    'ew_excess_mean': [ew_excess_mean_hl],'vw_excess_mean': [vw_excess_mean_hl],\n",
    "                    'ew_raw_std': [ew_raw_std_hl],'vw_raw_std': [vw_raw_std_hl],\n",
    "                    'n_months': [len(hl_monthly)], 'ew_sharpe': [ew_sharpe_hl],'vw_sharpe': [vw_sharpe_hl],\n",
    "                    'avg_stocks': [np.nan], 'Decile': ['H-L']\n",
    "                })\n",
    "\n",
    "        model_summary = pd.concat([decile_perf, hl_stats_df], ignore_index=True)\n",
    "        performance_summary_list.append(model_summary)\n",
    "\n",
    "        # Formatting Functions (keep as is)\n",
    "        def format_decile_table(summary_df, weight_scheme):\n",
    "            prefix = 'ew_' if weight_scheme == 'EW' else 'vw_'\n",
    "            cols_map = {f'{prefix}pred_mean': 'Pred', f'{prefix}excess_mean': 'Avg Ex Ret', f'{prefix}raw_std': 'SD (Raw Ret)', f'{prefix}sharpe': 'Ann SR', 'avg_stocks': 'Avg N'}\n",
    "            relevant_cols = [c for c in cols_map if c in summary_df.columns]\n",
    "            if not relevant_cols or 'Decile' not in summary_df.columns: return pd.DataFrame()\n",
    "            sub_df = summary_df[['Decile'] + relevant_cols].rename(columns=cols_map).copy().set_index('Decile')\n",
    "            for col in ['Pred', 'Avg Ex Ret', 'SD (Raw Ret)']:\n",
    "                 if col in sub_df.columns: sub_df[col] = pd.to_numeric(sub_df[col], errors='coerce') * 100\n",
    "            if 'Ann SR' in sub_df.columns: sub_df['Ann SR'] = pd.to_numeric(sub_df['Ann SR'], errors='coerce')\n",
    "            if 'Avg N' in sub_df.columns: sub_df['Avg N'] = pd.to_numeric(sub_df['Avg N'], errors='coerce')\n",
    "            def map_idx(x): return 'Low (L)' if str(x) == '1' else ('High (H)' if str(x) == '10' else str(x))\n",
    "            sub_df.index = sub_df.index.map(map_idx)\n",
    "            desired_order = ['Low (L)','2','3','4','5','6','7','8','9','High (H)','H-L']\n",
    "            sub_df = sub_df.reindex([i for i in desired_order if i in sub_df.index])\n",
    "            final_cols = [c for c in ['Pred', 'Avg Ex Ret', 'SD (Raw Ret)', 'Ann SR', 'Avg N'] if c in sub_df.columns]\n",
    "            sub_df_formatted = sub_df[final_cols].copy()\n",
    "            for col in ['Pred', 'Avg Ex Ret', 'SD (Raw Ret)']:\n",
    "                 if col in sub_df_formatted.columns: sub_df_formatted[col] = sub_df_formatted[col].map('{:.2f}%'.format).replace('nan%','N/A')\n",
    "            if 'Ann SR' in sub_df_formatted.columns: sub_df_formatted['Ann SR'] = sub_df_formatted['Ann SR'].map('{:.2f}'.format).replace('nan','N/A')\n",
    "            if 'Avg N' in sub_df_formatted.columns: sub_df_formatted['Avg N'] = sub_df_formatted['Avg N'].map('{:.0f}'.format).replace('nan','N/A')\n",
    "            return sub_df_formatted[final_cols]\n",
    "\n",
    "        def format_risk_table(data_dict, table_index):\n",
    "             df_risk = pd.DataFrame(data_dict, index=table_index)\n",
    "             for idx in df_risk.index:\n",
    "                  is_percent = '%' in idx; num_decimals = 2 if is_percent else 3; suffix = '%' if is_percent else ''\n",
    "                  try: df_risk.loc[idx] = df_risk.loc[idx].map(f'{{:.{num_decimals}f}}'.format).astype(str) + suffix\n",
    "                  except (ValueError, TypeError): df_risk.loc[idx] = df_risk.loc[idx].apply(lambda x: f'{float(x):.{num_decimals}f}{suffix}' if pd.notna(x) and isinstance(x,(int,float)) else 'N/A')\n",
    "                  df_risk.loc[idx] = df_risk.loc[idx].replace(['nan%', 'nan', ''], 'N/A', regex=False)\n",
    "             return df_risk\n",
    "\n",
    "        # Generate and Print Decile Tables\n",
    "        ew_table = format_decile_table(model_summary, 'EW'); decile_tables[f'{model_name}_EW'] = ew_table\n",
    "        vw_table = format_decile_table(model_summary, 'VW'); decile_tables[f'{model_name}_VW'] = vw_table\n",
    "        print(f\"\\n--- Ytelsestabell (Desiler): {model_name} - EW ---\"); print(ew_table)\n",
    "        print(f\"\\n--- Ytelsestabell (Desiler): {model_name} - VW ---\"); print(vw_table)\n",
    "\n",
    "        # Generate and Print H-L Risk/Performance Table\n",
    "        if not hl_stats_df.empty and not hl_monthly.empty:\n",
    "            hl_res = hl_stats_df.iloc[0]\n",
    "            turnover_ew_hl = turnover_results.get(model_name, {}).get('ew', np.nan)\n",
    "            turnover_vw_hl = turnover_results.get(model_name, {}).get('vw', np.nan)\n",
    "            max_loss_1m_ew_hl = hl_monthly['ew_excess_ret_HL'].min() * 100 if not hl_monthly.empty else np.nan\n",
    "            max_loss_1m_vw_hl = hl_monthly['vw_excess_ret_HL'].min() * 100 if not hl_monthly.empty else np.nan\n",
    "            risk_idx_hl = [\"Mean Excess Return [%]\", 'Std Dev (Raw) [%]', \"Ann. Sharpe Ratio\", \"Max Drawdown (Excess) [%]\", \"Max 1M Loss (Excess) [%]\", \"Avg Monthly Turnover [%]\", \"Factor Model Alpha [%]\", \"t(Alpha)\", \"Factor Model Adj R2\", \"Info Ratio\"]\n",
    "            ew_data_hl = {f'{model_name} H-L EW': [hl_res.get('ew_excess_mean', np.nan) * 100, hl_res.get('ew_raw_std', np.nan) * 100, hl_res.get('ew_sharpe', np.nan), abs(mdd_ew_hl), max_loss_1m_ew_hl, turnover_ew_hl * 100, alpha_ew_hl, tstat_ew_hl, r2_ew_hl, np.nan]}\n",
    "            vw_data_hl = {f'{model_name} H-L VW': [hl_res.get('vw_excess_mean', np.nan) * 100, hl_res.get('vw_raw_std', np.nan) * 100, hl_res.get('vw_sharpe', np.nan), abs(mdd_vw_hl), max_loss_1m_vw_hl, turnover_vw_hl * 100, alpha_vw_hl, tstat_vw_hl, r2_vw_hl, np.nan]}\n",
    "            ew_chart_hl = format_risk_table(ew_data_hl, risk_idx_hl); hl_risk_tables[f'{model_name}_EW'] = ew_chart_hl\n",
    "            vw_chart_hl = format_risk_table(vw_data_hl, risk_idx_hl); hl_risk_tables[f'{model_name}_VW'] = vw_chart_hl\n",
    "            print(f\"\\n--- H-L Portef√∏lje Risk/Performance ({model_name} EW) ---\"); print(ew_chart_hl)\n",
    "            print(f\"\\n--- H-L Portef√∏lje Risk/Performance ({model_name} VW) ---\"); print(vw_chart_hl)\n",
    "\n",
    "        # Generate and Print Long-Only (Decile 10) Risk/Performance Table\n",
    "        long_res_row = decile_perf[decile_perf['Decile'] == 10]\n",
    "        if not long_res_row.empty:\n",
    "            long_res = long_res_row.iloc[0]\n",
    "            long_monthly = model_results[model_results['Decile'] == 10].set_index('MonthYear')\n",
    "            if not long_monthly.empty: long_monthly_dfs_plotting[model_name] = long_monthly.reset_index()\n",
    "            mdd_ew_long = MDD(long_monthly['ew_excess_ret']) if not long_monthly.empty else np.nan\n",
    "            mdd_vw_long = MDD(long_monthly['vw_excess_ret']) if not long_monthly.empty else np.nan\n",
    "            max_loss_1m_ew_long = long_monthly['ew_excess_ret'].min() * 100 if not long_monthly.empty else np.nan\n",
    "            max_loss_1m_vw_long = long_monthly['vw_excess_ret'].min() * 100 if not long_monthly.empty else np.nan\n",
    "            turnover_ew_long = turnover_results.get(model_name, {}).get('long_ew', np.nan)\n",
    "            turnover_vw_long = turnover_results.get(model_name, {}).get('long_vw', np.nan)\n",
    "            alpha_long_ew, tstat_long_ew, r2_long_ew = np.nan, np.nan, np.nan # Factor placeholders\n",
    "            alpha_long_vw, tstat_long_vw, r2_long_vw = np.nan, np.nan, np.nan # Factor placeholders\n",
    "            risk_idx_long = [\"Mean Excess Return [%]\", 'Std Dev (Raw) [%]', \"Ann. Sharpe Ratio\", \"Max Drawdown (Excess) [%]\", \"Max 1M Loss (Excess) [%]\", \"Avg Monthly Turnover [%]\", \"Factor Model Alpha [%]\", \"t(Alpha)\", \"Factor Model Adj R2\", \"Info Ratio\"]\n",
    "            ew_data_long = {f'{model_name} Long EW': [long_res.get('ew_excess_mean', np.nan) * 100, long_res.get('ew_raw_std', np.nan) * 100, long_res.get('ew_sharpe', np.nan), abs(mdd_ew_long), max_loss_1m_ew_long, turnover_ew_long * 100, alpha_long_ew, tstat_long_ew, r2_long_ew, np.nan]}\n",
    "            vw_data_long = {f'{model_name} Long VW': [long_res.get('vw_excess_mean', np.nan) * 100, long_res.get('vw_raw_std', np.nan) * 100, long_res.get('vw_sharpe', np.nan), abs(mdd_vw_long), max_loss_1m_vw_long, turnover_vw_long * 100, alpha_long_vw, tstat_long_vw, r2_long_vw, np.nan]}\n",
    "            ew_chart_long = format_risk_table(ew_data_long, risk_idx_long); long_risk_tables[f'{model_name}_EW'] = ew_chart_long\n",
    "            vw_chart_long = format_risk_table(vw_data_long, risk_idx_long); long_risk_tables[f'{model_name}_VW'] = vw_chart_long\n",
    "            print(f\"\\n--- Long-Only (D10) Risk/Performance ({model_name} EW) ---\"); print(ew_chart_long)\n",
    "            print(f\"\\n--- Long-Only (D10) Risk/Performance ({model_name} VW) ---\"); print(vw_chart_long)\n",
    "        else: print(f\"  Advarsel: Ingen data for Desil 10 funnet for modell {model_name}.\")\n",
    "\n",
    "    # Plotting Cumulative Returns (keep as is)\n",
    "    fig_hl, ax_hl = plt.subplots(figsize=(14, 7)); plotted_hl = 0\n",
    "    sorted_models_hl = sorted(hl_monthly_dfs_plotting.keys())\n",
    "    for model_name in sorted_models_hl:\n",
    "        df_hl = hl_monthly_dfs_plotting[model_name]\n",
    "        if 'MonthYear' in df_hl.columns and not df_hl.empty:\n",
    "             df_hl['PlotDate'] = df_hl['MonthYear'].dt.to_timestamp() if pd.api.types.is_period_dtype(df_hl['MonthYear']) else pd.to_datetime(df_hl['MonthYear'])\n",
    "             df_hl = df_hl.set_index('PlotDate').sort_index()\n",
    "             if 'ew_excess_ret_HL' in df_hl.columns:\n",
    "                  ret_ew = df_hl['ew_excess_ret_HL'].dropna()\n",
    "                  if not ret_ew.empty: (1 + ret_ew).cumprod().plot(ax=ax_hl, label=f'{model_name} H-L EW'); plotted_hl += 1\n",
    "             if 'vw_excess_ret_HL' in df_hl.columns:\n",
    "                  ret_vw = df_hl['vw_excess_ret_HL'].dropna()\n",
    "                  if not ret_vw.empty: (1 + ret_vw).cumprod().plot(ax=ax_hl, label=f'{model_name} H-L VW', linestyle='--'); plotted_hl += 1\n",
    "    if plotted_hl > 0:\n",
    "        ax_hl.set_title('Kumulativ Excess Avkastning (H-L Portef√∏lje, t+1)'); ax_hl.set_ylabel('Kumulativ Verdi (Log Skala)'); ax_hl.set_xlabel('Dato'); ax_hl.set_yscale('log'); ax_hl.legend(loc='center left', bbox_to_anchor=(1, 0.5)); ax_hl.grid(True, which='both', linestyle='--', linewidth=0.5); fig_hl.tight_layout(rect=[0, 0, 0.85, 1]); plt.show()\n",
    "    else: plt.close(fig_hl)\n",
    "\n",
    "    fig_long, ax_long = plt.subplots(figsize=(14, 7)); plotted_long = 0\n",
    "    sorted_models_long = sorted(long_monthly_dfs_plotting.keys())\n",
    "    for model_name in sorted_models_long:\n",
    "        df_long = long_monthly_dfs_plotting[model_name]\n",
    "        if 'MonthYear' in df_long.columns and not df_long.empty:\n",
    "             df_long['PlotDate'] = df_long['MonthYear'].dt.to_timestamp() if pd.api.types.is_period_dtype(df_long['MonthYear']) else pd.to_datetime(df_long['MonthYear'])\n",
    "             df_long = df_long.set_index('PlotDate').sort_index()\n",
    "             if 'ew_excess_ret' in df_long.columns:\n",
    "                  ret_ew = df_long['ew_excess_ret'].dropna()\n",
    "                  if not ret_ew.empty: (1 + ret_ew).cumprod().plot(ax=ax_long, label=f'{model_name} Long EW'); plotted_long += 1\n",
    "             if 'vw_excess_ret' in df_long.columns:\n",
    "                  ret_vw = df_long['vw_excess_ret'].dropna()\n",
    "                  if not ret_vw.empty: (1 + ret_vw).cumprod().plot(ax=ax_long, label=f'{model_name} Long VW', linestyle='--'); plotted_long += 1\n",
    "    if plotted_long > 0:\n",
    "        ax_long.set_title('Kumulativ Excess Avkastning (Long-Only Portef√∏lje [D10], t+1)'); ax_long.set_ylabel('Kumulativ Verdi (Log Skala)'); ax_long.set_xlabel('Dato'); ax_long.set_yscale('log'); ax_long.legend(loc='center left', bbox_to_anchor=(1, 0.5)); ax_long.grid(True, which='both', linestyle='--', linewidth=0.5); fig_long.tight_layout(rect=[0, 0, 0.85, 1]); plt.show()\n",
    "    else: plt.close(fig_long)\n",
    "\n",
    "    print(\"--- Detaljert Portef√∏ljeanalyse Fullf√∏rt ---\")\n",
    "    return decile_tables, hl_risk_tables, long_risk_tables\n",
    "\n",
    "\n",
    "# === Stage 8: Variable Importance ===\n",
    "# (Keep calculate_variable_importance as is)\n",
    "def calculate_variable_importance(model_name, fitted_model, X_eval, y_eval, features, base_r2_is, vi_method='permutation_zero', model_params=None):\n",
    "    start_vi = time.time()\n",
    "    if vi_method != 'permutation_zero': print(f\"    FEIL: VI metode '{vi_method}' st√∏ttes ikke.\"); return pd.DataFrame()\n",
    "    if fitted_model is None: print(f\"    FEIL: Ingen modell for VI.\"); return pd.DataFrame()\n",
    "    if pd.isna(base_r2_is): print(f\"    ADVARSEL: Basis IS R2 NaN.\"); return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
    "    if len(features)==0 or X_eval.shape[0]==0 or y_eval.shape[0]==0 or X_eval.shape[1]!=len(features): print(f\"    FEIL: Ugyldige VI data dim.\"); return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
    "\n",
    "    importance_results = {}\n",
    "    y_eval_finite = y_eval[np.isfinite(y_eval)]\n",
    "    ss_tot_zero = np.sum(y_eval_finite**2)\n",
    "    if ss_tot_zero < 1e-15: return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
    "\n",
    "    params_retrain = model_params if model_params else {}\n",
    "    if model_name == 'ENET' and hasattr(fitted_model, 'alpha_'): params_retrain = {'alpha': fitted_model.alpha_, 'l1_ratio': fitted_model.l1_ratio_, **config.MODEL_PARAMS.get('ENET', {})}\n",
    "    elif model_name == 'PLS' and hasattr(fitted_model, 'n_components'): params_retrain = {'n_components': fitted_model.n_components, 'scale': False}\n",
    "    elif model_name == 'PCR' and hasattr(fitted_model, 'named_steps'):\n",
    "        try: params_retrain = {'n_components': fitted_model.named_steps['pca'].n_components_}\n",
    "        except KeyError: params_retrain = {'n_components': 1}\n",
    "    elif model_name in ['GLM_H', 'RF', 'GBRT_H'] and hasattr(fitted_model, 'get_params'):\n",
    "        params_retrain = fitted_model.get_params()\n",
    "        if model_params: params_retrain.update(model_params) # Use optimal params if provided\n",
    "    elif model_name == 'OLS3H': params_retrain = {k: v for k, v in config.MODEL_PARAMS.get('OLS3H', {}).items() if k != 'M'}\n",
    "\n",
    "    for idx, feat_name in enumerate(features):\n",
    "        X_permuted = X_eval.copy(); X_permuted[:, idx] = 0\n",
    "        permuted_model = None; permuted_preds = None; permuted_r2 = -np.inf\n",
    "        try:\n",
    "            if model_name == 'OLS': permuted_model = LinearRegression(fit_intercept=True).fit(X_permuted, y_eval)\n",
    "            elif model_name == 'OLS3H' and sm:\n",
    "                 X_perm_c = sm.add_constant(X_permuted)\n",
    "                 permuted_model_rlm = sm.RLM(y_eval, X_perm_c, M=sm.robust.norms.HuberT())\n",
    "                 permuted_model = permuted_model_rlm.fit(**params_retrain)\n",
    "                 permuted_preds = permuted_model.predict(X_perm_c)\n",
    "            elif model_name == 'PLS': permuted_model = PLSRegression(**params_retrain).fit(X_permuted, y_eval)\n",
    "            elif model_name == 'PCR': permuted_model = Pipeline([('pca', PCA(n_components=params_retrain.get('n_components', 1))), ('lr', LinearRegression())]).fit(X_permuted, y_eval)\n",
    "            elif model_name == 'ENET': permuted_model = ElasticNet(**params_retrain, fit_intercept=True).fit(X_permuted, y_eval)\n",
    "            elif model_name == 'GLM_H': permuted_model = HuberRegressor(**params_retrain, fit_intercept=True).fit(X_permuted, y_eval)\n",
    "            elif model_name == 'RF': permuted_model = RandomForestRegressor(**params_retrain).fit(X_permuted, y_eval)\n",
    "            elif model_name == 'GBRT_H': permuted_model = GradientBoostingRegressor(**params_retrain).fit(X_permuted, y_eval)\n",
    "\n",
    "            if permuted_model and model_name != 'OLS3H': permuted_preds = permuted_model.predict(X_permuted).flatten()\n",
    "            if permuted_preds is not None:\n",
    "                preds_finite = permuted_preds[np.isfinite(y_eval)]\n",
    "                if len(preds_finite) == len(y_eval_finite) and np.all(np.isfinite(preds_finite)):\n",
    "                    ss_res_permuted = np.sum((y_eval_finite - preds_finite)**2)\n",
    "                    permuted_r2 = 1.0 - (ss_res_permuted / ss_tot_zero)\n",
    "        except Exception as e: print(f\"    ADVARSEL: Unntak VI '{feat_name}' i {model_name}: {e}\")\n",
    "        reduction = base_r2_is - permuted_r2\n",
    "        importance_results[feat_name] = max(0, reduction) if pd.notna(reduction) else 0.0\n",
    "\n",
    "    if not importance_results: return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
    "    importance_df = pd.DataFrame(importance_results.items(), columns=['Feature', 'R2_Reduction'])\n",
    "    total_reduction = importance_df['R2_Reduction'].sum()\n",
    "    importance_df['Importance'] = importance_df['R2_Reduction'] / total_reduction if total_reduction > 1e-9 else 0.0\n",
    "    return importance_df[['Feature', 'Importance']]\n",
    "\n",
    "\n",
    "# === Stage 9: Complexity Plotting ===\n",
    "# (Keep plot_time_varying_complexity as is)\n",
    "def plot_time_varying_complexity(model_metrics, complexity_params_to_plot):\n",
    "     print(\"\\n--- 9. Plotter Tidsvarierende Modellkompleksitet ---\")\n",
    "     plotted_any = False\n",
    "     for model_name, param_keys in complexity_params_to_plot.items():\n",
    "         if model_name not in model_metrics: continue\n",
    "         print(f\"\\n  --- Modell: {model_name} ---\")\n",
    "         model_data = model_metrics[model_name]\n",
    "         for param_key in param_keys:\n",
    "             if param_key in model_data:\n",
    "                 values = model_data[param_key]\n",
    "                 valid_data = [(i + 1, v) for i, v in enumerate(values) if v is not None and pd.notna(v)]\n",
    "                 if valid_data:\n",
    "                     plotted_any = True\n",
    "                     windows, param_values = zip(*valid_data)\n",
    "                     param_label = param_key.replace('optim_', '').replace('_', ' ').title()\n",
    "                     data_table = pd.DataFrame({param_label: param_values}, index=pd.Index(windows, name='Vindu Nr.'))\n",
    "                     print(f\"    Optimal {param_label} per Vindu:\"); print(data_table.round(4))\n",
    "                     plt.figure(figsize=(10, 5))\n",
    "                     plt.plot(windows, param_values, marker='o', linestyle='-')\n",
    "                     plot_title = f\"Tidsvariasjon i Optimal {param_label} for {model_name}\"\n",
    "                     y_axis_label = f\"Optimal {param_label}\"\n",
    "                     if 'alpha' in param_key.lower() or 'lambda' in param_key.lower():\n",
    "                          if all(v > 1e-9 for v in param_values): plt.yscale('log'); y_axis_label += \" (Log Skala)\"\n",
    "                          else: print(f\"    Advarsel: Kan ikke bruke log-skala for {param_label}.\")\n",
    "                     plt.xlabel(\"Vindu Nr.\"); plt.ylabel(y_axis_label); plt.title(plot_title); plt.grid(True, which='both', linestyle='--', linewidth=0.5); plt.tight_layout(); plt.show()\n",
    "                 else: print(f\"    Ingen gyldige verdier funnet for '{param_label}' for {model_name}.\")\n",
    "             else: print(f\"    Metrikk '{param_key}' ikke funnet for {model_name}.\")\n",
    "     if not plotted_any: print(\"  Ingen kompleksitetsparametere √• plotte.\")\n",
    "\n",
    "\n",
    "# === Stage 10: Reporting & Saving ===\n",
    "# (Keep create_summary_table and save_results as is)\n",
    "def create_summary_table(model_metrics, annualization_factor=12):\n",
    "    print(\"\\n--- 10a. Lager Oppsummerende Resultattabell ---\")\n",
    "    summary_data = []\n",
    "    model_order = ['OLS','OLS3H','PLS','PCR','ENET','GLM_H','RF','GBRT_H','NN1','NN2','NN3','NN4','NN5']\n",
    "    models_in_results = list(model_metrics.keys())\n",
    "    models_sorted = [m for m in model_order if m in models_in_results] + \\\n",
    "                    [m for m in sorted(models_in_results) if m not in model_order]\n",
    "    if not models_sorted: print(\"Ingen modelldata funnet.\"); return pd.DataFrame()\n",
    "\n",
    "    for model_name in models_sorted:\n",
    "        metrics = model_metrics[model_name]\n",
    "        avg_is_r2 = np.nanmean(metrics.get('is_r2_train_val', [])) if metrics.get('is_r2_train_val') else np.nan\n",
    "        avg_oos_r2 = np.nanmean(metrics.get('oos_r2', [])) if metrics.get('oos_r2') else np.nan\n",
    "        avg_oos_sharpe = np.nanmean(metrics.get('oos_sharpe', [])) if metrics.get('oos_sharpe') else np.nan\n",
    "        overall_oos_r2_gu = metrics.get('oos_r2_overall_gu', np.nan)\n",
    "        avg_optim_params_str = \"\"\n",
    "        optim_parts = []\n",
    "        for k, v in metrics.items():\n",
    "            if k.startswith('optim_') and v:\n",
    "                 numeric_v = [item for item in v if isinstance(item, (int, float, np.number)) and pd.notna(item)]\n",
    "                 if numeric_v:\n",
    "                     try:\n",
    "                          mean_val = np.nanmean(numeric_v)\n",
    "                          if not np.isnan(mean_val): optim_parts.append(f\"{k.replace('optim_', '')}={mean_val:.2g}\")\n",
    "                     except Exception as e_mean: print(f\"  Advarsel: mean calc error for {k} {model_name}: {e_mean}.\")\n",
    "        avg_optim_params_str = \", \".join(optim_parts)\n",
    "\n",
    "        summary_data.append({\n",
    "            'Modell': model_name,\n",
    "            'Avg IS R¬≤ (%)': avg_is_r2 * 100 if pd.notna(avg_is_r2) else np.nan,\n",
    "            'Avg Window OOS R¬≤ (%)': avg_oos_r2 * 100 if pd.notna(avg_oos_r2) else np.nan,\n",
    "            'Overall OOS R¬≤ (%)': overall_oos_r2_gu * 100 if pd.notna(overall_oos_r2_gu) else np.nan,\n",
    "            'Avg Pred Sharpe (OOS)': avg_oos_sharpe if pd.notna(avg_oos_sharpe) else np.nan,\n",
    "            'Avg Optim Params': avg_optim_params_str\n",
    "        })\n",
    "    if not summary_data: print(\"Ingen data √• inkludere.\"); return pd.DataFrame()\n",
    "    summary_df = pd.DataFrame(summary_data).set_index('Modell')\n",
    "    print(\"\\n--- Oppsummeringstabell ---\")\n",
    "    print(summary_df.to_string(float_format=lambda x: f\"{x:.3f}\" if pd.notna(x) else \"N/A\", na_rep=\"N/A\"))\n",
    "    return summary_df\n",
    "\n",
    "def save_results(output_dir, subset_label, results_dict):\n",
    "    print(f\"\\n--- 10b. Lagrer Resultater for Subset: {subset_label} ---\")\n",
    "    subset_dir = os.path.join(output_dir, subset_label)\n",
    "    try:\n",
    "        if not os.path.exists(subset_dir): os.makedirs(subset_dir); print(f\"  Opprettet mappe: {subset_dir}\")\n",
    "    except OSError as e: print(f\"  FEIL: Kunne ikke opprette mappe {subset_dir}: {e}\"); return\n",
    "\n",
    "    for name, data in results_dict.items():\n",
    "        base_filename = os.path.join(subset_dir, f\"{name}\")\n",
    "        try:\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                if not data.empty: filename = f\"{base_filename}.csv\"; data.to_csv(filename); print(f\"  -> Lagret DataFrame: {filename}\")\n",
    "                else: print(f\"  -> Hoppet over tom DataFrame: {name}\")\n",
    "            elif isinstance(data, dict):\n",
    "                saved_sub = False\n",
    "                for sub_name, sub_data in data.items():\n",
    "                     if isinstance(sub_data, pd.DataFrame):\n",
    "                         if not sub_data.empty: sub_filename = f\"{base_filename}_{sub_name}.csv\"; sub_data.to_csv(sub_filename); print(f\"  -> Lagret Dict->DataFrame: {sub_filename}\"); saved_sub = True\n",
    "                     elif isinstance(sub_data, dict): print(f\"  -> Hoppet over Dict->Dict: {name}_{sub_name}\")\n",
    "                     else: print(f\"  -> Hoppet over ukjent datatype i dict: {name}_{sub_name} (Type: {type(sub_data)})\")\n",
    "            else: print(f\"  -> Hoppet over ukjent datatype: {name} (Type: {type(data)})\")\n",
    "        except Exception as e: print(f\"  FEIL under lagring av '{name}' til '{subset_dir}': {e}\"); traceback.print_exc(limit=1)\n",
    "    print(f\"--- Lagring for {subset_label} fullf√∏rt (eller fors√∏kt) ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd1bb7",
   "metadata": {},
   "source": [
    "# File 4: main_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2726854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- main_runner.py ---\n",
    "# Main orchestration script for running the ML asset pricing pipeline.\n",
    "# Imports config and utils, defines model training logic, runs the pipeline loops.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import re\n",
    "\n",
    "# --- Import Configuration & Utilities ---\n",
    "import config\n",
    "import pipeline_utils as utils\n",
    "\n",
    "# --- Import Model Specific Libraries ---\n",
    "from sklearn.linear_model import LinearRegression, ElasticNetCV, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    STATSMODELS_AVAILABLE = True\n",
    "except ImportError: STATSMODELS_AVAILABLE = False; print(\"ADVARSEL: Statsmodels ikke funnet.\")\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, regularizers, callbacks, backend as K\n",
    "    from tensorflow.keras.optimizers import Adam # Correct specific import if needed\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    # Set TF seeds and deterministic options (keep as is)\n",
    "    os.environ['PYTHONHASHSEED']=str(config.TF_SEED)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC']='1' # Note: May impact performance\n",
    "    random.seed(config.TF_SEED)\n",
    "    np.random.seed(config.TF_SEED)\n",
    "    tf.random.set_seed(config.TF_SEED)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"GPUs funnet ({len(gpus)}), minnevekst aktivert.\")\n",
    "        except RuntimeError as e: print(f\"Kunne ikke sette minnevekst for GPU: {e}\")\n",
    "except ImportError: TENSORFLOW_AVAILABLE = False; print(\"ADVARSEL: TensorFlow/Keras ikke funnet.\")\n",
    "\n",
    "# ==========================================================================\n",
    "# --- MODEL TRAINING/EVALUATION FUNCTIONS (Per Window) ---\n",
    "# (Keep ALL train_evaluate_* functions exactly as they were in the initially provided code)\n",
    "# Define OLS\n",
    "def train_evaluate_ols(X_train_val, y_train_val, X_test, model_params):\n",
    "    try:\n",
    "        model = LinearRegression(fit_intercept=True).fit(X_train_val, y_train_val)\n",
    "        preds_oos = model.predict(X_test) if X_test.shape[0] > 0 else np.array([])\n",
    "        preds_is = model.predict(X_train_val)\n",
    "        return model, preds_oos, preds_is, {}\n",
    "    except Exception as e: print(f\"    FEIL OLS: {e}\"); return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# Define OLS3H\n",
    "def train_evaluate_ols3h(X_train_val, y_train_val, X_test, model_params):\n",
    "    if not STATSMODELS_AVAILABLE: return None, np.array([]), np.array([]), {}\n",
    "    try:\n",
    "        X_tv_const = sm.add_constant(X_train_val, prepend=True)\n",
    "        X_test_const = sm.add_constant(X_test, prepend=True) if X_test.shape[0] > 0 else None\n",
    "        rlm_model = sm.RLM(y_train_val, X_tv_const, M=sm.robust.norms.HuberT())\n",
    "        valid_fit_params = {k: v for k, v in model_params.items() if k in ['maxiter', 'tol']}\n",
    "        fitted_model = rlm_model.fit(**valid_fit_params)\n",
    "        preds_oos = fitted_model.predict(X_test_const) if X_test_const is not None else np.array([])\n",
    "        preds_is = fitted_model.predict(X_tv_const)\n",
    "        optim_params = {'M': 'HuberT', **valid_fit_params}\n",
    "        return fitted_model, preds_oos, preds_is, optim_params\n",
    "    except Exception as e: print(f\"    FEIL OLS3H: {e}\"); traceback.print_exc(limit=1); return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# Define _tune_simple_model helper\n",
    "def _tune_simple_model(ModelClass, X_train, y_train, X_val, y_val, param_grid_dict):\n",
    "    best_mse = np.inf; best_param_value = None; param_name = list(param_grid_dict.keys())[0]\n",
    "    param_values = param_grid_dict[param_name]\n",
    "    max_components = X_train.shape[1] # Simplified constraint for example\n",
    "    valid_grid = [p for p in param_values if 0 < p <= max_components]\n",
    "    if not valid_grid: valid_grid = [1]\n",
    "    for p_val in valid_grid:\n",
    "        try:\n",
    "            if ModelClass == Pipeline: model_val = Pipeline([('pca', PCA(n_components=p_val)), ('lr', LinearRegression())])\n",
    "            else: model_val = ModelClass(**{param_name: p_val, 'scale': False})\n",
    "            model_val.fit(X_train, y_train)\n",
    "            y_pred_val = model_val.predict(X_val).flatten()\n",
    "            if not np.all(np.isfinite(y_pred_val)): continue\n",
    "            mse = mean_squared_error(y_val, y_pred_val)\n",
    "            if not np.isnan(mse) and mse < best_mse: best_mse = mse; best_param_value = p_val\n",
    "        except Exception as e: continue # print(f\"    FEIL tuning {param_name}={p_val}: {e}\")\n",
    "    if best_param_value is None: print(f\"    FEIL: Tuning mislyktes for {ModelClass.__name__}.\")\n",
    "    return best_param_value\n",
    "\n",
    "# Define PLS\n",
    "def train_evaluate_pls(X_train, y_train, X_val, y_val, X_test, model_params):\n",
    "    optimal_params = {}\n",
    "    try:\n",
    "        best_n = _tune_simple_model(PLSRegression, X_train, y_train, X_val, y_val, {'n_components': model_params['n_components_grid']})\n",
    "        if best_n is None: raise ValueError(\"PLS tuning failed.\")\n",
    "        optimal_params = {'n_components': best_n}\n",
    "        X_train_val = np.vstack((X_train, X_val)); y_train_val = np.concatenate((y_train, y_val))\n",
    "        final_model = PLSRegression(n_components=best_n, scale=False).fit(X_train_val, y_train_val)\n",
    "        preds_oos = final_model.predict(X_test).flatten() if X_test.shape[0] > 0 else np.array([])\n",
    "        preds_is = final_model.predict(X_train_val).flatten()\n",
    "        return final_model, preds_oos, preds_is, optimal_params\n",
    "    except Exception as e: print(f\"    FEIL PLS: {e}\"); traceback.print_exc(limit=1); return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# Define PCR\n",
    "def train_evaluate_pcr(X_train, y_train, X_val, y_val, X_test, model_params):\n",
    "    optimal_params = {}\n",
    "    try:\n",
    "        best_n = _tune_simple_model(Pipeline, X_train, y_train, X_val, y_val, {'n_components': model_params['n_components_grid']})\n",
    "        if best_n is None: raise ValueError(\"PCR tuning failed.\")\n",
    "        optimal_params = {'n_components': best_n}\n",
    "        final_model = Pipeline([('pca', PCA(n_components=best_n)), ('lr', LinearRegression())])\n",
    "        X_train_val = np.vstack((X_train, X_val)); y_train_val = np.concatenate((y_train, y_val))\n",
    "        final_model.fit(X_train_val, y_train_val)\n",
    "        preds_oos = final_model.predict(X_test) if X_test.shape[0] > 0 else np.array([])\n",
    "        preds_is = final_model.predict(X_train_val)\n",
    "        return final_model, preds_oos, preds_is, optimal_params\n",
    "    except Exception as e: print(f\"    FEIL PCR: {e}\"); traceback.print_exc(limit=1); return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# Define ENET\n",
    "def train_evaluate_enet(X_train, y_train, X_test, model_params):\n",
    "    optimal_params = {}\n",
    "    try:\n",
    "        cv_strategy = KFold(n_splits=model_params['cv_folds'], shuffle=True, random_state=config.GENERAL_SEED)\n",
    "        enet_cv = ElasticNetCV(alphas=model_params['alphas'], l1_ratio=model_params['l1_ratio'], fit_intercept=True, cv=cv_strategy, n_jobs=model_params.get('n_jobs', -1), max_iter=model_params.get('max_iter', 1000), tol=model_params.get('tol', 0.001), random_state=config.GENERAL_SEED)\n",
    "        enet_cv.fit(X_train, y_train)\n",
    "        optimal_params = {'alpha': enet_cv.alpha_, 'l1_ratio': enet_cv.l1_ratio_}\n",
    "        final_model = enet_cv\n",
    "        preds_oos = final_model.predict(X_test) if X_test.shape[0] > 0 else np.array([])\n",
    "        preds_is = final_model.predict(X_train)\n",
    "        return final_model, preds_oos, preds_is, optimal_params\n",
    "    except Exception as e: print(f\"    FEIL ENET: {e}\"); traceback.print_exc(limit=1); return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# Define GLM_H\n",
    "def train_evaluate_glm_h(X_train, y_train, X_val, y_val, X_test, model_params):\n",
    "    optimal_params = {}; best_mse = np.inf; optim_found_params = None\n",
    "    grid = list(ParameterGrid(model_params['param_grid'])); max_iter = model_params.get('max_iter', 300)\n",
    "    for params in grid:\n",
    "        try:\n",
    "            model_val = HuberRegressor(fit_intercept=True, **params, max_iter=max_iter).fit(X_train, y_train)\n",
    "            y_pred_val = model_val.predict(X_val)\n",
    "            if not np.all(np.isfinite(y_pred_val)): continue\n",
    "            mse = mean_squared_error(y_val, y_pred_val)\n",
    "            if not np.isnan(mse) and mse < best_mse: best_mse = mse; optim_found_params = params\n",
    "        except Exception as e: continue # print(f\"    FEIL GLM_H tuning params {params}: {e}\")\n",
    "    if optim_found_params is None: print(\"    FEIL: GLM_H tuning mislyktes.\"); return None, np.array([]), np.array([]), {}\n",
    "    optimal_params = optim_found_params.copy()\n",
    "    try:\n",
    "        X_train_val = np.vstack((X_train, X_val)); y_train_val = np.concatenate((y_train, y_val))\n",
    "        final_model = HuberRegressor(fit_intercept=True, **optimal_params, max_iter=max_iter).fit(X_train_val, y_train_val)\n",
    "        preds_oos = final_model.predict(X_test) if X_test.shape[0] > 0 else np.array([])\n",
    "        preds_is = final_model.predict(X_train_val)\n",
    "        return final_model, preds_oos, preds_is, optimal_params\n",
    "    except Exception as e: print(f\"    FEIL GLM_H final: {e}\"); traceback.print_exc(limit=1); return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# Define _tune_tree_model helper\n",
    "def _tune_tree_model(ModelClass, X_train, y_train, X_val, y_val, model_params):\n",
    "    best_mse = np.inf; best_params = None; param_grid = list(ParameterGrid(model_params['param_grid']))\n",
    "    base_params = {k: v for k, v in model_params.items() if k != 'param_grid'}\n",
    "    for params in param_grid:\n",
    "        try:\n",
    "            current_params = {**base_params, **params}\n",
    "            model_val = ModelClass(**current_params).fit(X_train, y_train)\n",
    "            y_pred_val = model_val.predict(X_val)\n",
    "            if not np.all(np.isfinite(y_pred_val)): continue\n",
    "            mse = mean_squared_error(y_val, y_pred_val)\n",
    "            if not np.isnan(mse) and mse < best_mse: best_mse = mse; best_params = params\n",
    "        except Exception as e: continue # print(f\"    FEIL {ModelClass.__name__} tuning params {params}: {e}\")\n",
    "    if best_params is None: print(f\"    FEIL: Tuning mislyktes for {ModelClass.__name__}.\")\n",
    "    return best_params\n",
    "\n",
    "# Define RF\n",
    "def train_evaluate_rf(X_train, y_train, X_val, y_val, X_test, model_params):\n",
    "    optimal_params = {}\n",
    "    try:\n",
    "        best_grid_params = _tune_tree_model(RandomForestRegressor, X_train, y_train, X_val, y_val, model_params)\n",
    "        if best_grid_params is None: raise ValueError(\"RF tuning failed.\")\n",
    "        optimal_params = best_grid_params.copy()\n",
    "        final_params = {**{k:v for k,v in model_params.items() if k!='param_grid'}, **optimal_params}\n",
    "        X_train_val = np.vstack((X_train, X_val)); y_train_val = np.concatenate((y_train, y_val))\n",
    "        final_model = RandomForestRegressor(**final_params).fit(X_train_val, y_train_val)\n",
    "        preds_oos = final_model.predict(X_test) if X_test.shape[0] > 0 else np.array([])\n",
    "        preds_is = final_model.predict(X_train_val)\n",
    "        return final_model, preds_oos, preds_is, optimal_params\n",
    "    except Exception as e: print(f\"    FEIL RF: {e}\"); traceback.print_exc(limit=1); return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# Define GBRT_H\n",
    "def train_evaluate_gbrt_h(X_train, y_train, X_val, y_val, X_test, model_params):\n",
    "    optimal_params = {}\n",
    "    try:\n",
    "        gbrt_params = model_params.copy(); gbrt_params['loss'] = 'huber'\n",
    "        best_grid_params = _tune_tree_model(GradientBoostingRegressor, X_train, y_train, X_val, y_val, gbrt_params)\n",
    "        if best_grid_params is None: raise ValueError(\"GBRT tuning failed.\")\n",
    "        optimal_params = best_grid_params.copy()\n",
    "        final_params = {**{k:v for k,v in gbrt_params.items() if k!='param_grid'}, **optimal_params}\n",
    "        X_train_val = np.vstack((X_train, X_val)); y_train_val = np.concatenate((y_train, y_val))\n",
    "        final_model = GradientBoostingRegressor(**final_params).fit(X_train_val, y_train_val)\n",
    "        preds_oos = final_model.predict(X_test) if X_test.shape[0] > 0 else np.array([])\n",
    "        preds_is = final_model.predict(X_train_val)\n",
    "        return final_model, preds_oos, preds_is, optimal_params\n",
    "    except Exception as e: print(f\"    FEIL GBRT: {e}\"); traceback.print_exc(limit=1); return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# Define NN functions (if TF available)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    def build_nn_model(input_shape, nn_config, lambda1):\n",
    "        model = keras.Sequential(name=nn_config['name'])\n",
    "        model.add(layers.Input(shape=(input_shape,)))\n",
    "        for units in nn_config['hidden_units']:\n",
    "            model.add(layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l1(lambda1)))\n",
    "        model.add(layers.Dense(1, activation='linear'))\n",
    "        return model\n",
    "\n",
    "    def train_evaluate_nn(X_train, y_train, X_val, y_val, X_test, model_params, nn_specific_config):\n",
    "        model_name = nn_specific_config['name']; optimal_params = {}; best_val_mse = np.inf; optim_found_params = None\n",
    "        input_shape = X_train.shape[1]; shared_params = model_params['NN_SHARED']\n",
    "        param_grid = list(ParameterGrid(shared_params['param_grid'])); epochs = shared_params['epochs']; batch_size = shared_params['batch_size']\n",
    "        patience = shared_params['patience']; ensemble_size = shared_params['ensemble_size']; base_seed = shared_params['random_seed_base']\n",
    "        # Tuning Loop\n",
    "        for params in param_grid:\n",
    "            lambda1 = params['lambda1']; learning_rate = params['learning_rate']; val_preds_ensemble = []\n",
    "            try:\n",
    "                for i in range(ensemble_size):\n",
    "                    K.clear_session(); tf.random.set_seed(base_seed + i); np.random.seed(base_seed + i); random.seed(base_seed + i)\n",
    "                    nn_model = build_nn_model(input_shape, nn_specific_config, lambda1)\n",
    "                    nn_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "                    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, verbose=0)\n",
    "                    history = nn_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, callbacks.TerminateOnNaN()], verbose=0)\n",
    "                    if not np.isnan(history.history['val_loss']).any() and 'val_loss' in history.history and history.history['val_loss']:\n",
    "                        val_preds_member = nn_model.predict(X_val, batch_size=batch_size).flatten()\n",
    "                        if np.all(np.isfinite(val_preds_member)): val_preds_ensemble.append(val_preds_member)\n",
    "                        else: val_preds_ensemble = []; break\n",
    "                    else: val_preds_ensemble = []; break\n",
    "                if not val_preds_ensemble: continue\n",
    "                avg_val_preds = np.mean(np.array(val_preds_ensemble), axis=0)\n",
    "                finite_mask = np.isfinite(avg_val_preds) & np.isfinite(y_val)\n",
    "                if np.sum(finite_mask) == 0: continue\n",
    "                val_mse = mean_squared_error(y_val[finite_mask], avg_val_preds[finite_mask])\n",
    "                if not np.isnan(val_mse) and val_mse < best_val_mse: best_val_mse = val_mse; optim_found_params = params\n",
    "            except Exception as e: print(f\"    FEIL NN tuning {params}: {e}\"); continue\n",
    "        # Check Tuning Success\n",
    "        if optim_found_params is None: print(f\"    FEIL: NN tuning mislyktes for {model_name}.\"); return None, np.array([]), np.array([]), {}\n",
    "        optimal_params = optim_found_params.copy(); opt_lambda1 = optimal_params['lambda1']; opt_lr = optimal_params['learning_rate']\n",
    "        # Final Ensemble Training\n",
    "        final_model = None; test_preds_ensemble = []; is_preds_ensemble = []\n",
    "        try:\n",
    "            X_train_val = np.vstack((X_train, X_val)); y_train_val = np.concatenate((y_train, y_val))\n",
    "            for i in range(ensemble_size):\n",
    "                K.clear_session(); final_seed = base_seed + i + ensemble_size; tf.random.set_seed(final_seed); np.random.seed(final_seed); random.seed(final_seed)\n",
    "                nn_model_final = build_nn_model(input_shape, nn_specific_config, opt_lambda1)\n",
    "                nn_model_final.compile(optimizer=Adam(learning_rate=opt_lr), loss='mse')\n",
    "                history_final = nn_model_final.fit(X_train_val, y_train_val, epochs=epochs, batch_size=batch_size, callbacks=[callbacks.TerminateOnNaN()], verbose=0)\n",
    "                if np.isnan(history_final.history['loss']).any(): test_preds_ensemble = []; is_preds_ensemble = []; break\n",
    "                if X_test.shape[0] > 0:\n",
    "                    preds_t = nn_model_final.predict(X_test, batch_size=batch_size).flatten()\n",
    "                    if np.all(np.isfinite(preds_t)): test_preds_ensemble.append(preds_t)\n",
    "                    else: print(f\"   Advarsel: Ikke-finite OOS pred NN\"); test_preds_ensemble = []; break\n",
    "                preds_i = nn_model_final.predict(X_train_val, batch_size=batch_size).flatten()\n",
    "                if np.all(np.isfinite(preds_i)): is_preds_ensemble.append(preds_i)\n",
    "                else: print(f\"   Advarsel: Ikke-finite IS pred NN\"); is_preds_ensemble = []; break\n",
    "                if i == 0: final_model = nn_model_final\n",
    "            # Aggregate Predictions\n",
    "            if (X_test.shape[0] > 0 and not test_preds_ensemble) or not is_preds_ensemble: raise ValueError(f\"{model_name} final ensemble failed.\")\n",
    "            preds_oos_final = np.mean(np.array(test_preds_ensemble), axis=0) if X_test.shape[0] > 0 else np.array([])\n",
    "            preds_is_final = np.mean(np.array(is_preds_ensemble), axis=0)\n",
    "            return final_model, preds_oos_final, preds_is_final, optimal_params\n",
    "        except Exception as e: print(f\"    FEIL NN final: {e}\"); traceback.print_exc(limit=1); return None, np.array([]), np.array([]), {}\n",
    "# ==========================================================================\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "# --- MAIN EXECUTION SCRIPT ---\n",
    "# ==========================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    overall_start_time = datetime.datetime.now()\n",
    "    print(f\"--- Starter ML Asset Pricing Pipeline ---\")\n",
    "    print(f\"--- Starttidspunkt: {overall_start_time:%Y-%m-%d %H:%M:%S} ---\")\n",
    "    print(f\"--- Output lagres i: {config.OUTPUT_DIR} ---\")\n",
    "\n",
    "    # === 1: Load Preprocessed Data ===\n",
    "    df_loaded = utils.load_prepare_data(\n",
    "        config.DATA_FILE, config.COLUMN_CONFIG,\n",
    "        config.TARGET_VARIABLE, config.NEXT_RETURN_VARIABLE, config.MARKET_CAP_ORIG_VARIABLE\n",
    "    )\n",
    "    if df_loaded is None: exit(\"Avslutter: Lasting av forh√•ndsbehandlet data feilet.\")\n",
    "\n",
    "    # === 2: Define Features (from preprocessed data) ===\n",
    "    base_exclude_list = [\n",
    "        config.TARGET_VARIABLE, config.NEXT_RETURN_VARIABLE, config.MARKET_CAP_ORIG_VARIABLE,\n",
    "        'Instrument', 'Date', 'id', 'date', # Include potential original names\n",
    "        'MonthlyReturn_t', 'AdjustedReturn_t', 'MonthlyRiskFreeRate_t', # Exclude intermediate calcs if present\n",
    "        # Exclude original cols IF their logged versions were successfully created and are found\n",
    "        # This logic is now handled more robustly inside define_features\n",
    "    ]\n",
    "    # Add specific raw names to exclude if their log versions are intended features\n",
    "    # Check preprocess_data.py output for final log names (e.g., log_marketcap vs log_MarketCap)\n",
    "    if utils.find_col(df_loaded, ['log_MarketCap', 'log_marketcap']): base_exclude_list.extend(utils.find_col(df_loaded, [n]) for n in ['MarketCap', 'CommonSharesOutstanding', 'ClosePrice'] if utils.find_col(df_loaded, [n]))\n",
    "    if utils.find_col(df_loaded, ['log_BM', 'log_bm']): base_exclude_list.append(utils.find_col(df_loaded, ['BM', 'bm']))\n",
    "    if utils.find_col(df_loaded, ['log_ClosePrice', 'log_closeprice']): base_exclude_list.append(utils.find_col(df_loaded, ['ClosePrice', 'closeprice']))\n",
    "    if utils.find_col(df_loaded, ['log_Volume', 'log_volume']): base_exclude_list.append(utils.find_col(df_loaded, ['Volume', 'volume']))\n",
    "    if utils.find_col(df_loaded, ['log_CommonSharesOutstanding', 'log_commonsharesoutstanding']): base_exclude_list.append(utils.find_col(df_loaded, ['CommonSharesOutstanding', 'commonsharesoutstanding']))\n",
    "    if utils.find_col(df_loaded, ['TermSpread', 'termspread']): base_exclude_list.extend(utils.find_col(df_loaded, [n]) for n in ['NorgesBank10Y','NIBOR3M'] if utils.find_col(df_loaded, [n]))\n",
    "\n",
    "    # Remove None values potentially added by find_col if column wasn't found\n",
    "    base_exclude_list = [col for col in base_exclude_list if col is not None]\n",
    "\n",
    "    all_numeric_features_init, ols3_subset_features_init, _ = utils.define_features(\n",
    "        df_loaded, config.OLS3_FEATURE_NAMES, base_exclude_list\n",
    "    )\n",
    "    if not all_numeric_features_init: exit(\"Avslutter: Ingen features definert etter lasting.\")\n",
    "\n",
    "    # === 3: Rank Standardize Features ===\n",
    "    df_std = utils.rank_standardize_features(df_loaded, all_numeric_features_init)\n",
    "    if df_std is None: exit(\"Avslutter: Standardisering feilet.\")\n",
    "\n",
    "    # === 4: Clean Data (Post-Standardization) ===\n",
    "    df_clean = utils.clean_data(\n",
    "        df_std,\n",
    "        all_numeric_features_init, # Features to check for NaN/inf\n",
    "        config.ESSENTIAL_COLS_FOR_DROPNA, # Columns where NaN forces row drop\n",
    "        config.MARKET_CAP_ORIG_VARIABLE   # Column to check for > 0\n",
    "    )\n",
    "    if df_clean is None or df_clean.empty: exit(\"Avslutter: Dataframe tom etter rensing.\")\n",
    "\n",
    "    # === Final Feature Definition and Model Assignment ===\n",
    "    all_numeric_features, ols3_subset_features, _ = utils.define_features(\n",
    "        df_clean, config.OLS3_FEATURE_NAMES, base_exclude_list # Use same exclusion list\n",
    "    )\n",
    "    if not all_numeric_features: exit(\"FEIL: Ingen numeriske features igjen etter rensing.\")\n",
    "\n",
    "    ols3_required_count = len(config.OLS3_FEATURE_NAMES)\n",
    "    if not ols3_subset_features or len(ols3_subset_features) < ols3_required_count:\n",
    "        if config.RUN_MODELS.get('OLS3H', False): print(f\"\\nADVARSEL: Ikke alle {ols3_required_count} OLS3 features funnet ({ols3_subset_features}). OLS3H deaktiveres.\")\n",
    "        config.RUN_MODELS['OLS3H'] = False\n",
    "    elif config.RUN_MODELS.get('OLS3H', False): print(f\"\\nINFO: Alle OLS3 features funnet ({ols3_subset_features}). OLS3H er aktiv.\")\n",
    "\n",
    "    feature_map = {}\n",
    "    for model, fset_key in config.MODEL_FEATURE_MAP.items():\n",
    "        if fset_key == 'ols3_features': feature_map[model] = ols3_subset_features if config.RUN_MODELS.get('OLS3H', False) else []\n",
    "        elif fset_key == 'all_numeric': feature_map[model] = all_numeric_features\n",
    "        else: print(f\"Advarsel: Ukjent feature set key '{fset_key}' for {model}.\"); feature_map[model] = all_numeric_features\n",
    "\n",
    "    # === Initialize Results Storage ===\n",
    "    all_metrics = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    all_vi = defaultdict(lambda: defaultdict(list)); all_vi_avg = defaultdict(dict)\n",
    "    all_portfolios = defaultdict(dict); all_summaries = {}\n",
    "\n",
    "    # --- Define Actual Column Names (using config now) ---\n",
    "    target_col_actual_name = config.TARGET_VARIABLE\n",
    "    next_ret_col_actual_name = config.NEXT_RETURN_VARIABLE\n",
    "    mcap_orig_col_actual_name = config.MARKET_CAP_ORIG_VARIABLE\n",
    "    date_col_actual_name = 'Date'; id_col_actual_name = 'Instrument'\n",
    "\n",
    "    essential_check_list = [date_col_actual_name, id_col_actual_name, target_col_actual_name, next_ret_col_actual_name, mcap_orig_col_actual_name]\n",
    "    missing_essentials = [c for c in essential_check_list if c not in df_clean.columns]\n",
    "    if missing_essentials: exit(f\"FEIL: N√∏dvendige kolonner mangler i df_clean: {missing_essentials}.\")\n",
    "\n",
    "    # === 5: Outer Loop: Subsets ===\n",
    "    print(f\"\\n--- Starter Subset Loop: {config.SUBSETS_TO_RUN} ---\")\n",
    "    for subset in config.SUBSETS_TO_RUN:\n",
    "        subset_start_time = datetime.datetime.now()\n",
    "        print(f\"\\n{'='*30} Starter Subset: {subset.upper()} {'='*30}\")\n",
    "\n",
    "        # --- Create Subset Data ---\n",
    "        df_subset = pd.DataFrame()\n",
    "        df_mc_temp = df_clean.dropna(subset=[date_col_actual_name, mcap_orig_col_actual_name]).copy()\n",
    "        if df_mc_temp.empty: print(f\"FEIL: Ingen data for subsetting i {subset}.\"); continue\n",
    "        if subset == 'all': df_subset = df_clean.copy(); print(\"  Bruker fullt datasett.\")\n",
    "        else:\n",
    "            print(f\"  Definerer subset basert p√• {mcap_orig_col_actual_name} persentiler...\")\n",
    "            df_mc_temp['MonthYear'] = pd.to_datetime(df_mc_temp[date_col_actual_name]).dt.to_period('M')\n",
    "            if subset == 'big':\n",
    "                cutoff_quantile = 1.0 - (config.BIG_FIRM_TOP_PERCENT / 100.0)\n",
    "                size_cutoffs = df_mc_temp.groupby('MonthYear')[mcap_orig_col_actual_name].quantile(cutoff_quantile)\n",
    "                print(f\"  -> Topp {config.BIG_FIRM_TOP_PERCENT}% (Quantile: {cutoff_quantile:.2f})\")\n",
    "                df_mc_temp = df_mc_temp.join(size_cutoffs.rename('cutoff'), on='MonthYear')\n",
    "                df_subset = df_mc_temp[df_mc_temp[mcap_orig_col_actual_name] >= df_mc_temp['cutoff']].copy()\n",
    "            elif subset == 'small':\n",
    "                cutoff_quantile = config.SMALL_FIRM_BOTTOM_PERCENT / 100.0\n",
    "                size_cutoffs = df_mc_temp.groupby('MonthYear')[mcap_orig_col_actual_name].quantile(cutoff_quantile)\n",
    "                print(f\"  -> Bunn {config.SMALL_FIRM_BOTTOM_PERCENT}% (Quantile: {cutoff_quantile:.2f})\")\n",
    "                df_mc_temp = df_mc_temp.join(size_cutoffs.rename('cutoff'), on='MonthYear')\n",
    "                df_subset = df_mc_temp[df_mc_temp[mcap_orig_col_actual_name] <= df_mc_temp['cutoff']].copy()\n",
    "            else: print(f\"FEIL: Ukjent subset '{subset}'.\"); continue\n",
    "            df_subset = df_subset.drop(columns=['MonthYear', 'cutoff'], errors='ignore')\n",
    "        if df_subset.empty: print(f\"FEIL: Subset '{subset}' er tomt.\"); continue\n",
    "        df_subset = df_subset.sort_values(by=[date_col_actual_name, id_col_actual_name]).reset_index(drop=True)\n",
    "        print(f\"  Subset '{subset}' klar. Form: {df_subset.shape}\")\n",
    "\n",
    "        # === 6: Inner Loop: Rolling Windows ===\n",
    "        try: splits = list(utils.get_yearly_rolling_splits(df_subset, config.INITIAL_TRAIN_YEARS, config.VALIDATION_YEARS, config.TEST_YEARS_PER_WINDOW))\n",
    "        except ValueError as e: print(f\"FEIL split gen for '{subset}': {e}\"); continue\n",
    "        except Exception as e_gen: print(f\"Uventet FEIL split gen for '{subset}': {e_gen}\"); traceback.print_exc(); continue\n",
    "        if not splits: print(f\"Ingen vinduer for '{subset}'.\"); continue\n",
    "        num_windows = len(splits)\n",
    "        print(f\"\\n--- Starter Rullerende Vindu Loop for Subset: {subset} ({num_windows} vinduer) ---\")\n",
    "        window_preds_list = []; last_train_idx, last_val_idx = None, None; last_models_fit = {}\n",
    "\n",
    "        for window_idx, (train_idx, val_idx, test_idx, train_dates, val_dates, test_dates) in enumerate(splits):\n",
    "            window_num = window_idx + 1; window_start_time = time.time()\n",
    "            print(f\"\\n-- Vindu {window_num}/{num_windows} ({subset}) --\")\n",
    "            # --- Use CORRECTED check for date ranges ---\n",
    "            if train_dates is not None: print(f\"  Train: {train_dates['min'].date()} -> {train_dates['max'].date()} ({len(train_idx)} obs)\")\n",
    "            if val_dates is not None:   print(f\"  Val:   {val_dates['min'].date()} -> {val_dates['max'].date()} ({len(val_idx)} obs)\")\n",
    "            if test_dates is not None:  print(f\"  Test:  {test_dates['min'].date()} -> {test_dates['max'].date()} ({len(test_idx)} obs)\")\n",
    "            # --- End corrected check ---\n",
    "            if test_idx.empty or train_idx.empty: print(\"  Advarsel: Tomt train/test sett.\"); continue\n",
    "            needs_val_set = lambda name: name not in ['OLS', 'OLS3H', 'ENET']\n",
    "            if val_idx.empty and any(config.RUN_MODELS[m] and needs_val_set(m) for m in config.RUN_MODELS): print(\"  Advarsel: Tomt val set, men trengs.\"); continue\n",
    "\n",
    "            y_train = df_subset.loc[train_idx, target_col_actual_name].values\n",
    "            y_val = df_subset.loc[val_idx, target_col_actual_name].values if not val_idx.empty else np.array([])\n",
    "            y_test = df_subset.loc[test_idx, target_col_actual_name].values\n",
    "            y_train_val = np.concatenate((y_train, y_val)) if not val_idx.empty else y_train\n",
    "\n",
    "            window_results = {date_col_actual_name: df_subset.loc[test_idx, date_col_actual_name].values, id_col_actual_name: df_subset.loc[test_idx, id_col_actual_name].values, target_col_actual_name: y_test}\n",
    "            nan_preds = np.full(len(test_idx), np.nan)\n",
    "            for model_name_init, run_flag in config.RUN_MODELS.items():\n",
    "                if run_flag: window_results[f'yhat_{model_name_init.lower()}'] = nan_preds.copy()\n",
    "            window_models_fitted_this_run = {}\n",
    "\n",
    "            # === 7: Innermost Loop: Models ===\n",
    "            for model_name, do_run in config.RUN_MODELS.items():\n",
    "                if not do_run: continue\n",
    "                if model_name == 'OLS3H' and (not STATSMODELS_AVAILABLE or not config.RUN_MODELS['OLS3H']): continue\n",
    "                if model_name.startswith('NN') and not TENSORFLOW_AVAILABLE: continue\n",
    "                print(f\"  -> Trener/Evaluerer: {model_name}...\")\n",
    "                model_start_time = time.time(); fitted_model, preds_oos, preds_is, optimal_hyperparams = None, np.array([]), np.array([]), {}; y_is_target = np.array([])\n",
    "                current_features = feature_map.get(model_name); current_features = [f for f in current_features if f in df_subset.columns]\n",
    "                if not current_features: print(f\"    Advarsel: Ingen features for {model_name}.\"); continue\n",
    "                X_train = df_subset.loc[train_idx, current_features].values; X_val = df_subset.loc[val_idx, current_features].values if not val_idx.empty else np.empty((0, len(current_features)))\n",
    "                X_test = df_subset.loc[test_idx, current_features].values; X_train_val = np.vstack((X_train, X_val)) if not val_idx.empty else X_train\n",
    "                min_obs_train = max(2, X_train.shape[1] + 1) if model_name == 'OLS3H' else 2\n",
    "                if X_train.shape[0] < min_obs_train or np.isnan(y_train).all(): print(f\"    Advarsel: Utilstrekkelig train data.\"); continue\n",
    "                if val_idx.empty and needs_val_set(model_name): print(f\"    Advarsel: Tomt val set.\"); continue\n",
    "                if not val_idx.empty and (X_val.shape[0] < 2 or np.isnan(y_val).all()) and needs_val_set(model_name): print(f\"    Advarsel: Utilstrekkelig val data.\"); continue\n",
    "\n",
    "                try:\n",
    "                    train_func_name = f\"train_evaluate_{model_name.lower().replace('-', '').replace('+', '_')}\"\n",
    "                    train_function = locals().get(train_func_name)\n",
    "                    if train_function:\n",
    "                        model_config_params = config.MODEL_PARAMS.get(model_name, {})\n",
    "                        if model_name in ['OLS', 'OLS3H']: fitted_model, preds_oos, preds_is, optimal_hyperparams = train_function(X_train_val, y_train_val, X_test, model_config_params); y_is_target = y_train_val\n",
    "                        elif model_name == 'ENET': fitted_model, preds_oos, preds_is, optimal_hyperparams = train_function(X_train, y_train, X_test, model_config_params); y_is_target = y_train\n",
    "                        elif model_name.startswith('NN'):\n",
    "                             if TENSORFLOW_AVAILABLE: nn_specific_config = config.MODEL_PARAMS.get(model_name, {}); fitted_model, preds_oos, preds_is, optimal_hyperparams = train_function(X_train, y_train, X_val, y_val, X_test, config.MODEL_PARAMS, nn_specific_config); y_is_target = y_train_val\n",
    "                             else: continue\n",
    "                        else: fitted_model, preds_oos, preds_is, optimal_hyperparams = train_function(X_train, y_train, X_val, y_val, X_test, model_config_params); y_is_target = y_train_val\n",
    "\n",
    "                        if preds_oos is not None and preds_is is not None and len(preds_oos) == len(y_test):\n",
    "                            preds_oos_finite=preds_oos[np.isfinite(preds_oos)]; y_test_aligned_oos=y_test[np.isfinite(preds_oos)]; preds_is_finite=preds_is[np.isfinite(preds_is)]; y_is_target_aligned_is=y_is_target[np.isfinite(preds_is)]\n",
    "                            r2_oos=utils.calculate_oos_r2(y_test_aligned_oos, preds_oos_finite) if len(y_test_aligned_oos)>=2 else np.nan; r2_is=utils.calculate_oos_r2(y_is_target_aligned_is, preds_is_finite) if len(y_is_target_aligned_is)>=2 else np.nan; sharpe_oos=utils.calculate_sharpe_of_predictions(preds_oos_finite) if len(preds_oos_finite)>=2 else np.nan\n",
    "                            all_metrics[subset][model_name]['oos_r2'].append(r2_oos); all_metrics[subset][model_name]['is_r2_train_val'].append(r2_is); all_metrics[subset][model_name]['oos_sharpe'].append(sharpe_oos)\n",
    "                            for param_name, param_value in optimal_hyperparams.items(): all_metrics[subset][model_name][f'optim_{param_name}'].append(param_value)\n",
    "                            pred_col_name=f'yhat_{model_name.lower()}'; window_results[pred_col_name]=preds_oos\n",
    "                            if fitted_model is not None: window_models_fitted_this_run[model_name] = fitted_model\n",
    "                            print(f\"    {model_name}: OOS R¬≤={r2_oos:.4f}, IS R¬≤={r2_is:.4f}, Sharpe={sharpe_oos:.3f} ({time.time()-model_start_time:.1f}s)\")\n",
    "                            if config.CALCULATE_VI and fitted_model is not None and config.MODEL_VI_STRATEGY.get(model_name) == 'per_window':\n",
    "                                if pd.notna(r2_is):\n",
    "                                    vi_start_time = time.time(); X_eval_vi = X_train_val if model_name not in ['ENET'] else X_train; y_eval_vi = y_is_target\n",
    "                                    if model_name == 'OLS3H': finite_mask_vi=np.isfinite(y_eval_vi);\n",
    "                                    if not np.all(finite_mask_vi): X_eval_vi=X_eval_vi[finite_mask_vi]; y_eval_vi=y_eval_vi[finite_mask_vi]\n",
    "                                    if X_eval_vi.shape[0] > 0 and y_eval_vi.shape[0] > 0:\n",
    "                                        vi_df = utils.calculate_variable_importance(model_name, fitted_model, X_eval_vi, y_eval_vi, current_features, r2_is, config.VI_METHOD, optimal_hyperparams)\n",
    "                                        if vi_df is not None and not vi_df.empty: all_vi[subset][model_name].append(vi_df)\n",
    "                                    else: print(f\"      Advarsel: Ingen data for VI for {model_name}.\")\n",
    "                                else: print(f\"    Advarsel: Hopper VI for {model_name} pga. NaN IS R2.\")\n",
    "                        else: print(f\"    Advarsel: {model_name} ingen/feil pred.\"); all_metrics[subset][model_name]['oos_r2'].append(np.nan); all_metrics[subset][model_name]['is_r2_train_val'].append(np.nan); all_metrics[subset][model_name]['oos_sharpe'].append(np.nan)\n",
    "                    else: print(f\"    FEIL: Treningsfunksjon '{train_func_name}' ikke funnet.\"); continue\n",
    "                except Exception as e_train: print(f\"    !!! KRITISK FEIL {model_name}: {e_train}\"); traceback.print_exc(); all_metrics[subset][model_name]['oos_r2'].append(np.nan); all_metrics[subset][model_name]['is_r2_train_val'].append(np.nan); all_metrics[subset][model_name]['oos_sharpe'].append(np.nan)\n",
    "\n",
    "            # End Model Loop\n",
    "            window_preds_list.append(pd.DataFrame(window_results))\n",
    "            if window_idx == num_windows - 1: last_train_idx=train_idx.copy(); last_val_idx=val_idx.copy() if not val_idx.empty else None; last_models_fit=window_models_fitted_this_run.copy()\n",
    "            print(f\"-- Vindu {window_num} ({subset}) fullf√∏rt ({time.time() - window_start_time:.1f}s) --\")\n",
    "        # End Window Loop\n",
    "\n",
    "        # === 8-10: Post-Window Analysis for the Subset ===\n",
    "        if not window_preds_list: print(f\"\\nFEIL: Ingen vindusprediksjoner for '{subset}'.\"); continue\n",
    "        print(f\"\\n--- Analyserer resultater for Subset: {subset} ---\"); results_df_subset=pd.concat(window_preds_list).reset_index(drop=True)\n",
    "        prediction_cols_subset = [c for c in results_df_subset.columns if c.startswith('yhat_')]\n",
    "        if not prediction_cols_subset: print(f\"FEIL: Ingen prediksjonskolonner for '{subset}'.\"); continue\n",
    "\n",
    "        # Calculate Overall OOS R2 (Gu Definition)\n",
    "        print(f\"\\n--- Overall OOS R¬≤ (Gu-stil) for Subset: {subset} ---\"); y_true_overall=results_df_subset[target_col_actual_name]; y_true_finite_overall=y_true_overall[np.isfinite(y_true_overall)]; ss_tot_overall=np.sum(y_true_finite_overall**2)\n",
    "        if len(y_true_finite_overall) > 1 and ss_tot_overall > 1e-15:\n",
    "            for pred_col in prediction_cols_subset:\n",
    "                model_name_oos=pred_col.replace('yhat_', '').upper(); y_pred_overall=results_df_subset[pred_col]; mask_overall=np.isfinite(y_true_overall)&np.isfinite(y_pred_overall); y_t_o=y_true_overall[mask_overall]; y_p_o=y_pred_overall[mask_overall]\n",
    "                if len(y_t_o) >= 2: r2_overall_gu=utils.calculate_oos_r2(y_t_o, y_p_o); all_metrics[subset][model_name_oos]['oos_r2_overall_gu']=r2_overall_gu; print(f\"  {model_name_oos}: {r2_overall_gu:.6f}\")\n",
    "                else: print(f\"  {model_name_oos}: N/A\"); all_metrics[subset][model_name_oos]['oos_r2_overall_gu']=np.nan\n",
    "        else: print(\"  Kan ikke beregne overall OOS R2.\")\n",
    "\n",
    "        # Perform Portfolio Analysis\n",
    "        print(f\"\\n--- Starter Detaljert Portef√∏ljeanalyse for Subset: {subset} ---\")\n",
    "        decile_tables, hl_risk_tables, long_risk_tables = utils.perform_detailed_portfolio_analysis(results_df_subset, df_clean, prediction_cols_subset, mcap_orig_col_actual_name, next_ret_col_actual_name, config.FILTER_SMALL_CAPS_PORTFOLIO, config.ANNUALIZATION_FACTOR, config.BENCHMARK_FILE, config.FF_FACTOR_FILE)\n",
    "        all_portfolios[subset]={'decile_tables': decile_tables, 'hl_risk_tables': hl_risk_tables, 'long_risk_tables': long_risk_tables}\n",
    "\n",
    "        # Calculate Average/Last Window Variable Importance\n",
    "        if config.CALCULATE_VI:\n",
    "            print(f\"\\n--- Beregner Variabel Viktighet (VI) for Subset: {subset} ---\")\n",
    "            for model_name, do_run in config.RUN_MODELS.items():\n",
    "                if not do_run: continue\n",
    "                if model_name=='OLS3H' and (not STATSMODELS_AVAILABLE or not config.RUN_MODELS['OLS3H']): continue\n",
    "                if model_name.startswith('NN') and not TENSORFLOW_AVAILABLE: continue\n",
    "                vi_strategy=config.MODEL_VI_STRATEGY.get(model_name); current_features_vi=feature_map.get(model_name); current_features_vi=[f for f in current_features_vi if f in df_subset.columns]\n",
    "                if not current_features_vi: continue\n",
    "                if vi_strategy=='per_window':\n",
    "                    vi_list_model = all_vi[subset].get(model_name, [])\n",
    "                    if vi_list_model:\n",
    "                        try:\n",
    "                            avg_vi_df=pd.concat(vi_list_model).groupby('Feature')['Importance'].mean().reset_index(); total_avg_importance=avg_vi_df['Importance'].sum(); avg_vi_df['Importance']=avg_vi_df['Importance']/total_avg_importance if total_avg_importance > 1e-9 else 0.0\n",
    "                            all_vi_avg[subset][model_name]=avg_vi_df.sort_values('Importance', ascending=False).reset_index(drop=True); print(f\"  VI (Avg/Window) beregnet for {model_name}.\")\n",
    "                        except Exception as e_vi_avg: print(f\"  FEIL VI avg for {model_name}: {e_vi_avg}\")\n",
    "                    else: print(f\"  Ingen 'per_window' VI data for {model_name}.\")\n",
    "                elif vi_strategy=='last_window':\n",
    "                    print(f\"  Beregner 'last_window' VI for {model_name}...\")\n",
    "                    if last_train_idx is None or model_name not in last_models_fit: print(f\"    Hoppet over (mangler data/modell).\"); continue\n",
    "                    last_model_instance=last_models_fit[model_name]; last_is_r2=all_metrics[subset][model_name]['is_r2_train_val'][-1] if all_metrics[subset][model_name]['is_r2_train_val'] else np.nan\n",
    "                    if pd.isna(last_is_r2): print(f\"    Advarsel: IS R2 siste vindu NaN.\"); continue\n",
    "                    if model_name=='ENET': X_eval_last=df_subset.loc[last_train_idx, current_features_vi].values; y_eval_last=df_subset.loc[last_train_idx, target_col_actual_name].values\n",
    "                    else: last_full_idx=last_train_idx.union(last_val_idx) if last_val_idx is not None else last_train_idx; X_eval_last=df_subset.loc[last_full_idx, current_features_vi].values; y_eval_last=df_subset.loc[last_full_idx, target_col_actual_name].values\n",
    "                    last_optimal_params={k.replace('optim_', ''): v[-1] for k, v in all_metrics[subset][model_name].items() if k.startswith('optim_') and v}\n",
    "                    vi_df_last=utils.calculate_variable_importance(model_name, last_model_instance, X_eval_last, y_eval_last, current_features_vi, last_is_r2, config.VI_METHOD, last_optimal_params)\n",
    "                    if vi_df_last is not None and not vi_df_last.empty: all_vi_avg[subset][model_name]=vi_df_last.sort_values('Importance', ascending=False).reset_index(drop=True); print(f\"    VI ({model_name}, siste vindu) beregnet.\")\n",
    "                    else: print(f\"    VI beregning siste vindu ({model_name}) mislyktes.\")\n",
    "\n",
    "        # Generate Summary Table, Plot Complexity, Plot VI\n",
    "        all_summaries[subset] = utils.create_summary_table(all_metrics[subset], config.ANNUALIZATION_FACTOR)\n",
    "        utils.plot_time_varying_complexity(all_metrics[subset], config.COMPLEXITY_PARAMS_TO_PLOT)\n",
    "        if config.CALCULATE_VI and all_vi_avg[subset]:\n",
    "             print(f\"\\n--- Plotter Variabel Viktighet for Subset: {subset} ---\")\n",
    "             for model_name, vi_df in all_vi_avg[subset].items():\n",
    "                 plt.figure(figsize=(10, max(6, min(len(vi_df), config.VI_PLOT_TOP_N) * 0.3)))\n",
    "                 plot_df = vi_df[vi_df['Importance'] > 1e-6].head(config.VI_PLOT_TOP_N).sort_values(by='Importance', ascending=True)\n",
    "                 if not plot_df.empty: plt.barh(plot_df['Feature'], plot_df['Importance']); plt.xlabel(\"Relativ Viktighet\"); plt.title(f\"{model_name} VI ({subset} - Top {len(plot_df)})\"); plt.tight_layout(); plt.show()\n",
    "                 else: print(f\"  Ingen VI data √• plotte for {model_name} ({subset}).\"); plt.close()\n",
    "\n",
    "        # Save Results\n",
    "        results_to_save={'summary_metrics': all_summaries[subset], 'portfolio_deciles': all_portfolios[subset].get('decile_tables', {}), 'portfolio_hl_risk': all_portfolios[subset].get('hl_risk_tables', {}), 'portfolio_long_risk': all_portfolios[subset].get('long_risk_tables', {}), 'variable_importance_avg': all_vi_avg[subset]}\n",
    "        utils.save_results(config.OUTPUT_DIR, subset, results_to_save)\n",
    "        subset_end_time = datetime.datetime.now()\n",
    "        print(f\"\\n{'='*30} Subset Fullf√∏rt: {subset.upper()} (Tid: {subset_end_time - subset_start_time}) {'='*30}\")\n",
    "    # End Subset Loop\n",
    "\n",
    "    # === Final Reporting ===\n",
    "    print(\"\\n\\n\" + \"=\"*35 + \" SLUTTSAMMENDRAG \" + \"=\"*35)\n",
    "    r2_final_data = defaultdict(dict)\n",
    "    for sub in config.SUBSETS_TO_RUN:\n",
    "        if sub in all_metrics:\n",
    "            for model, metrics in all_metrics[sub].items(): r2_final_data[sub][model] = metrics.get('oos_r2_overall_gu', np.nan) * 100\n",
    "    if r2_final_data:\n",
    "        r2_summary_final = pd.DataFrame.from_dict(r2_final_data, orient='index')\n",
    "        model_order_final = [m for m in config.RUN_MODELS if m in r2_summary_final.columns and config.RUN_MODELS[m]]\n",
    "        other_models = sorted([m for m in r2_summary_final.columns if m not in model_order_final])\n",
    "        r2_summary_final = r2_summary_final.reindex(columns=model_order_final + other_models, fill_value=np.nan)\n",
    "        r2_summary_final.index.name=\"Subset\"; r2_summary_final.columns.name=\"Model\"\n",
    "        print(\"\\n--- Tabell 1 Stil: Overall Monthly OOS R¬≤ (%) [Gu et al. Def] ---\")\n",
    "        print(r2_summary_final.to_string(float_format=lambda x: f\"{x:.3f}\" if pd.notna(x) else \"N/A\", na_rep=\"N/A\"))\n",
    "        utils.save_results(config.OUTPUT_DIR, \"consolidated\", {\"R2_summary_table1_style\": r2_summary_final})\n",
    "    else: print(\"\\nIngen data for endelig OOS R2-oppsummering.\")\n",
    "    overall_end_time = datetime.datetime.now()\n",
    "    print(f\"\\n--- Pipeline Fullf√∏rt ---\"); print(f\"--- Sluttidspunkt: {overall_end_time:%Y-%m-%d %H:%M:%S} ---\"); print(f\"--- Total Kj√∏retid: {overall_end_time - overall_start_time} ---\"); print(f\"--- Resultater lagret i: {config.OUTPUT_DIR} ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
