{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2f425b98",
      "metadata": {
        "id": "2f425b98"
      },
      "source": [
        "# GBRT + H"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "2lPbE_2tt41J",
        "outputId": "8fdb1e5c-8f23-4f8a-a4d5-aa2f1b4e95ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "id": "2lPbE_2tt41J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-72de3401-eec4-4762-827c-19a037a7ffaa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-72de3401-eec4-4762-827c-19a037a7ffaa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Cleaned_OSEFX_Market_Macro_Data.csv to Cleaned_OSEFX_Market_Macro_Data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fadb8e35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fadb8e35",
        "outputId": "80ad98b5-98a5-4761-ebb5-97e5e19ccb49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADVARSEL: FF faktorfil 'Europe_4_Factors_Monthly.csv' ikke funnet. Faktorregresjoner deaktivert.\n",
            "\n",
            "==================== Starter Kjøring: GBRT-H for 'All' Firms (ÅRLIG Refitting) ====================\n",
            "Starttid: 2025-04-02 01:26:50\n",
            "\n",
            "--- Steg 1: Laster og Forbereder Rådata ---\n",
            "Laster data fra: Cleaned_OSEFX_Market_Macro_Data.csv\n",
            "Data lastet inn. Form: (34476, 34)\n",
            "Dato konvertert og data sortert.\n",
            "Månedlig avkastning ('MonthlyReturn') beregnet/winsorisert.\n",
            "Risikojustert avkastning ('TargetReturn_t') beregnet (modellens y).\n",
            "Neste måneds rå avkastning ('NextMonthlyReturn_t+1') beregnet.\n",
            "Markedsverdi ('MarketCap') beregnet.\n",
            "Sektor dummy-variabler opprettet.\n",
            "Kolonnenavn renset.\n",
            "Log-transformerer spesifikke variabler...\n",
            "Log-transformasjon fullført.\n",
            "\n",
            "--- Steg 1.1: Lager Subset: All ---\n",
            "Subset 'All' initiell form: (34476, 52)\n",
            "\n",
            "--- Steg 2 & 1.5: Definerer Features og Rank Standardiserer ---\n",
            "Identifiserer numeriske features...\n",
            "  Funnet 50 numeriske kolonner totalt.\n",
            "  Identifisert 39 features for bruk i modellen.\n",
            "Rank standardiserer 39 features...\n",
            "Rank standardisering fullført.\n",
            "\n",
            "--- Steg 3: Renser Data (Missing/Inf/Filters) ---\n",
            "Starter datarensing (missing/inf)...\n",
            "  NaNs imputert med median i 0 feature kolonner.\n",
            "  Fjernet 188 rader pga. NaN i essensielle kolonner: ['Date', 'Instrument', 'MarketCap_orig', 'NextMonthlyReturn_t+1', 'TargetReturn_t']\n",
            "  Fjernet 8 rader der MarketCap_orig <= 0.\n",
            "Datarensing fullført. Form: (34280, 52). Fjernet totalt 196 rader.\n",
            "  Antall features etter rensing: 39\n",
            "\n",
            "--- Steg 4: Setter opp ÅRLIG Rullerende Vindu (InitTrain=9, Val=6, Test=1) ---\n",
            "Unike år i data: 30 (1995 - 2024)\n",
            "Genererer 15 årlige rullerende vinduer... (Første testår: 2010, Siste startår for test: 2024)\n",
            "\n",
            "  Vindu 1/15: Train 1995-2003 (4658 obs, 1995-01 to 2003-12), Val 2004-2009 (5459 obs, 2004-01 to 2009-12), Test 2010-2010 (1107 obs, 2010-01 to 2010-12)\n",
            "\n",
            "  Vindu 2/15: Train 1995-2004 (5348 obs, 1995-01 to 2004-12), Val 2005-2010 (5876 obs, 2005-01 to 2010-12), Test 2011-2011 (1165 obs, 2011-01 to 2011-12)\n",
            "\n",
            "  Vindu 3/15: Train 1995-2005 (6122 obs, 1995-01 to 2005-12), Val 2006-2011 (6267 obs, 2006-01 to 2011-12), Test 2012-2012 (1186 obs, 2012-01 to 2012-12)\n",
            "\n",
            "  Vindu 4/15: Train 1995-2006 (7017 obs, 1995-01 to 2006-12), Val 2007-2012 (6558 obs, 2007-01 to 2012-12), Test 2013-2013 (1210 obs, 2013-01 to 2013-12)\n",
            "\n",
            "  Vindu 5/15: Train 1995-2007 (8019 obs, 1995-01 to 2007-12), Val 2008-2013 (6766 obs, 2008-01 to 2013-12), Test 2014-2014 (1316 obs, 2014-01 to 2014-12)\n",
            "\n",
            "  Vindu 6/15: Train 1995-2008 (9061 obs, 1995-01 to 2008-12), Val 2009-2014 (7040 obs, 2009-01 to 2014-12), Test 2015-2015 (1451 obs, 2015-01 to 2015-12)\n",
            "\n",
            "  Vindu 7/15: Train 1995-2009 (10117 obs, 1995-01 to 2009-12), Val 2010-2015 (7435 obs, 2010-01 to 2015-12), Test 2016-2016 (1496 obs, 2016-01 to 2016-12)\n",
            "\n",
            "  Vindu 8/15: Train 1995-2010 (11224 obs, 1995-01 to 2010-12), Val 2011-2016 (7824 obs, 2011-01 to 2016-12), Test 2017-2017 (1536 obs, 2017-01 to 2017-12)\n",
            "\n",
            "  Vindu 9/15: Train 1995-2011 (12389 obs, 1995-01 to 2011-12), Val 2012-2017 (8195 obs, 2012-01 to 2017-12), Test 2018-2018 (1632 obs, 2018-01 to 2018-12)\n",
            "\n",
            "  Vindu 10/15: Train 1995-2012 (13575 obs, 1995-01 to 2012-12), Val 2013-2018 (8641 obs, 2013-01 to 2018-12), Test 2019-2019 (1707 obs, 2019-01 to 2019-12)\n",
            "\n",
            "  Vindu 11/15: Train 1995-2013 (14785 obs, 1995-01 to 2013-12), Val 2014-2019 (9138 obs, 2014-01 to 2019-12), Test 2020-2020 (1864 obs, 2020-01 to 2020-12)\n",
            "\n",
            "  Vindu 12/15: Train 1995-2014 (16101 obs, 1995-01 to 2014-12), Val 2015-2020 (9686 obs, 2015-01 to 2020-12), Test 2021-2021 (2081 obs, 2021-01 to 2021-12)\n",
            "\n",
            "  Vindu 13/15: Train 1995-2015 (17552 obs, 1995-01 to 2015-12), Val 2016-2021 (10316 obs, 2016-01 to 2021-12), Test 2022-2022 (2169 obs, 2022-01 to 2022-12)\n",
            "\n",
            "  Vindu 14/15: Train 1995-2016 (19048 obs, 1995-01 to 2016-12), Val 2017-2022 (10989 obs, 2017-01 to 2022-12), Test 2023-2023 (2193 obs, 2023-01 to 2023-12)\n",
            "\n",
            "  Vindu 15/15: Train 1995-2017 (20584 obs, 1995-01 to 2017-12), Val 2018-2023 (11646 obs, 2018-01 to 2023-12), Test 2024-2024 (2050 obs, 2024-01 to 2024-11)\n",
            "\n",
            "Antall årlige rullerende vinduer som skal kjøres: 15\n",
            "\n",
            "--- Starter GBRT-H ÅRLIG Rullerende Vindu Trening & Prediksjon ---\n",
            "------------------------------------------------------------\n",
            "Behandler Vindu 1/15...\n",
            "  Beregner variabelviktighet for Vindu 1 (GBRT Retraining - kan ta tid)...\n",
            "    Variabelviktighet beregnet på 74.95 sekunder.\n",
            "  Vindu 1 fullført på 105.9s. Optimal Depth: 7, Window OOS R2: 0.2140\n",
            "------------------------------------------------------------\n",
            "Behandler Vindu 2/15...\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# --- IMPORTS ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats.mstats import winsorize\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "# *** Import GBRT ***\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "# *** Import for Factor Regressions ***\n",
        "import statsmodels.api as sm # <--- Un-commented for portfolio analysis\n",
        "import datetime\n",
        "import warnings\n",
        "import traceback\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import time # For timing VI\n",
        "\n",
        "# --- WARNINGS CONFIGURATION ---\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"pandas\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Mean of empty slice\")\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"invalid value encountered in log\")\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Maximum number of iterations reached.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"invalid value encountered in divide\") # Added for Sharpe ratio etc.\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Degrees of freedom <= 0 for slice\") # Added for std dev etc.\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# FUNCTION DEFINITIONS\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "# Step 1: Load and Prepare Dataset\n",
        "def load_prepare_data(file_path):\n",
        "    \"\"\" Loads, cleans, calculates returns/market cap, handles dates/IDs. \"\"\"\n",
        "    print(f\"Laster data fra: {file_path}\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, low_memory=False)\n",
        "    except FileNotFoundError: print(f\"FEIL: Fil '{file_path}' ikke funnet.\"); return None\n",
        "    print(f\"Data lastet inn. Form: {df.shape}\")\n",
        "\n",
        "    # Standardize essential column names (adjust if your CSV uses different names)\n",
        "    date_col = 'Date' if 'Date' in df.columns else 'eom'\n",
        "    id_col = 'Instrument' if 'Instrument' in df.columns else 'id'\n",
        "    price_col = 'ClosePrice' if 'ClosePrice' in df.columns else 'prc'\n",
        "    shares_col = 'CommonSharesOutstanding'\n",
        "    rf_col = 'NorgesBank10Y'\n",
        "    sector_col = 'EconomicSector'\n",
        "\n",
        "    if date_col not in df.columns: print(\"FEIL: Dato-kolonne mangler.\"); return None\n",
        "    if id_col not in df.columns: print(\"FEIL: Instrument-ID mangler.\"); return None\n",
        "    if price_col not in df.columns: print(f\"FEIL: Pris-kolonne ('{price_col}') mangler.\"); return None\n",
        "\n",
        "    df = df.rename(columns={date_col: 'Date', id_col: 'Instrument'})\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df = df.sort_values(by=[\"Instrument\", \"Date\"]).reset_index(drop=True)\n",
        "    print(\"Dato konvertert og data sortert.\")\n",
        "\n",
        "    # Return Calculation\n",
        "    df[\"MonthlyReturn\"] = df.groupby(\"Instrument\")[price_col].pct_change()\n",
        "    df[\"MonthlyReturn\"].fillna(0, inplace=True)\n",
        "    df[\"MonthlyReturn\"] = winsorize(df[\"MonthlyReturn\"].values, limits=[0.01, 0.01])\n",
        "    print(\"Månedlig avkastning ('MonthlyReturn') beregnet/winsorisert.\")\n",
        "\n",
        "    # Target Variable (Adjusted Return)\n",
        "    if rf_col not in df.columns: df[rf_col] = 0; print(f\"ADVARSEL: '{rf_col}' mangler, bruker 0.\")\n",
        "    # Handle potential percentage vs decimal format for risk-free rate\n",
        "    df[\"MonthlyRiskFreeRate_t\"] = df[rf_col] / 12 / 100 if df[rf_col].abs().max() > 1 else df[rf_col] / 12\n",
        "    df[\"TargetReturn_t\"] = df[\"MonthlyReturn\"] - df[\"MonthlyRiskFreeRate_t\"] # Model target y\n",
        "    print(\"Risikojustert avkastning ('TargetReturn_t') beregnet (modellens y).\")\n",
        "\n",
        "    # Next Month's RAW Return (for portfolio eval)\n",
        "    df['NextMonthlyReturn_t+1'] = df.groupby('Instrument')['MonthlyReturn'].shift(-1)\n",
        "    print(\"Neste måneds rå avkastning ('NextMonthlyReturn_t+1') beregnet.\")\n",
        "\n",
        "    # Market Cap\n",
        "    if shares_col not in df.columns: print(f\"FEIL: '{shares_col}' mangler for MarketCap.\"); return None\n",
        "    df[\"MarketCap\"] = df[price_col] * df[shares_col]\n",
        "    df['MarketCap_orig'] = df['MarketCap'].copy() # Keep original market cap for portfolio weighting\n",
        "    df['MarketCap'] = df['MarketCap'].fillna(0)\n",
        "    print(\"Markedsverdi ('MarketCap') beregnet.\")\n",
        "\n",
        "    # Sector Dummies (Optional)\n",
        "    if sector_col in df.columns:\n",
        "        df = pd.get_dummies(df, columns=[sector_col], prefix=\"Sector\", dtype=int)\n",
        "        print(\"Sektor dummy-variabler opprettet.\")\n",
        "\n",
        "    # Clean Column Names\n",
        "    df.columns = df.columns.str.replace(\" \", \"_\").str.replace(\"-\", \"_\")\n",
        "    print(\"Kolonnenavn renset.\")\n",
        "\n",
        "    # Log Transformation\n",
        "    print(\"Log-transformerer spesifikke variabler...\")\n",
        "    vars_to_log = [\"MarketCap\", \"BM\", \"ClosePrice\", \"Volume\", \"CommonSharesOutstanding\"]\n",
        "    vars_to_log = [v if v in df.columns else v.lower() for v in vars_to_log]\n",
        "    vars_to_log = [v if v in df.columns else 'prc' if v == 'closeprice' else v for v in vars_to_log]\n",
        "    vars_to_log = [col for col in vars_to_log if col in df.columns]\n",
        "    for var in vars_to_log:\n",
        "        if var in df.columns and pd.api.types.is_numeric_dtype(df[var]):\n",
        "             # Added check for non-positive values before log\n",
        "             df[f\"{var}_positive\"] = df[var].where(df[var] > 1e-9, np.nan) # Use small threshold > 0\n",
        "             df[f\"log_{var}\"] = np.log(df[f\"{var}_positive\"])\n",
        "             log_median = df[f\"log_{var}\"].median() # Calculate median of the log-transformed values\n",
        "             df[f\"log_{var}\"] = df[f\"log_{var}\"].fillna(log_median) # Impute NaNs in log-transformed col with log median\n",
        "             if pd.isna(log_median): df[f\"log_{var}\"] = df[f\"log_{var}\"].fillna(0) # If median is NaN (rare), fill with 0\n",
        "             df.drop(columns=[f\"{var}_positive\"], inplace=True) # Drop temporary positive column\n",
        "        # else: print(f\"  ADVARSEL: Kolonne '{var}' ikke funnet eller ikke numerisk, hopper over log.\") # Keep less verbose\n",
        "    print(\"Log-transformasjon fullført.\")\n",
        "\n",
        "    # Ensure MarketCap_orig exists after log transform potentially overwriting MarketCap\n",
        "    if 'MarketCap_orig' not in df.columns and 'MarketCap' in df.columns:\n",
        "         df['MarketCap_orig'] = df['MarketCap'].copy() # Recopy if overwritten\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Step 1.5: Rank Standardization\n",
        "def rank_standardize_features(df, features_to_standardize):\n",
        "    print(f\"Rank standardiserer {len(features_to_standardize)} features...\")\n",
        "    if 'Date' not in df.columns: print(\"FEIL: 'Date' mangler.\"); return df\n",
        "    features_present = [f for f in features_to_standardize if f in df.columns]\n",
        "    if len(features_present) < len(features_to_standardize):\n",
        "        missing = [f for f in features_to_standardize if f not in features_present]; print(f\"  ADVARSEL: Features manglet for standardisering: {missing}\")\n",
        "    if not features_present: print(\"  Ingen features å standardisere.\"); return df\n",
        "    # Define the transformation function (handles potential non-numeric gracefully)\n",
        "    def rank_transform(x):\n",
        "        x_numeric = pd.to_numeric(x, errors='coerce') # Convert to numeric, coercing errors to NaT/NaN\n",
        "        ranks = x_numeric.rank(pct=True, na_option='keep') # Rank, keeping NaNs\n",
        "        return ranks * 2 - 1 # Scale to [-1, 1]\n",
        "    try:\n",
        "        # Apply the transformation using transform (faster)\n",
        "        ranked_cols = df.groupby('Date')[features_present].transform(rank_transform)\n",
        "        df[features_present] = ranked_cols\n",
        "    except Exception as e:\n",
        "        print(f\"  ADVARSEL under rank standardisering (transform): {e}. Prøver apply (langsommere).\")\n",
        "        # Fallback using apply (slower but sometimes more robust)\n",
        "        try:\n",
        "            df_std = df.set_index('Date')\n",
        "            for col in features_present:\n",
        "                df_std[col] = df_std.groupby(level=0)[col].apply(rank_transform)\n",
        "            df = df_std.reset_index()\n",
        "        except Exception as e2:\n",
        "            print(f\"  FEIL: Også alternativ standardisering feilet: {e2}\"); return df # Return original df if fallback fails\n",
        "    print(\"Rank standardisering fullført.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# Step 2: Define Feature Sets\n",
        "def define_features(df):\n",
        "    print(\"Identifiserer numeriske features...\")\n",
        "    if df is None or df.empty: print(\"  FEIL: DataFrame er tom.\"); return []\n",
        "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    print(f\"  Funnet {len(numeric_cols)} numeriske kolonner totalt.\")\n",
        "    # Define columns to exclude from features\n",
        "    cols_to_exclude = [\n",
        "        'Instrument', 'Date', 'level_0', 'index', 'Year', 'MonthYear', # Identifiers, helpers\n",
        "        'TargetReturn_t', 'NextMonthlyReturn_t+1', 'MonthlyReturn', 'MonthlyRiskFreeRate_t', # Target & related returns\n",
        "        'MarketCap_orig', # Original market cap (used for weighting, not feature)\n",
        "        'rank', 'DecileRank', 'ew_weights', 'vw_weights', # Portfolio construction helpers\n",
        "    ]\n",
        "    # Add any other dynamically generated return columns\n",
        "    cols_to_exclude.extend([col for col in df.columns if 'return_stock' in col or 'return_portfolio' in col])\n",
        "    # Handle log-transformed features: exclude the original if log version exists\n",
        "    log_cols = [col for col in numeric_cols if col.startswith('log_')]\n",
        "    originals_of_log = [col.replace('log_','') for col in log_cols]\n",
        "    # Specify common originals we definitely want to exclude if log exists\n",
        "    common_originals = ['MarketCap', 'ClosePrice', 'prc', 'Volume', 'CommonSharesOutstanding', 'BM']\n",
        "    originals_to_exclude = [orig for orig in originals_of_log if orig in df.columns and orig in common_originals]\n",
        "    cols_to_exclude.extend(originals_to_exclude)\n",
        "    cols_to_exclude = list(set(cols_to_exclude)) # Remove duplicates\n",
        "\n",
        "    # Identify potential features\n",
        "    potential_features = [col for col in numeric_cols if col not in cols_to_exclude]\n",
        "    # Final check: Ensure feature is in DataFrame, has more than 1 unique value (excluding NaN), and has non-zero std dev\n",
        "    final_features = []\n",
        "    for col in potential_features:\n",
        "        if col in df.columns:\n",
        "            series = df[col].dropna() # Drop NaNs for nunique and std checks\n",
        "            if series.nunique() > 1 and series.std(ddof=0) > 1e-9: # Check variability\n",
        "                 final_features.append(col)\n",
        "            # else: print(f\"  Feature '{col}' ekskludert (kun én verdi eller null std dev).\") # Optional debug info\n",
        "\n",
        "    final_features = sorted(list(set(final_features))) # Ensure unique and sorted\n",
        "    print(f\"  Identifisert {len(final_features)} features for bruk i modellen.\")\n",
        "    # print(f\"  Features: {final_features}\") # Optional: print feature list\n",
        "    return final_features\n",
        "\n",
        "\n",
        "# Step 3: Handle Missing / Infinite Values\n",
        "def clean_data(df, numeric_features_to_impute, essential_cols_for_dropna, target=\"TargetReturn_t\"):\n",
        "    print(\"Starter datarensing (missing/inf)...\"); initial_rows = len(df)\n",
        "    # Impute NaNs/Infs in FEATURES\n",
        "    features_present = [f for f in numeric_features_to_impute if f in df.columns]\n",
        "    if features_present:\n",
        "        # Replace Inf/-Inf with NaN first\n",
        "        inf_mask = df[features_present].isin([np.inf, -np.inf])\n",
        "        if inf_mask.any().any():\n",
        "            print(f\"  Erstatter inf med NaN i {inf_mask.any(axis=0).sum()} feature kolonner...\")\n",
        "            df[features_present] = df[features_present].replace([np.inf, -np.inf], np.nan)\n",
        "        # Impute NaNs using median\n",
        "        nan_counts_before = df[features_present].isnull().sum()\n",
        "        medians = df[features_present].median(skipna=True) # Calculate medians (robust to outliers)\n",
        "        df[features_present] = df[features_present].fillna(medians) # Fill NaNs with respective column median\n",
        "        nan_counts_after = df[features_present].isnull().sum()\n",
        "        imputed_cols = (nan_counts_before - nan_counts_after)\n",
        "        print(f\"  NaNs imputert med median i {imputed_cols[imputed_cols > 0].count()} feature kolonner.\")\n",
        "        # Handle cases where median itself is NaN (e.g., column is all NaN) - fill with 0\n",
        "        if medians.isnull().any():\n",
        "            cols_nan_median = medians[medians.isnull()].index.tolist()\n",
        "            print(f\"  ADVARSEL: Median var NaN for features: {cols_nan_median}. Fyller disse med 0.\")\n",
        "            df[cols_nan_median] = df[cols_nan_median].fillna(0)\n",
        "\n",
        "    # Drop rows with NaNs in ESSENTIAL columns (target, IDs, portfolio construction vars)\n",
        "    essential_cols_present = [col for col in essential_cols_for_dropna if col in df.columns]\n",
        "    # Automatically add target, next return, and market cap if they exist and aren't already included\n",
        "    for col in [target,'NextMonthlyReturn_t+1','MarketCap_orig']:\n",
        "        if col in df.columns and col not in essential_cols_present:\n",
        "             essential_cols_present.append(col)\n",
        "    unique_essential_cols = sorted(list(set(essential_cols_present))) # Ensure unique and sorted\n",
        "\n",
        "    if unique_essential_cols:\n",
        "        rows_before_dropna = len(df)\n",
        "        df = df.dropna(subset=unique_essential_cols)\n",
        "        rows_dropped = rows_before_dropna - len(df)\n",
        "        if rows_dropped > 0:\n",
        "            print(f\"  Fjernet {rows_dropped} rader pga. NaN i essensielle kolonner: {unique_essential_cols}\")\n",
        "    # else: print(\"  Ingen essensielle kolonner spesifisert for dropna.\") # Less verbose\n",
        "\n",
        "    # Filter based on Original Market Cap (important for portfolio construction)\n",
        "    mc_orig_col = 'MarketCap_orig'\n",
        "    if mc_orig_col in df.columns:\n",
        "        rows_before_mc_filter = len(df)\n",
        "        df = df[df[mc_orig_col] > 0] # Keep only firms with positive market cap\n",
        "        rows_dropped_mc = rows_before_mc_filter - len(df)\n",
        "        if rows_dropped_mc > 0:\n",
        "            print(f\"  Fjernet {rows_dropped_mc} rader der {mc_orig_col} <= 0.\")\n",
        "    # else: print(f\"  Advarsel: '{mc_orig_col}' ikke funnet for filtrering.\") # Less verbose\n",
        "\n",
        "    final_rows = len(df)\n",
        "    print(f\"Datarensing fullført. Form: {df.shape}. Fjernet totalt {initial_rows - final_rows} rader.\")\n",
        "    if df.empty:\n",
        "        print(\"FEIL: Ingen data igjen etter rensing.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# Step 4: Yearly Rolling Window Splits\n",
        "def get_yearly_rolling_splits(df, initial_train_years, val_years, test_years=1):\n",
        "    if \"Date\" not in df.columns: raise ValueError(\"'Date'-kolonnen mangler.\")\n",
        "    df['Year'] = df[\"Date\"].dt.year\n",
        "    unique_years = sorted(df[\"Year\"].unique())\n",
        "    n_unique_years = len(unique_years)\n",
        "    print(f\"Unike år i data: {n_unique_years} ({unique_years[0]} - {unique_years[-1]})\")\n",
        "\n",
        "    # Check if there are enough years for at least one split\n",
        "    if n_unique_years < initial_train_years + val_years + test_years:\n",
        "        df.drop(columns=['Year'], inplace=True, errors='ignore')\n",
        "        raise ValueError(f\"Ikke nok unike år ({n_unique_years}) for den spesifiserte split \"\n",
        "                         f\"(trenger minst {initial_train_years + val_years + test_years} år).\")\n",
        "\n",
        "    # Determine the first year for the test set\n",
        "    first_test_year_index = initial_train_years + val_years\n",
        "    if first_test_year_index >= n_unique_years:\n",
        "        df.drop(columns=['Year'], inplace=True, errors='ignore')\n",
        "        raise ValueError(\"Kombinasjonen av initial_train_years og val_years er for lang, \"\n",
        "                         \"etterlater ingen år for testing.\")\n",
        "\n",
        "    first_test_year = unique_years[first_test_year_index]\n",
        "    last_possible_test_start_year = unique_years[-test_years] # The last year a test window *can* start\n",
        "\n",
        "    # Calculate the number of rolling windows\n",
        "    num_windows = last_possible_test_start_year - first_test_year + 1\n",
        "    if num_windows <= 0:\n",
        "        df.drop(columns=['Year'], inplace=True, errors='ignore')\n",
        "        raise ValueError(\"Start/slutt år for testvinduer resulterer i 0 eller negativt antall vinduer.\")\n",
        "\n",
        "    print(f\"Genererer {num_windows} årlige rullerende vinduer... \"\n",
        "          f\"(Første testår: {first_test_year}, Siste startår for test: {last_possible_test_start_year})\")\n",
        "\n",
        "    splits_info = [] # To store info before yielding\n",
        "\n",
        "    for i in range(num_windows):\n",
        "        # Define years for the current window\n",
        "        current_test_start_year = first_test_year + i\n",
        "        current_test_end_year = current_test_start_year + test_years - 1\n",
        "\n",
        "        current_val_end_year = current_test_start_year - 1\n",
        "        current_val_start_year = current_val_end_year - val_years + 1\n",
        "\n",
        "        current_train_end_year = current_val_start_year - 1\n",
        "        # Training always starts from the first available year in the dataset\n",
        "        current_train_start_year = unique_years[0] # Expanding window for training\n",
        "\n",
        "        # Get indices for train, validation, and test sets\n",
        "        train_idx = df[(df['Year'] >= current_train_start_year) & (df['Year'] <= current_train_end_year)].index\n",
        "        val_idx = df[(df['Year'] >= current_val_start_year) & (df['Year'] <= current_val_end_year)].index\n",
        "        test_idx = df[(df['Year'] >= current_test_start_year) & (df['Year'] <= current_test_end_year)].index\n",
        "\n",
        "        # Get date ranges for reporting\n",
        "        train_dates = df.loc[train_idx, \"Date\"].agg(['min', 'max']) if not train_idx.empty else pd.Series([pd.NaT, pd.NaT], index=['min', 'max'])\n",
        "        val_dates = df.loc[val_idx, \"Date\"].agg(['min', 'max']) if not val_idx.empty else pd.Series([pd.NaT, pd.NaT], index=['min', 'max'])\n",
        "        test_dates = df.loc[test_idx, \"Date\"].agg(['min', 'max']) if not test_idx.empty else pd.Series([pd.NaT, pd.NaT], index=['min', 'max'])\n",
        "\n",
        "        splits_info.append((train_idx, val_idx, test_idx, train_dates, val_dates, test_dates,\n",
        "                            current_train_start_year, current_train_end_year,\n",
        "                            current_val_start_year, current_val_end_year,\n",
        "                            current_test_start_year, current_test_end_year))\n",
        "\n",
        "    # Now yield the results after printing the summary\n",
        "    for i, split_data in enumerate(splits_info):\n",
        "        train_idx, val_idx, test_idx, train_dates, val_dates, test_dates, \\\n",
        "        tr_s, tr_e, v_s, v_e, t_s, t_e = split_data\n",
        "\n",
        "        print(f\"\\n  Vindu {i+1}/{num_windows}: \"\n",
        "              f\"Train {tr_s}-{tr_e} ({len(train_idx)} obs, {train_dates['min'].strftime('%Y-%m') if pd.notna(train_dates['min']) else 'N/A'} to {train_dates['max'].strftime('%Y-%m') if pd.notna(train_dates['max']) else 'N/A'}), \"\n",
        "              f\"Val {v_s}-{v_e} ({len(val_idx)} obs, {val_dates['min'].strftime('%Y-%m') if pd.notna(val_dates['min']) else 'N/A'} to {val_dates['max'].strftime('%Y-%m') if pd.notna(val_dates['max']) else 'N/A'}), \"\n",
        "              f\"Test {t_s}-{t_e} ({len(test_idx)} obs, {test_dates['min'].strftime('%Y-%m') if pd.notna(test_dates['min']) else 'N/A'} to {test_dates['max'].strftime('%Y-%m') if pd.notna(test_dates['max']) else 'N/A'})\")\n",
        "\n",
        "        yield train_idx, val_idx, test_idx, train_dates, val_dates, test_dates\n",
        "\n",
        "    # Clean up the temporary 'Year' column\n",
        "    df.drop(columns=['Year'], inplace=True, errors='ignore')\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Step 5: Run GBRT with Huber Loss on a Single Rolling Window\n",
        "# --------------------------------------------------------------------------\n",
        "def run_gbrt_h_on_window(X_train, y_train, X_val, y_val, X_test, y_test, param_grid=None):\n",
        "    \"\"\" Trains GBRT with Huber loss, tunes hyperparameters on validation set.\n",
        "        Returns the trained model, OOS predictions, IS+Val predictions, metrics & optimal parameters. \"\"\"\n",
        "    model = None\n",
        "    optim_param_found = None\n",
        "    optimal_max_depth = np.nan\n",
        "    preds_oos = np.full(y_test.shape[0], np.nan)\n",
        "    # Predictions on the combined training and validation set used for final model fitting\n",
        "    preds_is_train_val = np.full(y_train.shape[0] + y_val.shape[0], np.nan)\n",
        "    r2_oos, mse_oos, sharpe_oos, r2_is_train_val = (np.nan,) * 4\n",
        "\n",
        "    # Default GBRT-H Hyperparameter Grid (Inspired by example, adapt as needed)\n",
        "    if param_grid is None:\n",
        "         param_grid = {\n",
        "             'n_estimators': [100],        # Fixed number of trees (like example, often tuned)\n",
        "             'learning_rate': [0.1, 0.05], # Common values\n",
        "             'max_depth': [3, 5, 7],       # Tree depth\n",
        "             'min_samples_split': [1000, 2000], # Min samples to split internal node\n",
        "             'min_samples_leaf': [500, 1000], # Min samples in leaf node\n",
        "             'max_features': ['sqrt'],     # Consider sqrt(n_features) at each split\n",
        "             'alpha': [0.9, 0.95]          # Quantile for Huber loss (sklearn default 0.9)\n",
        "             # 'subsample': [0.7]          # Optional: Fraction of samples for fitting trees (stochastic GB)\n",
        "         }\n",
        "\n",
        "    grid = list(ParameterGrid(param_grid))\n",
        "    best_mse_val = np.inf\n",
        "\n",
        "    # Basic checks for valid data\n",
        "    if X_val.shape[0] < 2 or y_val.shape[0] < 2 or np.isnan(y_val).all():\n",
        "        print(\"    ADVARSEL: Valideringssettet for lite eller inneholder bare NaNs. Hopper over tuning/trening.\")\n",
        "        return model, preds_oos, r2_oos, mse_oos, sharpe_oos, preds_is_train_val, r2_is_train_val, optimal_max_depth, optim_param_found\n",
        "\n",
        "    if X_train.shape[0] < 2 or y_train.shape[0] < 2 or np.isnan(y_train).all():\n",
        "        print(\"    ADVARSEL: Treningssettet for lite eller inneholder bare NaNs. Hopper over tuning/trening.\")\n",
        "        return model, preds_oos, r2_oos, mse_oos, sharpe_oos, preds_is_train_val, r2_is_train_val, optimal_max_depth, optim_param_found\n",
        "\n",
        "    # --- Hyperparameter Tuning Loop ---\n",
        "    # print(f\"    Tuner GBRT-H ({len(grid)} kombinasjoner)...\") # Verbose\n",
        "    for i, params in enumerate(grid):\n",
        "        try:\n",
        "             gbrt_val = GradientBoostingRegressor(loss='huber', random_state=42, **params) # Added random_state\n",
        "             gbrt_val.fit(X_train, y_train)\n",
        "             y_val_pred = gbrt_val.predict(X_val)\n",
        "\n",
        "             # Check for non-finite predictions or targets in validation set\n",
        "             valid_val_mask = np.isfinite(y_val) & np.isfinite(y_val_pred)\n",
        "             if not valid_val_mask.any(): continue # Skip if no valid comparison points\n",
        "\n",
        "             y_val_valid = y_val[valid_val_mask]\n",
        "             y_val_pred_valid = y_val_pred[valid_val_mask]\n",
        "\n",
        "             if y_val_valid.shape[0] < 1: continue # Should not happen if any() is true, but safe check\n",
        "\n",
        "             current_mse_val = mean_squared_error(y_val_valid, y_val_pred_valid)\n",
        "\n",
        "             # Update best parameters if current MSE is better and finite\n",
        "             if not np.isnan(current_mse_val) and current_mse_val < best_mse_val:\n",
        "                 best_mse_val = current_mse_val\n",
        "                 optim_param_found = params\n",
        "\n",
        "        except Exception as e:\n",
        "             # print(f\"      Tuning error for params {params}: {e}\") # Optional debug\n",
        "             continue # Continue to next parameter combination\n",
        "\n",
        "    if optim_param_found is None:\n",
        "        print(\"    FEIL: GBRT-H Tuning feilet (ingen gyldig parameterkombinasjon funnet). Hopper over trening.\")\n",
        "        return model, preds_oos, r2_oos, mse_oos, sharpe_oos, preds_is_train_val, r2_is_train_val, optimal_max_depth, optim_param_found\n",
        "\n",
        "    optimal_max_depth = optim_param_found.get('max_depth', np.nan)\n",
        "    # print(f\"    Optimal GBRT-H params funnet (basert på Val MSE): {optim_param_found}\") # Verbose\n",
        "\n",
        "    # --- Final Training on Train + Validation ---\n",
        "    preds_is_full = np.full(y_train.shape[0] + y_val.shape[0], np.nan) # Initialize with NaNs\n",
        "    try:\n",
        "        # print(\"    Trener endelig GBRT-H på Train+Val med optimale parametre...\") # Verbose\n",
        "        # Combine training and validation data\n",
        "        X_train_val = np.vstack((X_train, X_val))\n",
        "        y_train_val = np.concatenate((y_train, y_val))\n",
        "\n",
        "        # Filter out any potential NaNs in the combined target array before final training\n",
        "        valid_train_val_mask = np.isfinite(y_train_val)\n",
        "        if not valid_train_val_mask.any():\n",
        "             raise ValueError(\"Ingen gyldige target verdier i kombinert train+val sett.\")\n",
        "        X_train_val_final = X_train_val[valid_train_val_mask]\n",
        "        y_train_val_final = y_train_val[valid_train_val_mask]\n",
        "        if X_train_val_final.shape[0] < 2 :\n",
        "             raise ValueError(\"Ikke nok gyldige observasjoner i train+val for endelig trening.\")\n",
        "\n",
        "        model = GradientBoostingRegressor(loss='huber', random_state=42, **optim_param_found)\n",
        "        model.fit(X_train_val_final, y_train_val_final)\n",
        "\n",
        "        # --- OOS Evaluation (on Test set) ---\n",
        "        if X_test.shape[0] > 0 and y_test.shape[0] > 0:\n",
        "            preds_oos = model.predict(X_test)\n",
        "            nan_preds_oos_mask = ~np.isfinite(preds_oos)\n",
        "            if nan_preds_oos_mask.any():\n",
        "                 # print(f\"    ADVARSEL: Fant {nan_preds_oos_mask.sum()} ikke-endelige OOS prediksjoner. Setter til 0.\") # Less verbose\n",
        "                 preds_oos[nan_preds_oos_mask] = 0 # Impute non-finite predictions\n",
        "\n",
        "            # Calculate OOS R2 using valid targets and predictions\n",
        "            valid_oos_mask = np.isfinite(y_test) & np.isfinite(preds_oos)\n",
        "            y_test_valid = y_test[valid_oos_mask]\n",
        "            preds_oos_valid = preds_oos[valid_oos_mask]\n",
        "\n",
        "            if len(preds_oos_valid) > 1 and y_test_valid.std() > 1e-9 : # Need variance in target\n",
        "                 # Gu et al. (2020) R2 definition: 1 - SS_res / SS_true_sq\n",
        "                 ss_res_oos = np.sum((y_test_valid - preds_oos_valid)**2)\n",
        "                 ss_tot_oos_zero_mean = np.sum(y_test_valid**2) # Sum of squares of true values (centered at 0)\n",
        "                 if ss_tot_oos_zero_mean > 1e-15: # Avoid division by zero\n",
        "                     r2_oos = 1 - (ss_res_oos / ss_tot_oos_zero_mean)\n",
        "                 else: r2_oos = np.nan # Undefined if true values have no variance around 0\n",
        "\n",
        "                 mse_oos = mean_squared_error(y_test_valid, preds_oos_valid)\n",
        "\n",
        "                 # OOS Sharpe Ratio of predictions (hypothetical strategy performance)\n",
        "                 pred_std_oos = np.std(preds_oos_valid)\n",
        "                 sharpe_oos = (np.mean(preds_oos_valid) / pred_std_oos) * np.sqrt(12) if pred_std_oos > 1e-9 else np.nan\n",
        "\n",
        "        # --- IS Evaluation (on the data used for final training: Train+Val) ---\n",
        "        preds_is_train_val = model.predict(X_train_val_final) # Predict on the filtered train+val data\n",
        "        nan_preds_is_mask = ~np.isfinite(preds_is_train_val)\n",
        "        if nan_preds_is_mask.any():\n",
        "            # print(f\"    ADVARSEL: Fant {nan_preds_is_mask.sum()} ikke-endelige IS (Train+Val) prediksjoner. Setter til 0.\") # Less verbose\n",
        "            preds_is_train_val[nan_preds_is_mask] = 0 # Impute non-finite predictions\n",
        "\n",
        "        # Calculate IS R2 using valid targets and predictions from the final training set\n",
        "        # Note: y_train_val_final already filtered for NaNs\n",
        "        valid_is_mask = np.isfinite(preds_is_train_val) # Only need to check predictions now\n",
        "        y_train_val_valid = y_train_val_final[valid_is_mask]\n",
        "        preds_is_valid = preds_is_train_val[valid_is_mask]\n",
        "\n",
        "        if len(preds_is_valid) > 1 and y_train_val_valid.std() > 1e-9: # Need variance in target\n",
        "             # Gu et al. R2 definition for IS\n",
        "             ss_res_is = np.sum((y_train_val_valid - preds_is_valid)**2)\n",
        "             ss_tot_is_zero_mean = np.sum(y_train_val_valid**2) # Sum of squares of true values (centered at 0)\n",
        "             if ss_tot_is_zero_mean > 1e-15:\n",
        "                 r2_is_train_val = 1 - (ss_res_is / ss_tot_is_zero_mean)\n",
        "             else: r2_is_train_val = np.nan\n",
        "\n",
        "        # Adjust preds_is_train_val to match the original length of y_train_val, filling NaNs where original was NaN\n",
        "        # Place the valid predictions back into the full-sized array based on the valid mask used for training\n",
        "        preds_is_full[valid_train_val_mask] = preds_is_train_val\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  FEIL under endelig GBRT-H trening/prediksjon: {e}\")\n",
        "        # traceback.print_exc() # Optional detailed traceback\n",
        "        model = None; optimal_max_depth = np.nan; optim_param_found = None # Reset optimal params too\n",
        "        preds_oos.fill(np.nan); preds_is_full.fill(np.nan)\n",
        "        r2_oos, mse_oos, sharpe_oos, r2_is_train_val = (np.nan,) * 4\n",
        "\n",
        "    # Return optimal_max_depth and optimal parameters found during tuning\n",
        "    return model, preds_oos, r2_oos, mse_oos, sharpe_oos, preds_is_full, r2_is_train_val, optimal_max_depth, optim_param_found\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Step 6.7: Variable Importance Function (MODIFIED for GBRT)\n",
        "# -----------------------------------------------------------------------------\n",
        "def calculate_variable_importance_single_window(model_params, X_eval, y_eval, features, base_r2):\n",
        "    \"\"\"\n",
        "    Calculates permutation importance for GBRT-H for ONE window using Retraining.\n",
        "    Retrains the model with the same hyperparameters for each zeroed-out feature.\n",
        "    WARNING: Computationally expensive for GBRT. Uses IS R2 as base.\n",
        "    \"\"\"\n",
        "    importance_results = {}\n",
        "    n_features = X_eval.shape[1]\n",
        "\n",
        "    # Ensure y_eval is clean and calculate total sum of squares (using zero mean R2 definition)\n",
        "    valid_eval_mask = np.isfinite(y_eval)\n",
        "    if not valid_eval_mask.any():\n",
        "        print(\"  ADVARSEL (VI): Ingen gyldige target verdier i evalueringssettet. Kan ikke beregne viktighet.\")\n",
        "        return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
        "\n",
        "    y_eval_clean = y_eval[valid_eval_mask]\n",
        "    X_eval_clean = X_eval[valid_eval_mask]\n",
        "    if X_eval_clean.shape[0] < 2 or y_eval_clean.std() < 1e-9:\n",
        "        print(\"  ADVARSEL (VI): Ikke nok data eller varians i evalueringssettet. Kan ikke beregne viktighet.\")\n",
        "        return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
        "\n",
        "    ss_tot_eval = np.sum(y_eval_clean**2)\n",
        "    if ss_tot_eval < 1e-15:\n",
        "        # print(\"  ADVARSEL (VI): Total sum of squares for target er nær null. Viktighet blir 0.\") # Less verbose\n",
        "        return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
        "\n",
        "    # We need the optimal parameters found for this window's main model\n",
        "    if not model_params:\n",
        "         print(\"  ADVARSEL (VI): Modellparametre ikke gitt. Kan ikke beregne viktighet.\")\n",
        "         return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
        "\n",
        "    # Ensure loss is set correctly and random_state for reproducibility\n",
        "    current_model_params = model_params.copy()\n",
        "    current_model_params['loss'] = 'huber' # Ensure Huber loss is used\n",
        "    current_model_params['random_state'] = 42 # For reproducibility\n",
        "\n",
        "    # --- Calculate R2 for each permuted feature ---\n",
        "    r2_reductions = {}\n",
        "    for feature_idx, feature_name in enumerate(features):\n",
        "        X_eval_permuted = X_eval_clean.copy()\n",
        "\n",
        "        # Zero-out the feature column\n",
        "        X_eval_permuted[:, feature_idx] = 0\n",
        "\n",
        "        try:\n",
        "            # Re-train GBRT with the zeroed feature using same optimal params\n",
        "            # Note: Training on the potentially large X_eval_clean set each time\n",
        "            permuted_model = GradientBoostingRegressor(**current_model_params)\n",
        "            permuted_model.fit(X_eval_permuted, y_eval_clean) # Train on modified data\n",
        "\n",
        "            # Predict on the same modified data\n",
        "            permuted_preds = permuted_model.predict(X_eval_permuted)\n",
        "\n",
        "            # Check for non-finite predictions\n",
        "            valid_preds_mask = np.isfinite(permuted_preds)\n",
        "            if not valid_preds_mask.all():\n",
        "                # print(f\"    ADVARSEL (VI): Ikke-endelige prediksjoner for feature '{feature_name}'. Setter R2 red. til 0.\") # Less verbose\n",
        "                permuted_r2 = np.nan # Mark as invalid\n",
        "            else:\n",
        "                # Calculate R2 on the clean evaluation set\n",
        "                ss_res_permuted = np.sum((y_eval_clean - permuted_preds)**2)\n",
        "                permuted_r2 = 1 - (ss_res_permuted / ss_tot_eval)\n",
        "\n",
        "            # Importance is the reduction in R2 compared to the base R2\n",
        "            # Use max(0, ...) because permuting can sometimes *improve* R2 by chance\n",
        "            if pd.notna(base_r2) and pd.notna(permuted_r2):\n",
        "                 r2_reduction = base_r2 - permuted_r2\n",
        "                 r2_reductions[feature_name] = max(0, r2_reduction)\n",
        "            else:\n",
        "                 r2_reductions[feature_name] = 0.0 # Assign 0 if base or permuted R2 is NaN\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            # print(f\"    FEIL under VI re-trening for feature '{feature_name}': {e}\") # Keep less verbose\n",
        "            r2_reductions[feature_name] = 0.0 # Assign 0 importance if retraining fails\n",
        "\n",
        "    # --- Normalize Importance Scores ---\n",
        "    if not r2_reductions:\n",
        "        return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
        "\n",
        "    imp_df = pd.DataFrame(r2_reductions.items(), columns=['Feature', 'R2_reduction'])\n",
        "    total_reduction = imp_df['R2_reduction'].sum()\n",
        "\n",
        "    # Avoid division by zero or negative total reduction\n",
        "    if total_reduction > 1e-9:\n",
        "        imp_df['Importance'] = imp_df['R2_reduction'] / total_reduction\n",
        "    else:\n",
        "        imp_df['Importance'] = 0.0 # Assign zero importance if total reduction is non-positive\n",
        "\n",
        "    return imp_df[['Feature', 'Importance']]\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Step 6.8 Plot Time-Varying Complexity (MODIFIED for Max Depth)\n",
        "# --------------------------------------------------------------------------\n",
        "def plot_time_varying_complexity(model_metrics, model_name='GBRT-H'):\n",
        "     \"\"\" Plots the optimal max_depth over time for GBRT-H \"\"\"\n",
        "     print(f\"\\n--- Plotter Tidsvarierende Modellkompleksitet (Optimal Max Depth for {model_name}) ---\")\n",
        "     complexity_param = 'optim_max_depth' # Parameter to plot\n",
        "\n",
        "     if model_name in model_metrics and complexity_param in model_metrics[model_name]:\n",
        "         values = model_metrics[model_name][complexity_param]\n",
        "         if values and not all(np.isnan(d) for d in values):\n",
        "             # Create pairs of (window number, optimal depth) for valid depths\n",
        "             valid_values = [(i + 1, v) for i, v in enumerate(values) if pd.notna(v)]\n",
        "             if valid_values:\n",
        "                 windows, plot_values = zip(*valid_values)\n",
        "                 # Ensure plot_values are suitable for plotting (e.g., integers if depth)\n",
        "                 plot_values_clean = [int(v) if pd.notna(v) else np.nan for v in plot_values]\n",
        "\n",
        "                 data = pd.DataFrame({complexity_param: plot_values_clean}, index=pd.Index(windows, name='Window'))\n",
        "                 print(f\"\\n--- Optimal {complexity_param} per Vindu Tabell ({model_name}) ---\"); print(data)\n",
        "\n",
        "                 plt.figure(figsize=(10, 5))\n",
        "                 plt.plot(windows, plot_values_clean, marker='o', linestyle='-')\n",
        "                 plot_title = f\"{model_name} Optimal Max Depth per Rullerende Vindu\"\n",
        "                 y_label = f\"Optimal Max Depth\"\n",
        "                 plt.xlabel(\"Rullerende Vindu Nummer\")\n",
        "                 plt.ylabel(y_label)\n",
        "                 plt.title(plot_title)\n",
        "                 plt.grid(True, axis='y', linestyle='--') # Grid lines for y-axis\n",
        "                 # Ensure integer ticks if depth values are integers\n",
        "                 if all(isinstance(v, int) or np.isnan(v) for v in plot_values_clean):\n",
        "                    valid_int_values = [v for v in plot_values_clean if not np.isnan(v)]\n",
        "                    if valid_int_values:\n",
        "                         plt.yticks(np.arange(min(valid_int_values), max(valid_int_values)+1, step=1))\n",
        "                 plt.tight_layout()\n",
        "                 plt.show()\n",
        "             else:\n",
        "                 print(f\"  Ingen gyldige (ikke-NaN) verdier funnet for '{complexity_param}' for {model_name}.\")\n",
        "         else:\n",
        "             print(f\"  Ingen data eller bare NaN-verdier funnet for '{complexity_param}' for {model_name}.\")\n",
        "     else:\n",
        "         print(f\"  Metrikk '{complexity_param}' ikke funnet i model_metrics for {model_name}.\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Step 7: Calculate Maximum Drawdown (MDD) - ADDED\n",
        "# --------------------------------------------------------------------------\n",
        "def MDD(return_series: pd.Series) -> float:\n",
        "    \"\"\"Calculates the Maximum Drawdown (MDD) of a return series.\"\"\"\n",
        "    # Drop NaNs ensure we have data\n",
        "    clean_returns = return_series.dropna()\n",
        "    if clean_returns.empty or len(clean_returns) < 2:\n",
        "        return np.nan\n",
        "\n",
        "    # Calculate cumulative returns\n",
        "    comp_ret = (1 + clean_returns).cumprod()\n",
        "\n",
        "    # Calculate rolling max (peak)\n",
        "    peak = comp_ret.expanding(min_periods=1).max()\n",
        "\n",
        "    # Calculate drawdown\n",
        "    dd = (comp_ret / peak) - 1\n",
        "\n",
        "    # Get the minimum drawdown (maximum loss)\n",
        "    mdd = dd.min()\n",
        "\n",
        "    # Return as a percentage\n",
        "    return mdd * 100 if pd.notna(mdd) else np.nan\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Step 7: Detailed Portfolio Performance Analysis (ADDED)\n",
        "# --------------------------------------------------------------------------\n",
        "def perform_detailed_portfolio_analysis(results_df, df_full, benchmark_file=None, ff_factor_file=None, filter_small_caps=False, model_name_label='Model'):\n",
        "    \"\"\"\n",
        "    Performs decile sorts based on model predictions and analyzes portfolio performance.\n",
        "    Handles EW and VW returns, calculates metrics, and optional factor regressions/benchmarking.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starter Detaljert Porteføljeanalyse ---\")\n",
        "    if results_df is None or results_df.empty:\n",
        "        print(\"  FEIL: Ingen resultatdata for porteføljeanalyse.\")\n",
        "        return (None,) * 6\n",
        "    if df_full is None or df_full.empty:\n",
        "         print(\"  FEIL: Fullstendig DataFrame (df_full) mangler for å hente nødvendige kolonner.\")\n",
        "         return (None,) * 6\n",
        "\n",
        "    # Identify the prediction column dynamically\n",
        "    yhat_col = next((col for col in results_df.columns if col.startswith('yhat_')), None)\n",
        "    if yhat_col is None:\n",
        "        print(f\"  FEIL: Fant ingen prediksjonskolonne ('yhat_...') i results_df.\")\n",
        "        return (None,) * 6\n",
        "    print(f\"  Bruker prediksjonskolonne: {yhat_col}\")\n",
        "\n",
        "    # --- Merge Predictions with Necessary Data ---\n",
        "    essential_cols = ['Date', 'Instrument', 'NextMonthlyReturn_t+1', 'MarketCap_orig']\n",
        "    if not all(col in df_full.columns for col in essential_cols):\n",
        "        missing = [col for col in essential_cols if col not in df_full.columns]\n",
        "        print(f\"  FEIL: Nødvendige kolonner mangler i df_full: {missing}\")\n",
        "        return (None,) * 6\n",
        "\n",
        "    # Ensure correct types before merge\n",
        "    try:\n",
        "        results_df['Date'] = pd.to_datetime(results_df['Date'])\n",
        "        results_df['Instrument'] = results_df['Instrument'].astype(str) # Ensure consistent ID type\n",
        "        df_full['Date'] = pd.to_datetime(df_full['Date'])\n",
        "        df_full['Instrument'] = df_full['Instrument'].astype(str) # Ensure consistent ID type\n",
        "    except Exception as e:\n",
        "        print(f\"  FEIL under konvertering av Date/Instrument typer før merge: {e}\")\n",
        "        return (None,) * 6\n",
        "\n",
        "\n",
        "    # Merge results (predictions) with the required columns from the full dataset\n",
        "    print(\"  Merger prediksjoner med avkastning og markedsverdi...\")\n",
        "    try:\n",
        "        # Select only necessary columns to avoid memory issues and conflicts\n",
        "        df_full_subset = df_full[essential_cols].drop_duplicates(subset=['Date', 'Instrument'], keep='first')\n",
        "        results_subset = results_df[['Date', 'Instrument', yhat_col]].drop_duplicates(subset=['Date', 'Instrument'], keep='first')\n",
        "\n",
        "        merged_df = pd.merge(results_subset,\n",
        "                             df_full_subset,\n",
        "                             on=['Date', 'Instrument'],\n",
        "                             how='inner') # Use inner merge to keep only matching rows\n",
        "        print(f\"  Størrelse etter merge: {merged_df.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  FEIL under merging av prediksjoner med full data: {e}\")\n",
        "        print(f\"  Resultater subset dtypes:\\n{results_subset.dtypes}\")\n",
        "        print(f\"  Full DF subset dtypes:\\n{df_full_subset.dtypes}\")\n",
        "        # traceback.print_exc() # More detailed error\n",
        "        return (None,) * 6\n",
        "\n",
        "\n",
        "    if merged_df.empty:\n",
        "        print(\"  FEIL: Merged DataFrame er tom etter merge. Sjekk Date/Instrument matching.\")\n",
        "        return (None,) * 6\n",
        "\n",
        "    # Check for NaNs in critical columns AFTER merge\n",
        "    if merged_df['NextMonthlyReturn_t+1'].isnull().all():\n",
        "        print(\"  FEIL: 'NextMonthlyReturn_t+1' er helt tom etter merge.\")\n",
        "        return (None,) * 6\n",
        "    if merged_df[yhat_col].isnull().all():\n",
        "         print(f\"  FEIL: Prediksjonskolonne '{yhat_col}' er helt tom etter merge.\")\n",
        "         return (None,) * 6\n",
        "    if merged_df['MarketCap_orig'].isnull().all():\n",
        "         print(\"  FEIL: 'MarketCap_orig' er helt tom etter merge.\")\n",
        "         return (None,) * 6\n",
        "\n",
        "    # Drop rows where essential data for portfolio construction is missing or invalid\n",
        "    initial_rows_merge = len(merged_df)\n",
        "    merged_df = merged_df.dropna(subset=['Date', yhat_col, 'NextMonthlyReturn_t+1', 'MarketCap_orig'])\n",
        "    merged_df = merged_df[merged_df['MarketCap_orig'] > 0] # Crucial for VW portfolios\n",
        "    rows_dropped = initial_rows_merge - len(merged_df)\n",
        "    if rows_dropped > 0: print(f\"  Fjernet {rows_dropped} rader fra merged_df pga. NaN/ugyldig MC_orig.\")\n",
        "\n",
        "    if merged_df.empty:\n",
        "        print(\"  FEIL: Ingen gyldige rader igjen etter dropna/MC filter i portfolio analyse.\")\n",
        "        return (None,) * 6\n",
        "\n",
        "    print(f\"  Data klar for porteføljesortering. Antall observasjoner: {len(merged_df)}\")\n",
        "    merged_df = merged_df.sort_values(by=['Date', 'Instrument']) # Sort for consistency\n",
        "\n",
        "    # --- Optional Small Cap Filtering ---\n",
        "    # Note: `filter_small_caps` flag is passed but not currently used for extra filtering here\n",
        "    # as basic MC>0 filtering is already done. Implementation could be added if needed.\n",
        "    # if filter_small_caps:\n",
        "    #     print(\"  Bruker filter for små selskaper (filter_small_caps=True)...\")\n",
        "    #     # Example: Define a market cap threshold (e.g., based on NYSE breakpoints or a fixed percentile)\n",
        "    #     # mc_threshold = merged_df.groupby('Date')['MarketCap_orig'].quantile(0.2) # Example: Bottom 20%\n",
        "    #     # merged_df = merged_df.merge(mc_threshold.rename('mc_threshold'), left_on='Date', right_index=True)\n",
        "    #     # merged_df = merged_df[merged_df['MarketCap_orig'] >= merged_df['mc_threshold']]\n",
        "    #     # merged_df = merged_df.drop(columns=['mc_threshold'])\n",
        "    #     # print(f\"  Antall observasjoner etter small cap filter: {len(merged_df)}\")\n",
        "    #     if merged_df.empty:\n",
        "    #          print(\"  FEIL: Ingen observasjoner igjen etter small cap filter.\")\n",
        "    #          return (None,) * 6\n",
        "\n",
        "    # --- Decile Sort ---\n",
        "    print(\"  Sorterer aksjer i desiler basert på prediksjoner...\")\n",
        "    try:\n",
        "        # Ensure qcut handles edge cases (e.g., fewer than 10 unique predictions per date)\n",
        "        merged_df['DecileRank'] = merged_df.groupby('Date')[yhat_col].transform(\n",
        "            lambda x: pd.qcut(x.rank(method='first'), 10, labels=False, duplicates='drop') if x.nunique(dropna=False) >= 10 else pd.qcut(x.rank(method='first'), max(1, x.nunique(dropna=False)), labels=False, duplicates='drop')\n",
        "        )\n",
        "        merged_df['DecileRank'] = merged_df['DecileRank'] + 1 # Rank 1 (Low Prediction) to 10 (High Prediction)\n",
        "    except Exception as e:\n",
        "        print(f\"  FEIL under desilsortering: {e}. Sjekk prediksjonsverdiene ({yhat_col}).\")\n",
        "        traceback.print_exc()\n",
        "        return (None,) * 6\n",
        "\n",
        "    # --- Calculate Portfolio Returns (EW & VW) ---\n",
        "    print(\"  Beregner likevektede (EW) og verdi-vektede (VW) porteføljeavkastninger...\")\n",
        "\n",
        "    # EW Returns: Simple mean of returns within each Date/DecileRank group\n",
        "    ew_returns = merged_df.groupby(['Date', 'DecileRank'])['NextMonthlyReturn_t+1'].mean().unstack()\n",
        "\n",
        "    # VW Returns: Weighted mean using MarketCap_orig\n",
        "    def weighted_mean(group):\n",
        "        weights = group['MarketCap_orig']\n",
        "        returns = group['NextMonthlyReturn_t+1']\n",
        "        # Ensure no NaN weights or returns corrupt the weighted average\n",
        "        valid_mask = weights.notna() & returns.notna() & (weights > 0) # Add weight > 0 check\n",
        "        if valid_mask.sum() == 0 or weights[valid_mask].sum() <= 0:\n",
        "            return np.nan # Return NaN if no valid data or zero/negative total weight\n",
        "        return np.average(returns[valid_mask], weights=weights[valid_mask])\n",
        "\n",
        "    # Use apply with the custom weighted_mean function\n",
        "    # Handle potential differences in pandas apply behavior regarding group keys\n",
        "    try:\n",
        "         # Try with include_groups=False first (for pandas >= 2.1)\n",
        "         vw_returns = merged_df.groupby(['Date', 'DecileRank']).apply(weighted_mean, include_groups=False).unstack()\n",
        "    except TypeError:\n",
        "         # Fallback for older pandas versions\n",
        "         vw_returns = merged_df.groupby(['Date', 'DecileRank']).apply(weighted_mean).unstack()\n",
        "\n",
        "\n",
        "    # Rename columns for clarity (convert potential float column names from unstack to int)\n",
        "    ew_returns.columns = [f'EW_D{int(float(col))}' for col in ew_returns.columns]\n",
        "    vw_returns.columns = [f'VW_D{int(float(col))}' for col in vw_returns.columns]\n",
        "\n",
        "    # --- Calculate Long-Short Portfolio Returns ---\n",
        "    # Ensure Decile 1 and 10 columns exist before attempting subtraction\n",
        "    ew_hl_possible = 'EW_D10' in ew_returns.columns and 'EW_D1' in ew_returns.columns\n",
        "    vw_hl_possible = 'VW_D10' in vw_returns.columns and 'VW_D1' in vw_returns.columns\n",
        "\n",
        "    if ew_hl_possible:\n",
        "        ew_returns['EW_H-L'] = ew_returns['EW_D10'] - ew_returns['EW_D1']\n",
        "    else:\n",
        "        print(\"  ADVARSEL: Kunne ikke beregne EW H-L (mangler D1 og/eller D10).\")\n",
        "        ew_returns['EW_H-L'] = np.nan\n",
        "\n",
        "    if vw_hl_possible:\n",
        "        vw_returns['VW_H-L'] = vw_returns['VW_D10'] - vw_returns['VW_D1']\n",
        "    else:\n",
        "        print(\"  ADVARSEL: Kunne ikke beregne VW H-L (mangler D1 og/eller D10).\")\n",
        "        vw_returns['VW_H-L'] = np.nan\n",
        "\n",
        "    # Combine EW and VW results into a single DataFrame indexed by Date\n",
        "    portfolio_returns = pd.concat([ew_returns, vw_returns], axis=1)\n",
        "    portfolio_returns = portfolio_returns.sort_index() # Ensure chronological order\n",
        "\n",
        "    # --- Performance Metrics Calculation ---\n",
        "    print(\"  Beregner ytelsesmålinger for porteføljer...\")\n",
        "    metrics = {}\n",
        "    MIN_MONTHS_FOR_STATS = 12 # Minimum observations for annualized metrics\n",
        "\n",
        "    for col in portfolio_returns.columns:\n",
        "        ret = portfolio_returns[col].dropna() # Use only non-NaN returns for calculations\n",
        "        if len(ret) < MIN_MONTHS_FOR_STATS:\n",
        "             # Assign NaN if not enough data points\n",
        "             metrics[col] = {'Mean (%)': np.nan, 'Std Dev (%)': np.nan, 'Sharpe': np.nan, 'MDD (%)': np.nan}\n",
        "             continue # Skip to next portfolio\n",
        "\n",
        "        mean_ret_monthly = ret.mean()\n",
        "        std_dev_monthly = ret.std()\n",
        "\n",
        "        # Calculate Sharpe Ratio (annualized) - assumes Rf=0 for simplicity here, adjust if needed\n",
        "        # Handle zero standard deviation case\n",
        "        sharpe_ratio = (mean_ret_monthly / std_dev_monthly) * np.sqrt(12) if std_dev_monthly > 1e-9 else np.nan\n",
        "\n",
        "        # Calculate Maximum Drawdown using the MDD function\n",
        "        max_dd = MDD(ret) # Assumes MDD function is defined and returns percentage\n",
        "\n",
        "        # Store annualized metrics\n",
        "        metrics[col] = {\n",
        "            'Mean (%)': mean_ret_monthly * 12 * 100,\n",
        "            'Std Dev (%)': std_dev_monthly * np.sqrt(12) * 100,\n",
        "            'Sharpe': sharpe_ratio,\n",
        "            'MDD (%)': max_dd # MDD function should return percentage\n",
        "        }\n",
        "    metrics_df = pd.DataFrame(metrics).T # Transpose for standard table format\n",
        "    metrics_df = metrics_df[['Mean (%)', 'Std Dev (%)', 'Sharpe', 'MDD (%)']] # Ensure consistent column order\n",
        "\n",
        "    # Separate EW and VW decile tables for clarity\n",
        "    # Sort columns numerically within EW/VW groups, handle H-L separately\n",
        "    ew_d_cols = sorted([col for col in metrics_df.index if col.startswith('EW_D')], key=lambda x: int(x.split('D')[1]))\n",
        "    vw_d_cols = sorted([col for col in metrics_df.index if col.startswith('VW_D')], key=lambda x: int(x.split('D')[1]))\n",
        "    ew_cols_ordered = ew_d_cols + (['EW_H-L'] if 'EW_H-L' in metrics_df.index else [])\n",
        "    vw_cols_ordered = vw_d_cols + (['VW_H-L'] if 'VW_H-L' in metrics_df.index else [])\n",
        "\n",
        "    metrics_deciles_ew_df = metrics_df.loc[ew_cols_ordered]\n",
        "    metrics_deciles_vw_df = metrics_df.loc[vw_cols_ordered]\n",
        "\n",
        "    print(\"\\n  --- Ytelse - Desilporteføljer (Annualisert) ---\")\n",
        "    print(\"  Likevektet (EW):\")\n",
        "    print(metrics_deciles_ew_df.round(4))\n",
        "    print(\"\\n  Verdivektet (VW):\")\n",
        "    print(metrics_deciles_vw_df.round(4))\n",
        "\n",
        "\n",
        "    # --- Optional: Load Benchmark and Factor Data ---\n",
        "    benchmark_ret = None\n",
        "    factor_data = None\n",
        "    if benchmark_file:\n",
        "        try:\n",
        "            bench_df = pd.read_csv(benchmark_file, index_col=0, parse_dates=True)\n",
        "            # *** Adapt column name if necessary ***\n",
        "            bench_col_name = 'BenchmarkReturn' # Or the actual name in your file\n",
        "            if bench_col_name in bench_df.columns:\n",
        "                # Align benchmark returns with portfolio return dates\n",
        "                benchmark_ret = bench_df[bench_col_name].reindex(portfolio_returns.index).ffill()\n",
        "                print(f\"  Benchmark data lastet ({len(benchmark_ret.dropna())} mnd) fra {benchmark_file}\")\n",
        "            else: print(f\"  ADVARSEL: Fant ikke kolonne '{bench_col_name}' i {benchmark_file}\")\n",
        "        except Exception as e: print(f\"  ADVARSEL: Kunne ikke laste/prosessere benchmark fil {benchmark_file}: {e}\")\n",
        "\n",
        "    if ff_factor_file:\n",
        "        try:\n",
        "            fact_df = pd.read_csv(ff_factor_file, index_col=0, parse_dates=True)\n",
        "            # *** Adapt column names if necessary ***\n",
        "            required_factors = ['Mkt-RF', 'SMB', 'HML', 'Mom'] # Example: FF4 factors + Mkt\n",
        "            rf_col_name = 'RF' # Risk-free rate column\n",
        "\n",
        "            if all(f in fact_df.columns for f in required_factors) and rf_col_name in fact_df.columns:\n",
        "                 # Select and align factor data with portfolio return dates\n",
        "                 factor_data = fact_df[required_factors + [rf_col_name]].reindex(portfolio_returns.index).ffill()\n",
        "                 print(f\"  Faktordata lastet ({len(factor_data.dropna())} mnd) fra {ff_factor_file}\")\n",
        "            else:\n",
        "                 missing_f = [f for f in required_factors + [rf_col_name] if f not in fact_df.columns]\n",
        "                 print(f\"  ADVARSEL: Ikke alle nødvendige faktorer/RF funnet i {ff_factor_file}. Mangler: {missing_f}\")\n",
        "                 factor_data = None # Ensure factor_data is None if loading failed\n",
        "        except Exception as e: print(f\"  ADVARSEL: Kunne ikke laste/prosessere faktorfil {ff_factor_file}: {e}\")\n",
        "\n",
        "    # --- Risk-Adjusted Metrics (Alpha, Beta) using Statsmodels OLS ---\n",
        "    risk_metrics = {}\n",
        "    if factor_data is not None:\n",
        "        print(\"  Beregner risikojusterte målinger (Alpha, Beta) via OLS...\")\n",
        "        # Check which factor models are possible\n",
        "        has_mkt = 'Mkt-RF' in factor_data.columns\n",
        "        has_smb_hml = 'SMB' in factor_data.columns and 'HML' in factor_data.columns\n",
        "        has_mom = 'Mom' in factor_data.columns\n",
        "        has_rf = 'RF' in factor_data.columns\n",
        "\n",
        "        # Determine the most comprehensive model available\n",
        "        if has_mkt and has_smb_hml and has_mom and has_rf: model_factors = ['Mkt-RF', 'SMB', 'HML', 'Mom']; model_desc = \"FF4\"\n",
        "        elif has_mkt and has_smb_hml and has_rf: model_factors = ['Mkt-RF', 'SMB', 'HML']; model_desc = \"FF3\"\n",
        "        elif has_mkt and has_rf: model_factors = ['Mkt-RF']; model_desc = \"CAPM\"\n",
        "        else: model_factors = None; model_desc = \"Ingen\"; print(\"  ADVARSEL: Ikke nok faktorer for regresjonsanalyse.\")\n",
        "\n",
        "        if model_factors:\n",
        "            for col in portfolio_returns.columns:\n",
        "                 port_ret = portfolio_returns[col].dropna()\n",
        "                 if len(port_ret) < MIN_MONTHS_FOR_STATS + len(model_factors): # Need enough obs for regression\n",
        "                    risk_metrics[col] = {'Alpha (%)': np.nan, 'Mkt Beta': np.nan, 'SMB Beta': np.nan, 'HML Beta': np.nan, 'Mom Beta': np.nan, 'Info Ratio': np.nan, 'R2 (%)': np.nan}\n",
        "                    continue\n",
        "\n",
        "                 # Align portfolio returns with factor data and drop NaNs for the regression period\n",
        "                 aligned_data = pd.concat([port_ret.rename('PortfolioRet'), factor_data], axis=1).dropna()\n",
        "                 if len(aligned_data) < MIN_MONTHS_FOR_STATS + len(model_factors): # Check again after alignment & dropna\n",
        "                     risk_metrics[col] = {'Alpha (%)': np.nan, 'Mkt Beta': np.nan, 'SMB Beta': np.nan, 'HML Beta': np.nan, 'Mom Beta': np.nan, 'Info Ratio': np.nan, 'R2 (%)': np.nan}\n",
        "                     continue\n",
        "\n",
        "                 # Define dependent variable (Portfolio Excess Return)\n",
        "                 Y = aligned_data['PortfolioRet'] - aligned_data['RF']\n",
        "                 # Define independent variables (Factors + Constant for Alpha)\n",
        "                 X = aligned_data[model_factors]\n",
        "                 X = sm.add_constant(X) # Add intercept term\n",
        "\n",
        "                 try:\n",
        "                     model = sm.OLS(Y, X).fit()\n",
        "                     alpha = model.params.get('const', np.nan)\n",
        "                     beta_mkt = model.params.get('Mkt-RF', np.nan)\n",
        "                     beta_smb = model.params.get('SMB', np.nan) # Will be NaN if SMB not in model_factors\n",
        "                     beta_hml = model.params.get('HML', np.nan) # Will be NaN if HML not in model_factors\n",
        "                     beta_mom = model.params.get('Mom', np.nan) # Will be NaN if Mom not in model_factors\n",
        "                     r2 = model.rsquared_adj # Use adjusted R-squared\n",
        "\n",
        "                     # Calculate Information Ratio (Annualized Alpha / Annualized Std Dev of Residuals)\n",
        "                     tracking_error = model.resid.std()\n",
        "                     # Avoid division by zero for IR\n",
        "                     info_ratio = (alpha * 12 / (tracking_error * np.sqrt(12))) if tracking_error > 1e-9 else np.nan # Corrected Annualized IR\n",
        "\n",
        "                     # Store results\n",
        "                     risk_metrics[col] = {\n",
        "                        'Alpha (%)': alpha * 12 * 100, # Annualized alpha %\n",
        "                        'Mkt Beta': beta_mkt,\n",
        "                        'SMB Beta': beta_smb,\n",
        "                        'HML Beta': beta_hml,\n",
        "                        'Mom Beta': beta_mom,\n",
        "                        'Info Ratio': info_ratio,\n",
        "                        'R2 (%)': r2 * 100\n",
        "                     }\n",
        "                 except Exception as e:\n",
        "                     # print(f\"      FEIL under {model_desc} OLS for {col}: {e}\") # Less verbose\n",
        "                     risk_metrics[col] = {'Alpha (%)': np.nan, 'Mkt Beta': np.nan, 'SMB Beta': np.nan, 'HML Beta': np.nan, 'Mom Beta': np.nan, 'Info Ratio': np.nan, 'R2 (%)': np.nan}\n",
        "                     continue # Skip to next portfolio\n",
        "\n",
        "        risk_metrics_df = pd.DataFrame(risk_metrics).T\n",
        "\n",
        "        # Select only H-L and Long (Decile 10) portfolios for the risk summary tables\n",
        "        hl_cols = sorted([col for col in risk_metrics_df.index if 'H-L' in col])\n",
        "        long_cols = sorted([col for col in risk_metrics_df.index if '_D10' in col])\n",
        "\n",
        "        # Ensure columns exist before trying to locate them and handle potential empty lists\n",
        "        risk_hl_ew_df = risk_metrics_df.loc[[c for c in hl_cols if c.startswith('EW') and c in risk_metrics_df.index]] if any(c.startswith('EW') for c in hl_cols) else pd.DataFrame(columns=risk_metrics_df.columns)\n",
        "        risk_hl_vw_df = risk_metrics_df.loc[[c for c in hl_cols if c.startswith('VW') and c in risk_metrics_df.index]] if any(c.startswith('VW') for c in hl_cols) else pd.DataFrame(columns=risk_metrics_df.columns)\n",
        "        risk_long_ew_df = risk_metrics_df.loc[[c for c in long_cols if c.startswith('EW') and c in risk_metrics_df.index]] if any(c.startswith('EW') for c in long_cols) else pd.DataFrame(columns=risk_metrics_df.columns)\n",
        "        risk_long_vw_df = risk_metrics_df.loc[[c for c in long_cols if c.startswith('VW') and c in risk_metrics_df.index]] if any(c.startswith('VW') for c in long_cols) else pd.DataFrame(columns=risk_metrics_df.columns)\n",
        "\n",
        "        print(f\"\\n  --- Risikoanalyse ({model_desc} Alpha, Beta, etc.) ---\")\n",
        "        if not risk_hl_ew_df.empty or not risk_hl_vw_df.empty:\n",
        "            print(\"  High Minus Low:\")\n",
        "            print(pd.concat([risk_hl_ew_df, risk_hl_vw_df]).round(4))\n",
        "        else: print(\"  High Minus Low: Ingen data å vise.\")\n",
        "        if not risk_long_ew_df.empty or not risk_long_vw_df.empty:\n",
        "            print(\"\\n  Long Only (Decile 10):\")\n",
        "            print(pd.concat([risk_long_ew_df, risk_long_vw_df]).round(4))\n",
        "        else: print(\"\\n  Long Only (Decile 10): Ingen data å vise.\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"  Skipping risikojusterte målinger (Alpha, Beta) - Faktordata mangler eller var utilstrekkelige.\")\n",
        "        # Create empty DFs with correct columns if no factor analysis was done\n",
        "        risk_cols = ['Alpha (%)', 'Mkt Beta', 'SMB Beta', 'HML Beta', 'Mom Beta', 'Info Ratio', 'R2 (%)']\n",
        "        risk_hl_ew_df = pd.DataFrame(columns=risk_cols)\n",
        "        risk_hl_vw_df = pd.DataFrame(columns=risk_cols)\n",
        "        risk_long_ew_df = pd.DataFrame(columns=risk_cols)\n",
        "        risk_long_vw_df = pd.DataFrame(columns=risk_cols)\n",
        "\n",
        "\n",
        "    print(\"--- Detaljert Porteføljeanalyse Fullført ---\")\n",
        "    # Return the 6 specific tables expected by the main script's saving logic\n",
        "    return metrics_deciles_ew_df, metrics_deciles_vw_df, \\\n",
        "           risk_hl_ew_df, risk_hl_vw_df, \\\n",
        "           risk_long_ew_df, risk_long_vw_df\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Step 7.5: Analyze Prespecified Portfolios (ADDED - Placeholder)\n",
        "# --------------------------------------------------------------------------\n",
        "def analyze_prespecified_portfolios(results_df, df_full, portfolio_definitions_file=None, model_name_label='Model'):\n",
        "    \"\"\"\n",
        "    Analyzes the performance of prespecified portfolios based on model predictions.\n",
        "    (Placeholder implementation - returns empty DataFrames).\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starter Analyse av Prespesifiserte Porteføljer ---\")\n",
        "    yhat_col = next((col for col in results_df.columns if col.startswith('yhat_')), None)\n",
        "\n",
        "    if portfolio_definitions_file is None:\n",
        "        print(\"  Ingen porteføljedefinisjonsfil spesifisert. Hopper over.\")\n",
        "        return pd.DataFrame(columns=['Portfolio', f'{model_name_label}_R2']), pd.DataFrame(columns=['Portfolio', f'{model_name_label}_Timing']) # Return two empty DFs with columns\n",
        "    if not os.path.exists(portfolio_definitions_file):\n",
        "        print(f\"  FEIL: Porteføljedefinisjonsfil '{portfolio_definitions_file}' ikke funnet. Hopper over.\")\n",
        "        return pd.DataFrame(columns=['Portfolio', f'{model_name_label}_R2']), pd.DataFrame(columns=['Portfolio', f'{model_name_label}_Timing'])\n",
        "    if results_df is None or results_df.empty or yhat_col is None:\n",
        "        print(\"  FEIL: Mangler resultatdata eller prediksjoner for analyse av prespesifiserte porteføljer.\")\n",
        "        return pd.DataFrame(columns=['Portfolio', f'{model_name_label}_R2']), pd.DataFrame(columns=['Portfolio', f'{model_name_label}_Timing'])\n",
        "    if df_full is None or df_full.empty:\n",
        "        print(\"  FEIL: Fullstendig DataFrame mangler for analyse av prespesifiserte porteføljer.\")\n",
        "        return pd.DataFrame(columns=['Portfolio', f'{model_name_label}_R2']), pd.DataFrame(columns=['Portfolio', f'{model_name_label}_Timing'])\n",
        "\n",
        "\n",
        "    print(f\"  Bruker prediksjonskolonne: {yhat_col}\")\n",
        "    print(f\"  Laster porteføljedefinisjoner fra: {portfolio_definitions_file}\")\n",
        "    print(\"  ADVARSEL: Funksjonalitet for prespesifiserte porteføljer er ikke implementert (placeholder).\")\n",
        "\n",
        "    # --- Placeholder Logic ---\n",
        "    # In a full implementation:\n",
        "    # 1. Load definitions from portfolio_definitions_file (e.g., a CSV mapping characteristics to portfolio names).\n",
        "    # 2. Merge results_df with df_full to get characteristics and predictions.\n",
        "    # 3. For each date, assign stocks to the prespecified portfolios based on their characteristics.\n",
        "    # 4. Calculate portfolio returns (EW/VW) based on model predictions (yhat) for these portfolios? (Unusual - typically uses actual next returns).\n",
        "    # 5. OR, calculate R2 comparing yhat to actual returns *within* these predefined portfolios.\n",
        "    # 6. Calculate timing ability metrics (e.g., Treynor-Mazuy or Merton-Henriksson if using yhat to time market exposure).\n",
        "\n",
        "    # Return empty dataframes as per placeholder nature\n",
        "    prespec_r2_table = pd.DataFrame({'Portfolio': ['Placeholder'], f'{model_name_label}_R2': [np.nan]})\n",
        "    prespec_timing_table = pd.DataFrame({'Portfolio': ['Placeholder'], f'{model_name_label}_Timing': [np.nan]})\n",
        "\n",
        "    print(\"--- Analyse av Prespesifiserte Porteføljer Fullført (Placeholder) ---\")\n",
        "    return prespec_r2_table, prespec_timing_table\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Step 8: Main Orchestration Function (MODIFIED for GBRT-H)\n",
        "# --------------------------------------------------------------------------\n",
        "# --- Define Yearly Split Parameters Here ---\n",
        "INITIAL_TRAIN_YEARS_DEFAULT = 9\n",
        "VALIDATION_YEARS_DEFAULT = 6\n",
        "TEST_YEARS_PER_WINDOW_DEFAULT = 1\n",
        "\n",
        "def run_analysis_for_subset(file_path,\n",
        "                            data_subset='all',\n",
        "                            benchmark_file=None,\n",
        "                            ff_factor_file=None,\n",
        "                            portfolio_defs_file=None,\n",
        "                            filter_portfolio_construction=False, # Pass flag if needed\n",
        "                            top_n=1000,\n",
        "                            bottom_n=1000,\n",
        "                            initial_train_years=INITIAL_TRAIN_YEARS_DEFAULT,\n",
        "                            val_years=VALIDATION_YEARS_DEFAULT,\n",
        "                            test_years=TEST_YEARS_PER_WINDOW_DEFAULT\n",
        "                           ):\n",
        "    \"\"\" Runs the full pipeline for one data subset using GBRT-H with YEARLY refitting. \"\"\"\n",
        "    run_label = data_subset.capitalize()\n",
        "    model_name = 'GBRT-H' # *** Define model name ***\n",
        "    start_time = datetime.datetime.now()\n",
        "    print(f\"\\n{'='*20} Starter Kjøring: {model_name} for '{run_label}' Firms (ÅRLIG Refitting) {'='*20}\")\n",
        "    if data_subset == 'big': print(f\"(Definert som Topp {top_n} basert på MarketCap per måned)\")\n",
        "    if data_subset == 'small': print(f\"(Definert som Bunn {bottom_n} basert på MarketCap per måned)\")\n",
        "    print(f\"Starttid: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # --- Load & Subset ---\n",
        "    print(\"\\n--- Steg 1: Laster og Forbereder Rådata ---\")\n",
        "    df_raw = load_prepare_data(file_path)\n",
        "    if df_raw is None: return np.nan, None, None, (None,)*6, (None, None)\n",
        "    if 'MarketCap_orig' not in df_raw.columns: print(\"FEIL: MarketCap_orig mangler etter lasting.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
        "    if 'Date' not in df_raw.columns: print(\"FEIL: Date mangler etter lasting.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
        "\n",
        "    print(f\"\\n--- Steg 1.1: Lager Subset: {run_label} ---\")\n",
        "    df = pd.DataFrame()\n",
        "    # Ensure MarketCap_orig and Date are not NaN before subsetting\n",
        "    df_raw_mc = df_raw.dropna(subset=['MarketCap_orig', 'Date'])\n",
        "    if df_raw_mc.empty: print(\"FEIL: Ingen rader med gyldig MarketCap_orig og Date i rådata.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
        "\n",
        "    if data_subset == 'all':\n",
        "        df = df_raw.copy() # Use the original loaded data\n",
        "    elif data_subset == 'big':\n",
        "        df_raw_mc['MonthYear'] = df_raw_mc['Date'].dt.to_period('M')\n",
        "        # Use group_keys=False to avoid adding MonthYear to the index\n",
        "        df = df_raw_mc.groupby('MonthYear', group_keys=False).apply(lambda x: x.nlargest(top_n, \"MarketCap_orig\"))\n",
        "        df = df.drop(columns=['MonthYear'], errors='ignore')\n",
        "    elif data_subset == 'small':\n",
        "        df_raw_mc['MonthYear'] = df_raw_mc['Date'].dt.to_period('M')\n",
        "        df = df_raw_mc.groupby('MonthYear', group_keys=False).apply(lambda x: x.nsmallest(bottom_n, \"MarketCap_orig\"))\n",
        "        df = df.drop(columns=['MonthYear'], errors='ignore')\n",
        "    else:\n",
        "        print(f\"FEIL: Ukjent subset '{data_subset}'.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
        "\n",
        "    if df.empty: print(f\"FEIL: Subset '{run_label}' er tomt.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
        "    print(f\"Subset '{run_label}' initiell form: {df.shape}\")\n",
        "\n",
        "    # --- Define Features, Standardize & Clean ---\n",
        "    print(\"\\n--- Steg 2 & 1.5: Definerer Features og Rank Standardiserer ---\")\n",
        "    gbrt_features = define_features(df) # Use same feature definition logic\n",
        "    if not gbrt_features: print(f\"FEIL: Ingen features funnet i subset '{run_label}'.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
        "    df = rank_standardize_features(df, gbrt_features)\n",
        "\n",
        "    print(\"\\n--- Steg 3: Renser Data (Missing/Inf/Filters) ---\")\n",
        "    # Essential cols needed for modeling and portfolio evaluation\n",
        "    essential_cols = ['TargetReturn_t', 'NextMonthlyReturn_t+1', 'MarketCap_orig', 'Date', 'Instrument']\n",
        "    df = clean_data(df, gbrt_features, essential_cols, target=\"TargetReturn_t\")\n",
        "    if df is None or df.empty: print(f\"FEIL: DataFrame er tom etter rensing for subset '{run_label}'.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
        "\n",
        "    # Refresh feature list after cleaning (some might become constant)\n",
        "    gbrt_features = [f for f in gbrt_features if f in df.columns and df[f].nunique(dropna=False) > 1 and df[f].std(ddof=0, skipna=True) > 1e-9]\n",
        "    if not gbrt_features: print(\"FEIL: Ingen features igjen etter datarensing.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
        "    print(f\"  Antall features etter rensing: {len(gbrt_features)}\")\n",
        "\n",
        "    # Sort data before splitting\n",
        "    df = df.sort_values([\"Date\", \"Instrument\"]).reset_index(drop=True)\n",
        "    df_cleaned_for_portfolio = df.copy() # Keep a copy of the cleaned data for portfolio analysis merge\n",
        "\n",
        "    # --- Yearly Rolling Window Setup ---\n",
        "    print(f\"\\n--- Steg 4: Setter opp ÅRLIG Rullerende Vindu (InitTrain={initial_train_years}, Val={val_years}, Test={test_years}) ---\")\n",
        "    results_list = []; model_metrics = defaultdict(lambda: defaultdict(list)); variable_importance_scores_all_windows = []\n",
        "    yhat_col_name = f'yhat_{model_name.lower().replace(\"-\", \"_\").replace(\"+\",\"h\")}' # e.g., yhat_gbrt_h\n",
        "    try:\n",
        "        splits_generator = get_yearly_rolling_splits(df, initial_train_years, val_years, test_years)\n",
        "        # Convert generator to list to know the number of windows in advance (optional)\n",
        "        splits = list(splits_generator)\n",
        "        num_windows = len(splits)\n",
        "    except ValueError as e:\n",
        "        print(f\"FEIL ved generering av årlige rullerende vinduer: {e}\\nAvslutter kjøring for {run_label}.\")\n",
        "        if 'Year' in df.columns: df.drop(columns=['Year'], inplace=True, errors='ignore')\n",
        "        return np.nan, None, model_metrics, (None,)*6, (None, None)\n",
        "\n",
        "    print(f\"\\nAntall årlige rullerende vinduer som skal kjøres: {num_windows}\");\n",
        "    if num_windows == 0: print(\"Ingen årlige vinduer å kjøre. Avslutter.\"); return np.nan, None, model_metrics, (None,)*6, (None, None)\n",
        "\n",
        "    # --- Rolling Window Loop ---\n",
        "    print(f\"\\n--- Starter {model_name} ÅRLIG Rullerende Vindu Trening & Prediksjon ---\")\n",
        "    total_vi_time = 0\n",
        "    for window, (train_idx, val_idx, test_idx, train_dates, val_dates, test_dates) in enumerate(splits):\n",
        "        window_start_time = datetime.datetime.now(); window_num = window + 1; print(\"-\" * 60)\n",
        "        print(f\"Behandler Vindu {window_num}/{num_windows}...\")\n",
        "\n",
        "        # Basic checks for empty indices\n",
        "        preds_oos = None # Initialize preds_oos for this window\n",
        "        r2_oos=np.nan\n",
        "        optim_depth=np.nan\n",
        "        r2_is=np.nan\n",
        "        trained_model = None\n",
        "        optim_params = None\n",
        "\n",
        "        if val_idx.empty: print(f\"Vindu {window_num}: Tomt valideringssett (idx). Hopper over tuning/trening.\"); continue # Cannot tune without validation\n",
        "        if train_idx.empty: print(f\"Vindu {window_num}: Tomt treningssett (idx). Hopper over tuning/trening.\"); continue # Cannot train\n",
        "        if test_idx.empty: print(f\"Vindu {window_num}: Tomt testsett (idx). Vil hoppe over OOS prediksjon/evaluering.\")\n",
        "\n",
        "        # Prepare data for the current window\n",
        "        X_train = df.loc[train_idx, gbrt_features].values; y_train = df.loc[train_idx, \"TargetReturn_t\"].values\n",
        "        X_val = df.loc[val_idx, gbrt_features].values;     y_val = df.loc[val_idx, \"TargetReturn_t\"].values\n",
        "        X_test = df.loc[test_idx, gbrt_features].values if not test_idx.empty else np.array([])\n",
        "        y_test = df.loc[test_idx, \"TargetReturn_t\"].values if not test_idx.empty else np.array([])\n",
        "        # Combined Train+Val data for final model fit and VI base R2 calculation\n",
        "        X_train_val = np.vstack((X_train, X_val));         y_train_val = np.concatenate((y_train, y_val))\n",
        "\n",
        "        # Check for sufficient data and variance before calling model function\n",
        "        if np.isnan(y_train).all() or np.isnan(y_val).all() or X_train.shape[0]<2 or X_val.shape[0]<2 or np.nanstd(y_train) < 1e-9 or np.nanstd(y_val) < 1e-9:\n",
        "            print(f\"Vindu {window_num}: Utilstrekkelig data/varians i train/val. Hopper over.\");\n",
        "            model_metrics[model_name]['oos_r2'].append(np.nan); model_metrics[model_name]['optim_max_depth'].append(np.nan); model_metrics[model_name]['is_r2_train_val'].append(np.nan)\n",
        "            continue\n",
        "\n",
        "        # --- Run GBRT-H for the current window ---\n",
        "        # *** Define GBRT parameter grid (can be customized per run) ***\n",
        "        gbrt_param_grid = {\n",
        "             'n_estimators': [100],\n",
        "             'learning_rate': [0.1, 0.05],\n",
        "             'max_depth': [3, 5, 7],\n",
        "             'min_samples_split': [1000, 2000],\n",
        "             'min_samples_leaf': [500, 1000],\n",
        "             'max_features': ['sqrt'],\n",
        "             'alpha': [0.9, 0.95] # For Huber loss\n",
        "         }\n",
        "\n",
        "        # *** Call the GBRT function ***\n",
        "        trained_model, preds_oos, r2_oos, mse_oos, sharpe_oos, preds_is, r2_is, optim_depth, optim_params = run_gbrt_h_on_window(\n",
        "            X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "            param_grid=gbrt_param_grid\n",
        "        )\n",
        "\n",
        "        # --- Store Results ---\n",
        "        # Store OOS predictions if they exist and test set was not empty\n",
        "        if preds_oos is not None and not test_idx.empty and len(preds_oos) == len(test_idx):\n",
        "             window_predictions = {\n",
        "                 'Date': df.loc[test_idx, 'Date'].values,\n",
        "                 'Instrument': df.loc[test_idx, 'Instrument'].values,\n",
        "                 'TargetReturn_t': y_test, # Actual target return for the test period\n",
        "                 yhat_col_name: preds_oos # Model's prediction\n",
        "             }\n",
        "             results_list.append(pd.DataFrame(window_predictions))\n",
        "        elif not test_idx.empty:\n",
        "             print(f\"  ADVARSEL Vindu {window_num}: Test index fantes, men ingen OOS prediksjoner generert.\")\n",
        "\n",
        "\n",
        "        # Store window metrics (even if they are NaN)\n",
        "        model_metrics[model_name]['oos_r2'].append(r2_oos)\n",
        "        model_metrics[model_name]['optim_max_depth'].append(optim_depth) # Store optimal max_depth\n",
        "        model_metrics[model_name]['is_r2_train_val'].append(r2_is)       # Store IS R2 (on Train+Val)\n",
        "\n",
        "        # --- Variable Importance (using GBRT Retraining Method) ---\n",
        "        # Check if model trained successfully and we have optimal params and a base IS R2\n",
        "        if trained_model is not None and optim_params is not None and pd.notna(r2_is):\n",
        "            print(f\"  Beregner variabelviktighet for Vindu {window_num} (GBRT Retraining - kan ta tid)...\")\n",
        "            vi_start_time = time.time()\n",
        "            # Calculate importance based on the reduction in IS R2 when retraining with a zeroed feature\n",
        "            # Use the combined X_train_val and y_train_val as the evaluation set\n",
        "            window_vi_df = calculate_variable_importance_single_window(\n",
        "                optim_params, X_train_val, y_train_val, gbrt_features, r2_is # Use IS R2 as base\n",
        "            )\n",
        "            if window_vi_df is not None and not window_vi_df.empty:\n",
        "                 variable_importance_scores_all_windows.append(window_vi_df)\n",
        "                 vi_duration = time.time() - vi_start_time\n",
        "                 total_vi_time += vi_duration\n",
        "                 print(f\"    Variabelviktighet beregnet på {vi_duration:.2f} sekunder.\")\n",
        "            else:\n",
        "                 print(f\"    ADVARSEL: Kunne ikke beregne VI for vindu {window_num}.\")\n",
        "        elif trained_model is None:\n",
        "            print(\"    ADVARSEL: Hovedmodell feilet for dette vinduet, hopper over VI.\")\n",
        "        elif optim_params is None:\n",
        "            print(\"    ADVARSEL: Optimal parametre ikke funnet for dette vinduet, hopper over VI.\")\n",
        "        elif pd.isna(r2_is):\n",
        "             print(\"    ADVARSEL: Base IS R2 er NaN for dette vinduet, hopper over VI.\")\n",
        "\n",
        "\n",
        "        # --- CORRECTED Print Statement ---\n",
        "        print(f\"  Vindu {window_num} fullført på {(datetime.datetime.now() - window_start_time).total_seconds():.1f}s. \"\n",
        "              f\"Optimal Depth: {optim_depth}, Window OOS R2: {f'{r2_oos:.4f}' if pd.notna(r2_oos) else 'N/A'}\")\n",
        "\n",
        "\n",
        "    # --- Aggregate Results & Overall Analysis ---\n",
        "    if not results_list:\n",
        "        print(f\"\\nFEIL: Ingen OOS resultater ble samlet inn for {run_label}. Kan ikke utføre porteføljeanalyse.\");\n",
        "        # Return NaN R2, None VI, existing metrics, and empty portfolio/prespec results\n",
        "        return np.nan, None, model_metrics, (None,)*6, (None, None)\n",
        "\n",
        "    results_df = pd.concat(results_list).reset_index(drop=True)\n",
        "    print(f\"\\n--- Samlet Resultatanalyse ({run_label}, {model_name}, Årlig Refit) ---\")\n",
        "    print(f\"Totalt antall OOS prediksjoner samlet: {len(results_df)}\")\n",
        "\n",
        "    # Calculate Overall OOS R2 (Gu et al. 2020 definition) across all windows\n",
        "    y_true_all = results_df['TargetReturn_t']\n",
        "    y_pred_all = results_df[yhat_col_name]\n",
        "    valid_idx_all = y_true_all.notna() & y_pred_all.notna() & np.isfinite(y_true_all) & np.isfinite(y_pred_all)\n",
        "\n",
        "    R2OOS_overall = np.nan\n",
        "    if valid_idx_all.sum() > 1:\n",
        "        y_t_valid_all = y_true_all[valid_idx_all]\n",
        "        y_p_valid_all = y_pred_all[valid_idx_all]\n",
        "        ss_res_all = np.sum((y_t_valid_all - y_p_valid_all)**2)\n",
        "        ss_true_sq_all = np.sum(y_t_valid_all**2) # Sum of squared true returns (mean assumed zero for R2 calc)\n",
        "        if ss_true_sq_all > 1e-15: # Avoid division by zero\n",
        "            R2OOS_overall = 1 - (ss_res_all / ss_true_sq_all)\n",
        "\n",
        "    # Average of the individual window OOS R2 values\n",
        "    avg_yearly_window_r2 = np.nanmean(model_metrics[model_name]['oos_r2']) # Use nanmean to ignore NaN windows\n",
        "\n",
        "    print(f\"Overall OOS R² ({run_label}, Gu et al. Def): {R2OOS_overall:.6f}\")\n",
        "    print(f\"Average Yearly Window OOS R² ({run_label}):  {avg_yearly_window_r2:.6f}\")\n",
        "    model_metrics[model_name]['oos_r2_overall_gu'] = R2OOS_overall # Store the overall R2\n",
        "\n",
        "    # --- Detailed Portfolio Analysis ---\n",
        "    # Use the aggregated results_df and the cleaned df (before splitting)\n",
        "    portfolio_tables = perform_detailed_portfolio_analysis(\n",
        "        results_df,\n",
        "        df_cleaned_for_portfolio, # Use the cleaned data from before the loops\n",
        "        benchmark_file=benchmark_file,\n",
        "        ff_factor_file=ff_factor_file,\n",
        "        filter_small_caps=filter_portfolio_construction, # Pass filter flag\n",
        "        model_name_label=model_name\n",
        "    )\n",
        "\n",
        "    # --- Analyze Prespecified Portfolios ---\n",
        "    # Use the aggregated results_df and the cleaned df\n",
        "    prespec_r2_table, prespec_timing_table = analyze_prespecified_portfolios(\n",
        "        results_df,\n",
        "        df_cleaned_for_portfolio,\n",
        "        portfolio_definitions_file=portfolio_defs_file,\n",
        "        model_name_label=model_name\n",
        "    )\n",
        "\n",
        "    # --- Aggregate and Plot Variable Importance ---\n",
        "    averaged_vi_df = None\n",
        "    if variable_importance_scores_all_windows:\n",
        "         print(f\"\\n--- Aggregerer Variabel Viktighet over {len(variable_importance_scores_all_windows)} ÅRLIGE vinduer --- (Total VI tid: {total_vi_time:.1f}s)\")\n",
        "         all_vi_df = pd.concat(variable_importance_scores_all_windows)\n",
        "         # Average the importance scores across all windows for each feature\n",
        "         averaged_vi_df = all_vi_df.groupby('Feature')['Importance'].mean().reset_index()\n",
        "         # Re-normalize the averaged importances so they sum to 1\n",
        "         total_avg_importance = averaged_vi_df['Importance'].sum()\n",
        "         if total_avg_importance > 1e-9:\n",
        "              averaged_vi_df['Importance'] = averaged_vi_df['Importance'] / total_avg_importance\n",
        "         else: averaged_vi_df['Importance'] = 0.0 # Handle case where all importances were zero\n",
        "\n",
        "         averaged_vi_df = averaged_vi_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "         print(f\"\\n--- Gjennomsnittlig Variabel Viktighet ({model_name}, Top 10) ---\"); print(averaged_vi_df[['Feature', 'Importance']].head(10).round(4))\n",
        "\n",
        "         # Plot top N features\n",
        "         plt.figure(figsize=(12, 8)); top_n_plot = 20\n",
        "         # Filter out features with zero or negligible importance for plotting\n",
        "         plot_df = averaged_vi_df[averaged_vi_df['Importance'] > 1e-6].head(top_n_plot).sort_values(by='Importance', ascending=True)\n",
        "         if not plot_df.empty:\n",
        "              plt.barh(plot_df['Feature'], plot_df['Importance'])\n",
        "              plt.xlabel(\"Gjennomsnittlig Relativ Viktighet (Permutasjon - Retraining)\")\n",
        "              plt.ylabel(\"Feature\")\n",
        "              plt.title(f'{model_name} Gjennomsnittlig Variabel Viktighet (Top {top_n_plot}) over Alle Årlige Vinduer')\n",
        "              plt.tight_layout(); plt.show()\n",
        "         else: print(\"  Ingen features med vesentlig viktighet (> 1e-6) å plotte.\")\n",
        "    else: print(\"\\nIngen variabel viktighetsdata ble samlet inn (muligens pga. feil i modelltrening eller VI).\")\n",
        "\n",
        "    # --- Plot Time-Varying Complexity (Max Depth) ---\n",
        "    # *** Call the MODIFIED plot function for GBRT ***\n",
        "    plot_time_varying_complexity(model_metrics, model_name)\n",
        "\n",
        "    # --- Final Summary ---\n",
        "    end_time = datetime.datetime.now()\n",
        "    print(f\"\\n--- Kjøring ({run_label}, {model_name}, Årlig Refit) fullført ---\")\n",
        "    print(f\"Sluttid: {end_time.strftime('%Y-%m-%d %H:%M:%S')} (Total tid: {(end_time - start_time)})\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Return overall R2, averaged VI, metrics dict, portfolio tables tuple, prespecified tables tuple\n",
        "    return R2OOS_overall, averaged_vi_df, model_metrics, portfolio_tables, (prespec_r2_table, prespec_timing_table)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Main Execution Block (MODIFIED for GBRT-H)\n",
        "# --------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # --- CONFIGURATION ---\n",
        "    data_file = \"Cleaned_OSEFX_Market_Macro_Data.csv\" # <--- SET YOUR INPUT DATA FILE\n",
        "\n",
        "    # --- Yearly Split Parameters ---\n",
        "    INITIAL_TRAIN_YEARS = 9\n",
        "    VALIDATION_YEARS = 6\n",
        "    TEST_YEARS_PER_WINDOW = 1\n",
        "\n",
        "    # --- Optional External Data Files ---\n",
        "    # Set to None if files are not available or not needed\n",
        "    benchmark_csv_file = None #\"path/to/your/benchmark.csv\"\n",
        "    # Provide the path to your Fama-French factor file if you have one\n",
        "    ff_factor_csv_file = \"Europe_4_Factors_Monthly.csv\" # <--- SET YOUR FF FACTOR FILE or None\n",
        "    portfolio_defs_csv_file = None #\"path/to/your/portfolio_defs.csv\"\n",
        "\n",
        "    # --- Analysis Settings ---\n",
        "    # filter_small_caps_portfolio = False # Set to True to apply extra filtering (if implemented)\n",
        "    TOP_N_FIRMS = 1000 # For 'big' subset\n",
        "    BOTTOM_N_FIRMS = 1000 # For 'small' subset\n",
        "\n",
        "    # --- Output Directory ---\n",
        "    output_dir = \"GBRT_H_Results_YearlyRefit\" # *** CHANGED Directory Name ***\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir); print(f\"Opprettet mappe: {output_dir}\")\n",
        "\n",
        "    # --- Check File Existence ---\n",
        "    if not os.path.exists(data_file): print(f\"FEIL: Input datafil ikke funnet på '{data_file}'. Avslutter.\"); exit()\n",
        "    if benchmark_csv_file and not os.path.exists(benchmark_csv_file): print(f\"ADVARSEL: Benchmark fil '{benchmark_csv_file}' ikke funnet. Benchmark-sammenligning deaktivert.\"); benchmark_csv_file = None\n",
        "    if ff_factor_csv_file and not os.path.exists(ff_factor_csv_file): print(f\"ADVARSEL: FF faktorfil '{ff_factor_csv_file}' ikke funnet. Faktorregresjoner deaktivert.\"); ff_factor_csv_file = None\n",
        "    if portfolio_defs_csv_file and not os.path.exists(portfolio_defs_csv_file): print(f\"ADVARSEL: Porteføljedefinisjonsfil '{portfolio_defs_csv_file}' ikke funnet. Analyse av prespesifiserte porteføljer deaktivert.\"); portfolio_defs_csv_file = None\n",
        "\n",
        "    # --- RUN ANALYSIS FOR EACH SUBSET ---\n",
        "    results_r2_summary = {}; overall_variable_importance_df = None; all_subset_metrics = {}; all_portfolio_results_tables = {}; all_prespecified_results = {}\n",
        "    subsets_to_run = ['all', 'big', 'small']\n",
        "    model_run_name = 'GBRT-H' # *** Define model name for labels ***\n",
        "\n",
        "    for subset in subsets_to_run:\n",
        "        r2, vi_df_avg_run, metrics_run, port_tables_run, prespec_tables_run = run_analysis_for_subset(\n",
        "            file_path=data_file, data_subset=subset,\n",
        "            benchmark_file=benchmark_csv_file,\n",
        "            ff_factor_file=ff_factor_csv_file,\n",
        "            portfolio_defs_file=portfolio_defs_csv_file,\n",
        "            filter_portfolio_construction=False, # Set specific value\n",
        "            top_n=TOP_N_FIRMS, bottom_n=BOTTOM_N_FIRMS,\n",
        "            initial_train_years=INITIAL_TRAIN_YEARS, val_years=VALIDATION_YEARS, test_years=TEST_YEARS_PER_WINDOW\n",
        "        )\n",
        "        # Store results for this subset\n",
        "        results_r2_summary[subset] = r2\n",
        "        all_subset_metrics[subset] = metrics_run\n",
        "        all_portfolio_results_tables[subset] = port_tables_run\n",
        "        all_prespecified_results[subset] = prespec_tables_run\n",
        "        # Store the VI from the 'all' run specifically\n",
        "        if subset == 'all' and vi_df_avg_run is not None:\n",
        "             overall_variable_importance_df = vi_df_avg_run\n",
        "\n",
        "    # --- Print Final R2 Summary Table ---\n",
        "    print(\"\\n\\n\" + \"=\"*30 + f\" Final OOS R2 Summary ({model_run_name} - Yearly Refit) \" + \"=\"*30) # Use variable\n",
        "    summary_r2_data = {\n",
        "        \"Full Sample (all)\": results_r2_summary.get('all', np.nan),\n",
        "        f\"Large Firms (Top {TOP_N_FIRMS})\": results_r2_summary.get('big', np.nan),\n",
        "        f\"Small Firms (Bottom {BOTTOM_N_FIRMS})\": results_r2_summary.get('small', np.nan)\n",
        "    }\n",
        "    # Create DataFrame from dict, ensuring keys are used as index\n",
        "    r2_summary_df = pd.DataFrame.from_dict(summary_r2_data, orient='index', columns=[f'{model_run_name} R2oos (%)'])\n",
        "    r2_summary_df[f'{model_run_name} R2oos (%)'] *= 100 # Convert to percentage points\n",
        "    print(r2_summary_df.round(4))\n",
        "    # Save summary table\n",
        "    try:\n",
        "        r2_summary_filename = os.path.join(output_dir, f\"{model_run_name.lower().replace('+','h')}_R2oos_summary_subsets_yearly.csv\")\n",
        "        r2_summary_df.to_csv(r2_summary_filename)\n",
        "        print(f\" -> R2 Sammendrag lagret til {r2_summary_filename}\")\n",
        "    except Exception as e: print(f\"  FEIL ved lagring av R2 Sammendrag: {e}\")\n",
        "    print(\"=\"* (62 + len(f\" Final OOS R2 Summary ({model_run_name} - Yearly Refit) \")))\n",
        "\n",
        "    # --- Save Averaged VI results (from 'all' subset run) ---\n",
        "    if overall_variable_importance_df is not None:\n",
        "         print(\"\\nLagrer Gjennomsnittlig Variabel Viktighet resultater (fra 'all' subset)...\")\n",
        "         try:\n",
        "             vi_filename = os.path.join(output_dir, f\"{model_run_name.lower().replace('+','h')}_variable_importance_averaged_yearly.csv\")\n",
        "             overall_variable_importance_df.to_csv(vi_filename, index=False)\n",
        "             print(f\" -> Gjennomsnittlig Variabel Viktighet lagret til {vi_filename}\")\n",
        "         except Exception as e: print(f\"  FEIL ved lagring av VI: {e}\")\n",
        "    else:\n",
        "        print(\"\\nIngen gjennomsnittlig variabel viktighetsdata (fra 'all' subset) å lagre.\")\n",
        "\n",
        "\n",
        "    # --- Save Portfolio Decile Tables and Risk Tables for each subset ---\n",
        "    print(\"\\nLagrer Portefølje Desil- og Risikotabeller for hver subset...\")\n",
        "    for subset, tables in all_portfolio_results_tables.items():\n",
        "         if tables and len(tables) == 6: # Check if we got the 6 expected tables\n",
        "              names = ['decile_ew', 'decile_vw', 'hl_risk_ew', 'hl_risk_vw', 'long_risk_ew', 'long_risk_vw']\n",
        "              for i, table_df in enumerate(tables):\n",
        "                   if table_df is not None and not table_df.empty:\n",
        "                        filename = os.path.join(output_dir, f\"{model_run_name.lower().replace('+','h')}_portfolio_{subset}_{names[i]}_yearly.csv\")\n",
        "                        try:\n",
        "                            # Save with index (portfolio names) and format floats\n",
        "                            table_df.to_csv(filename, float_format='%.4f')\n",
        "                            print(f\" -> Porteføljetabell ({subset}, {names[i]}) lagret til {filename}\")\n",
        "                        except Exception as e: print(f\"  FEIL ved lagring av porteføljetabell {filename}: {e}\")\n",
        "                   # else: print(f\"  Hoppet over lagring for {subset} - {names[i]} (tom eller None).\") # Less verbose\n",
        "         # else: print(f\"  Ingen porteføljetabeller funnet for subset '{subset}' å lagre.\") # Less verbose\n",
        "\n",
        "    # --- Save Prespecified Portfolio Results (Optional - Placeholder data) ---\n",
        "    print(\"\\nLagrer Prespesifiserte Portefølje Resultater (Placeholder)...\")\n",
        "    for subset, tables in all_prespecified_results.items():\n",
        "         if tables and len(tables) == 2:\n",
        "             names = ['prespec_r2', 'prespec_timing']\n",
        "             for i, table_df in enumerate(tables):\n",
        "                  if table_df is not None and not table_df.empty:\n",
        "                       filename = os.path.join(output_dir, f\"{model_run_name.lower().replace('+','h')}_portfolio_{subset}_{names[i]}_yearly.csv\")\n",
        "                       try:\n",
        "                           table_df.to_csv(filename, index=False, float_format='%.4f') # No meaningful index here\n",
        "                           print(f\" -> Prespesifisert porteføljetabell ({subset}, {names[i]}) lagret til {filename}\")\n",
        "                       except Exception as e: print(f\"  FEIL ved lagring av prespesifisert tabell {filename}: {e}\")\n",
        "\n",
        "    print(f\"\\nFull {model_run_name} analyse (Årlig Refitting) fullført.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}