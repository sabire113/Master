{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f425b98",
   "metadata": {},
   "source": [
    "# GBRT + H "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fadb8e35",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3237511281.py, line 587)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 587\u001b[0;36m\u001b[0m\n\u001b[0;31m    except ValueError as e: print(f\"FEIL ved generering av årlige vinduer: {e}\"); if 'Year' in df.columns: df.drop(columns=['Year'], inplace=True, errors='ignore'); return np.nan, None, model_metrics, (None,)*6, (None, None)\u001b[0m\n\u001b[0m                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "# *** Import GBRT ***\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# *** Import for Factor Regressions (commented out until needed) ***\n",
    "# import statsmodels.api as sm\n",
    "import datetime\n",
    "import warnings\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import time # For timing VI\n",
    "\n",
    "# --- WARNINGS CONFIGURATION ---\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"pandas\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Mean of empty slice\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"invalid value encountered in log\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Maximum number of iterations reached.*\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# FUNCTION DEFINITIONS\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# Step 1: Load and Prepare Dataset\n",
    "def load_prepare_data(file_path):\n",
    "    \"\"\" Loads, cleans, calculates returns/market cap, handles dates/IDs. \"\"\"\n",
    "    print(f\"Laster data fra: {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "    except FileNotFoundError: print(f\"FEIL: Fil '{file_path}' ikke funnet.\"); return None\n",
    "    print(f\"Data lastet inn. Form: {df.shape}\")\n",
    "\n",
    "    date_col = 'Date' if 'Date' in df.columns else 'eom'\n",
    "    id_col = 'Instrument' if 'Instrument' in df.columns else 'id'\n",
    "    price_col = 'ClosePrice' if 'ClosePrice' in df.columns else 'prc'\n",
    "    shares_col = 'CommonSharesOutstanding'\n",
    "    rf_col = 'NorgesBank10Y'\n",
    "    sector_col = 'EconomicSector'\n",
    "\n",
    "    if date_col not in df.columns: print(\"FEIL: Dato-kolonne mangler.\"); return None\n",
    "    if id_col not in df.columns: print(\"FEIL: Instrument-ID mangler.\"); return None\n",
    "    if price_col not in df.columns: print(f\"FEIL: Pris-kolonne ('{price_col}') mangler.\"); return None\n",
    "\n",
    "    df = df.rename(columns={date_col: 'Date', id_col: 'Instrument'})\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values(by=[\"Instrument\", \"Date\"]).reset_index(drop=True)\n",
    "    print(\"Dato konvertert og data sortert.\")\n",
    "\n",
    "    df[\"MonthlyReturn\"] = df.groupby(\"Instrument\")[price_col].pct_change()\n",
    "    df[\"MonthlyReturn\"].fillna(0, inplace=True)\n",
    "    df[\"MonthlyReturn\"] = winsorize(df[\"MonthlyReturn\"].values, limits=[0.01, 0.01])\n",
    "    print(\"Månedlig avkastning ('MonthlyReturn') beregnet/winsorisert.\")\n",
    "\n",
    "    if rf_col not in df.columns: df[rf_col] = 0; print(f\"ADVARSEL: '{rf_col}' mangler, bruker 0.\")\n",
    "    df[\"MonthlyRiskFreeRate_t\"] = df[rf_col] / 12 / 100 if df[rf_col].abs().max() > 1 else df[rf_col] / 12\n",
    "    df[\"TargetReturn_t\"] = df[\"MonthlyReturn\"] - df[\"MonthlyRiskFreeRate_t\"]\n",
    "    print(\"Risikojustert avkastning ('TargetReturn_t') beregnet (modellens y).\")\n",
    "\n",
    "    df['NextMonthlyReturn_t+1'] = df.groupby('Instrument')['MonthlyReturn'].shift(-1)\n",
    "    print(\"Neste måneds rå avkastning ('NextMonthlyReturn_t+1') beregnet.\")\n",
    "\n",
    "    if shares_col not in df.columns: print(f\"FEIL: '{shares_col}' mangler for MarketCap.\"); return None\n",
    "    df[\"MarketCap\"] = df[price_col] * df[shares_col]\n",
    "    df['MarketCap_orig'] = df['MarketCap'].copy()\n",
    "    df['MarketCap'] = df['MarketCap'].fillna(0)\n",
    "    print(\"Markedsverdi ('MarketCap') beregnet.\")\n",
    "\n",
    "    if sector_col in df.columns:\n",
    "        df = pd.get_dummies(df, columns=[sector_col], prefix=\"Sector\", dtype=int)\n",
    "        print(\"Sektor dummy-variabler opprettet.\")\n",
    "\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\").str.replace(\"-\", \"_\")\n",
    "    print(\"Kolonnenavn renset.\")\n",
    "\n",
    "    print(\"Log-transformerer spesifikke variabler...\")\n",
    "    vars_to_log = [\"MarketCap\", \"BM\", \"ClosePrice\", \"Volume\", \"CommonSharesOutstanding\"]\n",
    "    vars_to_log = [v if v in df.columns else v.lower() for v in vars_to_log]\n",
    "    vars_to_log = [v if v in df.columns else 'prc' if v == 'closeprice' else v for v in vars_to_log]\n",
    "    vars_to_log = [col for col in vars_to_log if col in df.columns]\n",
    "    for var in vars_to_log:\n",
    "        if var in df.columns and pd.api.types.is_numeric_dtype(df[var]):\n",
    "             df[f\"{var}_positive\"] = df[var].where(df[var] > 1e-9, np.nan)\n",
    "             df[f\"log_{var}\"] = np.log(df[f\"{var}_positive\"])\n",
    "             log_median = df[f\"log_{var}\"].median()\n",
    "             df[f\"log_{var}\"] = df[f\"log_{var}\"].fillna(log_median)\n",
    "             if pd.isna(log_median): df[f\"log_{var}\"] = df[f\"log_{var}\"].fillna(0)\n",
    "             df.drop(columns=[f\"{var}_positive\"], inplace=True)\n",
    "    print(\"Log-transformasjon fullført.\")\n",
    "\n",
    "    if 'MarketCap_orig' not in df.columns and 'MarketCap' in df.columns:\n",
    "         df['MarketCap_orig'] = df['MarketCap'].copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Step 1.5: Rank Standardization\n",
    "def rank_standardize_features(df, features_to_standardize):\n",
    "    print(f\"Rank standardiserer {len(features_to_standardize)} features...\")\n",
    "    if 'Date' not in df.columns: print(\"FEIL: 'Date' mangler.\"); return df\n",
    "    features_present = [f for f in features_to_standardize if f in df.columns]\n",
    "    if len(features_present) < len(features_to_standardize):\n",
    "        missing = [f for f in features_to_standardize if f not in features_present]; print(f\"  ADVARSEL: Features manglet for standardisering: {missing}\")\n",
    "    if not features_present: print(\"  Ingen features å standardisere.\"); return df\n",
    "    def rank_transform(x): x_numeric = pd.to_numeric(x, errors='coerce'); ranks = x_numeric.rank(pct=True); return ranks * 2 - 1\n",
    "    try: ranked_cols = df.groupby('Date')[features_present].transform(rank_transform); df[features_present] = ranked_cols\n",
    "    except Exception as e:\n",
    "        print(f\"  ADVARSEL rank (transform): {e}. Prøver apply.\")\n",
    "        try: df_std = df.set_index('Date'); [df_std.groupby(level=0)[col].apply(rank_transform) for col in features_present]; df = df_std.reset_index()\n",
    "        except Exception as e2: print(f\"  FEIL: Også alternativ standardisering feilet: {e2}\"); return df\n",
    "    print(\"Rank standardisering fullført.\")\n",
    "    return df\n",
    "\n",
    "# Step 2: Define Feature Sets\n",
    "def define_features(df):\n",
    "    print(\"Identifiserer numeriske features...\")\n",
    "    if df is None or df.empty: print(\"  FEIL: DataFrame er tom.\"); return []\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    print(f\"  Funnet {len(numeric_cols)} numeriske kolonner totalt.\")\n",
    "    cols_to_exclude = ['Instrument', 'Date', 'level_0', 'index', 'Year', 'MonthYear', 'TargetReturn_t', 'NextMonthlyReturn_t+1', 'MonthlyReturn', 'MonthlyRiskFreeRate_t', 'MarketCap_orig', 'rank', 'DecileRank', 'eq_weights', 'me_weights']\n",
    "    cols_to_exclude.extend([col for col in df.columns if 'return_stock' in col or 'return_portfolio' in col])\n",
    "    log_cols = [col for col in numeric_cols if col.startswith('log_')]; originals_of_log = [col.replace('log_','') for col in log_cols]; common_originals = ['MarketCap', 'ClosePrice', 'prc', 'Volume', 'CommonSharesOutstanding', 'BM']; originals_to_exclude = [orig for orig in originals_of_log if orig in df.columns and orig in common_originals]; cols_to_exclude.extend(originals_to_exclude); cols_to_exclude = list(set(cols_to_exclude))\n",
    "    potential_features = [col for col in numeric_cols if col not in cols_to_exclude]\n",
    "    final_features = [col for col in potential_features if col in df.columns and df[col].nunique(dropna=True) > 1 and df[col].std(ddof=0, skipna=True) > 1e-9]\n",
    "    final_features = sorted(list(set(final_features))); print(f\"  Identifisert {len(final_features)} features for bruk i modellen.\")\n",
    "    return final_features\n",
    "\n",
    "# Step 3: Handle Missing / Infinite Values\n",
    "def clean_data(df, numeric_features_to_impute, essential_cols_for_dropna, target=\"TargetReturn_t\"):\n",
    "    print(\"Starter datarensing (missing/inf)...\"); initial_rows = len(df)\n",
    "    features_present = [f for f in numeric_features_to_impute if f in df.columns]\n",
    "    if features_present:\n",
    "        inf_mask = df[features_present].isin([np.inf, -np.inf]);\n",
    "        if inf_mask.any().any(): print(f\"  Erstatter inf med NaN i {inf_mask.any(axis=0).sum()} feature kolonner...\"); df[features_present] = df[features_present].replace([np.inf, -np.inf], np.nan)\n",
    "        nan_counts_before = df[features_present].isnull().sum(); medians = df[features_present].median(skipna=True); df[features_present] = df[features_present].fillna(medians); nan_counts_after = df[features_present].isnull().sum()\n",
    "        imputed_cols = (nan_counts_before - nan_counts_after); print(f\"  NaNs imputert med median i {imputed_cols[imputed_cols > 0].count()} feature kolonner.\")\n",
    "        if medians.isnull().any(): cols_nan_median = medians[medians.isnull()].index.tolist(); print(f\"  ADVARSEL: Median NaN for: {cols_nan_median}. Fyller med 0.\"); df[cols_nan_median] = df[cols_nan_median].fillna(0)\n",
    "    essential_cols_present = [col for col in essential_cols_for_dropna if col in df.columns]; [essential_cols_present.append(col) for col in [target,'NextMonthlyReturn_t+1','MarketCap_orig'] if col in df.columns and col not in essential_cols_present]\n",
    "    unique_essential_cols = sorted(list(set(essential_cols_present)))\n",
    "    if unique_essential_cols: rows_before_dropna = len(df); df = df.dropna(subset=unique_essential_cols); rows_dropped = rows_before_dropna - len(df);\n",
    "    if rows_dropped > 0: print(f\"  Fjernet {rows_dropped} rader pga. NaN i essensielle kolonner: {unique_essential_cols}\")\n",
    "    mc_orig_col = 'MarketCap_orig'\n",
    "    if mc_orig_col in df.columns: rows_before_mc_filter = len(df); df = df[df[mc_orig_col] > 0]; rows_dropped_mc = rows_before_mc_filter - len(df);\n",
    "    if rows_dropped_mc > 0: print(f\"  Fjernet {rows_dropped_mc} rader der {mc_orig_col} <= 0.\")\n",
    "    final_rows = len(df); print(f\"Datarensing fullført. Form: {df.shape}. Fjernet totalt {initial_rows - final_rows} rader.\");\n",
    "    if df.empty: print(\"FEIL: Ingen data igjen etter rensing.\")\n",
    "    return df\n",
    "\n",
    "# Step 4: Yearly Rolling Window Splits\n",
    "def get_yearly_rolling_splits(df, initial_train_years, val_years, test_years=1):\n",
    "    if \"Date\" not in df.columns: raise ValueError(\"'Date'-kolonnen mangler.\")\n",
    "    df['Year'] = df[\"Date\"].dt.year; unique_years = sorted(df[\"Year\"].unique()); n_unique_years = len(unique_years); print(f\"Unike år i data: {n_unique_years} ({unique_years[0]} - {unique_years[-1]})\")\n",
    "    if n_unique_years < initial_train_years + val_years + test_years: df.drop(columns=['Year'], inplace=True, errors='ignore'); raise ValueError(f\"Ikke nok unike år ({n_unique_years}) for split.\")\n",
    "    first_test_year_index = initial_train_years + val_years;\n",
    "    if first_test_year_index >= n_unique_years: df.drop(columns=['Year'], inplace=True, errors='ignore'); raise ValueError(\"Train+Val år for lange.\")\n",
    "    first_test_year = unique_years[first_test_year_index]; last_test_year = unique_years[-test_years]; num_windows = last_test_year - first_test_year + 1\n",
    "    if num_windows <= 0: df.drop(columns=['Year'], inplace=True, errors='ignore'); raise ValueError(\"Negativt antall vinduer.\")\n",
    "    print(f\"Genererer {num_windows} årlige rullerende vinduer... (Første testår: {first_test_year}, Siste: {last_test_year})\")\n",
    "    for i in range(num_windows):\n",
    "        current_test_start_year = first_test_year + i; current_test_end_year = current_test_start_year + test_years - 1\n",
    "        current_val_end_year = current_test_start_year - 1; current_val_start_year = current_val_end_year - val_years + 1\n",
    "        current_train_end_year = current_val_start_year - 1; current_train_start_year = unique_years[0]\n",
    "        train_idx = df[(df['Year'] >= current_train_start_year) & (df['Year'] <= current_train_end_year)].index\n",
    "        val_idx = df[(df['Year'] >= current_val_start_year) & (df['Year'] <= current_val_end_year)].index\n",
    "        test_idx = df[(df['Year'] >= current_test_start_year) & (df['Year'] <= current_test_end_year)].index\n",
    "        train_dates = df.loc[train_idx, \"Date\"].agg(['min', 'max']) if not train_idx.empty else None\n",
    "        val_dates = df.loc[val_idx, \"Date\"].agg(['min', 'max']) if not val_idx.empty else None\n",
    "        test_dates = df.loc[test_idx, \"Date\"].agg(['min', 'max']) if not test_idx.empty else None\n",
    "        print(f\"\\n  Vindu {i+1}/{num_windows}: Train {current_train_start_year}-{current_train_end_year} ({len(train_idx)}), Val {current_val_start_year}-{current_val_end_year} ({len(val_idx)}), Test {current_test_start_year}-{current_test_end_year} ({len(test_idx)})\")\n",
    "        yield train_idx, val_idx, test_idx, train_dates, val_dates, test_dates\n",
    "    df.drop(columns=['Year'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Step 5: Run GBRT with Huber Loss on a Single Rolling Window (MODIFIED)\n",
    "# --------------------------------------------------------------------------\n",
    "def run_gbrt_h_on_window(X_train, y_train, X_val, y_val, X_test, y_test, param_grid=None):\n",
    "    \"\"\"\n",
    "    Trains GBRT with Huber loss (alpha=0.999 fixed), tunes other hyperparameters via validation MSE.\n",
    "    Returns model, predictions, metrics, optimal max_depth, and optimal parameter dictionary.\n",
    "    \"\"\"\n",
    "    model = None\n",
    "    optim_param_found = None\n",
    "    optimal_max_depth = np.nan\n",
    "    preds_oos = np.full(y_test.shape[0], np.nan)\n",
    "    preds_is_train_val = np.full(y_train.shape[0] + y_val.shape[0], np.nan)\n",
    "    r2_oos, mse_oos, sharpe_oos, r2_is_train_val = (np.nan,) * 4\n",
    "\n",
    "    # Default GBRT-H Hyperparameter Grid (Inspired by article/example, fixed alpha)\n",
    "    if param_grid is None:\n",
    "         param_grid = {\n",
    "             'n_estimators': [100], # Fixed number of trees (as in German example) - Can be tuned with early stopping if needed\n",
    "             'learning_rate': [0.1, 0.01], # As per Table A.5\n",
    "             'max_depth': [1, 2], # As per Table A.5\n",
    "             # Adding reasonable regularization based on German example\n",
    "             'min_samples_split': [1000, 5000], # Adjust based on dataset size\n",
    "             'min_samples_leaf': [500, 1000],   # Adjust based on dataset size\n",
    "             'max_features': ['sqrt'] # Common choice\n",
    "             # 'alpha' is NOT tuned here, fixed at 0.999 later based on Table A.5\n",
    "         }\n",
    "\n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "    best_mse_val = np.inf\n",
    "\n",
    "    if X_val.shape[0] < 2:\n",
    "        print(\"    ADVARSEL: Valideringssettet for lite (<2 obs). Hopper over tuning/trening.\")\n",
    "        return model, preds_oos, r2_oos, mse_oos, sharpe_oos, preds_is_train_val, r2_is_train_val, optimal_max_depth, optim_param_found\n",
    "\n",
    "    # --- Hyperparameter Tuning Loop (Minimizing Validation MSE) ---\n",
    "    # print(f\"    Tuner GBRT-H ({len(grid)} kombinasjoner via Val MSE)...\") # Verbose\n",
    "    for i, params in enumerate(grid):\n",
    "        try:\n",
    "             # *** Use loss='huber' and fix alpha=0.999 ***\n",
    "             gbrt_val = GradientBoostingRegressor(loss='huber', alpha=0.999, random_state=42, **params)\n",
    "             gbrt_val.fit(X_train, y_train)\n",
    "             y_val_pred = gbrt_val.predict(X_val)\n",
    "\n",
    "             if not np.all(np.isfinite(y_val_pred)): continue # Skip if prediction failed\n",
    "\n",
    "             # *** Use MSE for validation objective ***\n",
    "             current_mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "             if not np.isnan(current_mse_val) and current_mse_val < best_mse_val:\n",
    "                 best_mse_val = current_mse_val\n",
    "                 optim_param_found = params\n",
    "\n",
    "        except Exception as e:\n",
    "             # print(f\"      Tuning error for params {params}: {e}\") # Optional debug\n",
    "             continue # Continue tuning even if one combination fails\n",
    "\n",
    "    if optim_param_found is None:\n",
    "        print(\"    FEIL: GBRT-H Tuning feilet (ingen gyldig parameter funnet basert på Val MSE). Hopper over trening.\")\n",
    "        return model, preds_oos, r2_oos, mse_oos, sharpe_oos, preds_is_train_val, r2_is_train_val, optimal_max_depth, optim_param_found\n",
    "\n",
    "    optimal_max_depth = optim_param_found.get('max_depth', np.nan)\n",
    "    # print(f\"    Optimal GBRT-H params funnet (basert på Val MSE): {optim_param_found}\") # Verbose\n",
    "\n",
    "    # --- Final Training (using Train + Validation set) ---\n",
    "    try:\n",
    "        # print(\"    Trener endelig GBRT-H på Train+Val...\") # Verbose\n",
    "        X_train_val = np.vstack((X_train, X_val)); y_train_val = np.concatenate((y_train, y_val))\n",
    "\n",
    "        # *** Instantiate final model with fixed alpha and optimal params ***\n",
    "        model = GradientBoostingRegressor(loss='huber', alpha=0.999, random_state=42, **optim_param_found)\n",
    "        model.fit(X_train_val, y_train_val)\n",
    "\n",
    "        # --- OOS Evaluation ---\n",
    "        if X_test.shape[0] > 0:\n",
    "            preds_oos = model.predict(X_test); nan_preds_oos_mask = ~np.isfinite(preds_oos)\n",
    "            if nan_preds_oos_mask.any(): preds_oos[nan_preds_oos_mask] = 0 # Replace non-finite\n",
    "            valid_oos_mask = np.isfinite(y_test) & np.isfinite(preds_oos); y_test_valid = y_test[valid_oos_mask]; preds_oos_valid = preds_oos[valid_oos_mask]\n",
    "            if len(preds_oos_valid) > 1:\n",
    "                ss_res_oos = np.sum((y_test_valid - preds_oos_valid)**2); ss_tot_oos = np.sum(y_test_valid**2)\n",
    "                r2_oos = 1 - (ss_res_oos / ss_tot_oos) if ss_tot_oos > 1e-9 else np.nan\n",
    "                mse_oos = mean_squared_error(y_test_valid, preds_oos_valid)\n",
    "                pred_std_oos = np.std(preds_oos_valid); sharpe_oos = (np.mean(preds_oos_valid)/pred_std_oos)*np.sqrt(12) if pred_std_oos > 1e-9 else np.nan\n",
    "\n",
    "        # --- IS Evaluation (on Train+Val set for VI baseline) ---\n",
    "        preds_is_train_val = model.predict(X_train_val); nan_preds_is_mask = ~np.isfinite(preds_is_train_val)\n",
    "        if nan_preds_is_mask.any(): preds_is_train_val[nan_preds_is_mask] = 0\n",
    "        valid_is_mask = np.isfinite(y_train_val) & np.isfinite(preds_is_train_val); y_train_val_valid = y_train_val[valid_is_mask]; preds_is_valid = preds_is_train_val[valid_is_mask]\n",
    "        if len(preds_is_valid) > 1:\n",
    "            ss_res_is = np.sum((y_train_val_valid - preds_is_valid)**2); ss_tot_is = np.sum(y_train_val_valid**2)\n",
    "            r2_is_train_val = 1 - (ss_res_is / ss_tot_is) if ss_tot_is > 1e-9 else np.nan\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  FEIL under endelig GBRT-H trening/prediksjon: {e}\")\n",
    "        model = None; optimal_max_depth = np.nan; preds_oos.fill(np.nan); preds_is_train_val.fill(np.nan)\n",
    "        r2_oos, mse_oos, sharpe_oos, r2_is_train_val = (np.nan,) * 4\n",
    "        optim_param_found = None # Reset params if training failed\n",
    "\n",
    "    # Return optimal_max_depth and the full parameter dictionary\n",
    "    return model, preds_oos, r2_oos, mse_oos, sharpe_oos, preds_is_train_val, r2_is_train_val, optimal_max_depth, optim_param_found\n",
    "\n",
    "\n",
    "# Step 6.5: Detailed Portfolio Analysis Function\n",
    "def MDD(returns):\n",
    "    \"\"\" Calculates Maximum Drawdown using arithmetic returns for NAV. \"\"\"\n",
    "    returns = pd.Series(returns).fillna(0)\n",
    "    if returns.empty: return np.nan\n",
    "    nav = (1 + returns).cumprod(); hwm = nav.cummax(); dd = nav / hwm - 1\n",
    "    return dd.min() if not dd.empty else np.nan\n",
    "\n",
    "def perform_detailed_portfolio_analysis(results_df, original_df_subset, benchmark_file=None, ff_factor_file=None, filter_small_caps=False, model_name_label=\"GBRT-H\"): # Default Label Changed\n",
    "    \"\"\" Performs detailed portfolio analysis, generates tables and plots for the given model label. \"\"\"\n",
    "    print(\"\\n--- Starter Detaljert Porteføljeanalyse (Prediction-Sorted Deciles) ---\")\n",
    "    pred_col = f'yhat_{model_name_label.lower().replace(\"-\",\"_\").replace(\"+\",\"h\")}' # e.g., yhat_gbrt_h\n",
    "    ew_table, vw_table, ew_chart_hl, vw_chart_hl, ew_chart_long, vw_chart_long = (pd.DataFrame(),)*6\n",
    "\n",
    "    if pred_col not in results_df.columns: print(f\"FEIL: Kolonne '{pred_col}' mangler.\"); return (pd.DataFrame(),)*6\n",
    "    required_cols = ['Date', 'Instrument', 'MarketCap_orig', 'NextMonthlyReturn_t+1', 'MonthlyReturn', 'MonthlyRiskFreeRate_t']\n",
    "    if not all(c in original_df_subset.columns for c in required_cols): print(\"FEIL: Mangler kolonner i original subset.\"); return (pd.DataFrame(),)*6\n",
    "\n",
    "    portfolio_data = pd.merge(results_df[['Date', 'Instrument', 'TargetReturn_t', pred_col]], original_df_subset[required_cols], on=['Date', 'Instrument'], how='inner')\n",
    "    portfolio_data = portfolio_data.rename(columns={'TargetReturn_t': 'y_true_t', pred_col: 'yhat_t+1', 'MarketCap_orig': 'me', 'NextMonthlyReturn_t+1': 'ret_t+1'})\n",
    "    portfolio_data['MonthYear'] = portfolio_data['Date'].dt.to_period('M')\n",
    "    monthly_rf_map = portfolio_data.groupby('MonthYear')['MonthlyRiskFreeRate_t'].mean().shift(-1)\n",
    "    portfolio_data['NextMonthRiskFreeRate_t+1'] = portfolio_data['MonthYear'].map(monthly_rf_map)\n",
    "    cols_for_eval_dropna = ['yhat_t+1', 'ret_t+1', 'me', 'NextMonthRiskFreeRate_t+1']\n",
    "    portfolio_data = portfolio_data.dropna(subset=cols_for_eval_dropna)\n",
    "    if portfolio_data.empty: print(\"  FEIL: Ingen data igjen for analyse.\"); return (pd.DataFrame(),)*6\n",
    "    portfolio_data['excess_ret_t+1'] = portfolio_data['ret_t+1'] - portfolio_data['NextMonthRiskFreeRate_t+1']\n",
    "\n",
    "    print(\"  Sorterer i desiler og beregner vekter...\")\n",
    "    monthly_data_dict = {}; all_decile_dfs = {i: [] for i in range(10)}\n",
    "    unique_months = sorted(portfolio_data['MonthYear'].unique())\n",
    "    for month in unique_months:\n",
    "        monthly_df = portfolio_data[portfolio_data['MonthYear'] == month].copy()\n",
    "        if filter_small_caps:\n",
    "            if 'me' in monthly_df.columns and len(monthly_df) > 10: mc_cutoff = monthly_df['me'].quantile(0.10); monthly_df = monthly_df[monthly_df['me'] >= mc_cutoff].copy()\n",
    "        if len(monthly_df) < 10: continue\n",
    "        monthly_df = monthly_df.sort_values('yhat_t+1')\n",
    "        try:\n",
    "            monthly_df['rank'] = monthly_df['yhat_t+1'].rank(method='first'); monthly_df['DecileRank'] = pd.qcut(monthly_df['rank'], 10, labels=False, duplicates='drop')\n",
    "            if monthly_df['DecileRank'].nunique() < 10: continue\n",
    "        except ValueError: continue\n",
    "        monthly_df = monthly_df.drop(columns=['rank']); monthly_df[\"eq_weights\"] = 1 / monthly_df.groupby('DecileRank')[\"Instrument\"].transform('size'); monthly_df[\"me_weights\"] = monthly_df[\"me\"] / monthly_df.groupby('DecileRank')[\"me\"].transform('sum'); monthly_df[\"me_weights\"] = monthly_df[\"me_weights\"].fillna(0); monthly_data_dict[month] = monthly_df\n",
    "        for decile_rank, group_df in monthly_df.groupby('DecileRank'):\n",
    "            if decile_rank in all_decile_dfs: all_decile_dfs[decile_rank].append(group_df)\n",
    "    decile_portfolios = {j: pd.concat(all_decile_dfs[j], ignore_index=True) if all_decile_dfs[j] else pd.DataFrame() for j in range(10)}\n",
    "    if not any(not df.empty for df in decile_portfolios.values()): print(\"  FEIL: Ingen desilporteføljer konstruert.\"); return (pd.DataFrame(),)*6\n",
    "\n",
    "    print(\"  Beregner desil-metrikker...\")\n",
    "    decile_results = []; monthly_agg_data = {}\n",
    "    for j in range(10):\n",
    "        rank_df = decile_portfolios.get(j, pd.DataFrame());\n",
    "        if rank_df.empty: continue\n",
    "        rank_df['excess_return_stock_ew']=rank_df[\"excess_ret_t+1\"]*rank_df[\"eq_weights\"]; rank_df['excess_return_stock_vw']=rank_df[\"excess_ret_t+1\"]*rank_df[\"me_weights\"]\n",
    "        rank_df['pred_excess_return_stock_ew']=rank_df[\"yhat_t+1\"]*rank_df[\"eq_weights\"]; rank_df['pred_excess_return_stock_vw']=rank_df[\"yhat_t+1\"]*rank_df[\"me_weights\"]\n",
    "        rank_df['return_stock_ew']=rank_df[\"ret_t+1\"]*rank_df[\"eq_weights\"]; rank_df['return_stock_vw']=rank_df[\"ret_t+1\"]*rank_df[\"me_weights\"]\n",
    "        monthly_rank_j = rank_df.groupby('MonthYear').agg(excess_return_portfolio_ew=('excess_return_stock_ew','sum'), excess_return_portfolio_vw=('excess_return_stock_vw','sum'), pred_excess_return_portfolio_ew=('pred_excess_return_stock_ew','sum'), pred_excess_return_portfolio_vw=('pred_excess_return_stock_vw','sum'), return_portfolio_ew=('return_stock_ew','sum'), return_portfolio_vw=('return_stock_vw','sum')).reset_index()\n",
    "        monthly_rank_j['DecileRank'] = j; monthly_agg_data[j] = monthly_rank_j\n",
    "        ew_mean_ret=monthly_rank_j[\"excess_return_portfolio_ew\"].mean(); vw_mean_ret=monthly_rank_j[\"excess_return_portfolio_vw\"].mean()\n",
    "        ew_mean_pred=monthly_rank_j[\"pred_excess_return_portfolio_ew\"].mean(); vw_mean_pred=monthly_rank_j[\"pred_excess_return_portfolio_vw\"].mean()\n",
    "        std_ew_raw=monthly_rank_j[\"return_portfolio_ew\"].std(); std_vw_raw=monthly_rank_j[\"return_portfolio_vw\"].std()\n",
    "        sharpe_ew=(ew_mean_ret/std_ew_raw)*np.sqrt(12) if std_ew_raw>1e-9 else np.nan; sharpe_vw=(vw_mean_ret/std_vw_raw)*np.sqrt(12) if std_vw_raw>1e-9 else np.nan\n",
    "        decile_results.append({'DecileRank':j,'ew_mean_pred':ew_mean_pred,'ew_mean_ret':ew_mean_ret,'std_ew_ret':std_ew_raw,'sharpe_ew':sharpe_ew,'vw_mean_pred':vw_mean_pred,'vw_mean_ret':vw_mean_ret,'std_vw_ret':std_vw_raw,'sharpe_vw':sharpe_vw})\n",
    "\n",
    "    zeronet_monthly = pd.DataFrame(); hl_calculated = False\n",
    "    if 0 in monthly_agg_data and 9 in monthly_agg_data and not monthly_agg_data[0].empty and not monthly_agg_data[9].empty:\n",
    "        long_monthly=monthly_agg_data[9].set_index('MonthYear'); short_monthly=monthly_agg_data[0].set_index('MonthYear'); common_index=long_monthly.index.intersection(short_monthly.index)\n",
    "        if not common_index.empty:\n",
    "             zeronet_monthly = long_monthly.loc[common_index].subtract(short_monthly.loc[common_index], fill_value=0).rename(columns=lambda x: x+'_HL')\n",
    "             if 'DecileRank_HL' in zeronet_monthly.columns: zeronet_monthly = zeronet_monthly.drop(columns=['DecileRank_HL'],errors='ignore')\n",
    "             zeronet_monthly = zeronet_monthly.reset_index(); hl_calculated = True\n",
    "             ew_mean_ret_hl=zeronet_monthly[\"excess_return_portfolio_ew_HL\"].mean(); vw_mean_ret_hl=zeronet_monthly[\"excess_return_portfolio_vw_HL\"].mean(); ew_mean_pred_hl=zeronet_monthly[\"pred_excess_return_portfolio_ew_HL\"].mean(); vw_mean_pred_hl=zeronet_monthly[\"pred_excess_return_portfolio_vw_HL\"].mean()\n",
    "             std_ew_raw_hl=zeronet_monthly[\"return_portfolio_ew_HL\"].std(); std_vw_raw_hl=zeronet_monthly[\"return_portfolio_vw_HL\"].std()\n",
    "             sharpe_ew_hl=(ew_mean_ret_hl/std_ew_raw_hl)*np.sqrt(12) if std_ew_raw_hl>1e-9 else np.nan; sharpe_vw_hl=(vw_mean_ret_hl/std_vw_raw_hl)*np.sqrt(12) if std_vw_raw_hl>1e-9 else np.nan\n",
    "             decile_results.append({'DecileRank':'H-L','ew_mean_pred':ew_mean_pred_hl,'ew_mean_ret':ew_mean_ret_hl,'std_ew_ret':std_ew_raw_hl,'sharpe_ew':sharpe_ew_hl,'vw_mean_pred':vw_mean_pred_hl,'vw_mean_ret':vw_mean_ret_hl,'std_vw_ret':std_vw_raw_hl,'sharpe_vw':sharpe_vw_hl})\n",
    "        else: print(\"  ADVARSEL: Ingen overlappende måneder for H-L.\")\n",
    "    else: print(\"  ADVARSEL: Kan ikke beregne H-L.\");\n",
    "\n",
    "    if not decile_results: print(\"  FEIL: Ingen desilresultater.\"); return (pd.DataFrame(),)*6\n",
    "    results_summary_df = pd.DataFrame(decile_results).set_index('DecileRank')\n",
    "\n",
    "    def format_decile_table(summary_df, weight_scheme):\n",
    "        cols_map={'ew_mean_pred':'Pred','ew_mean_ret':'Avg','std_ew_ret':'SD','sharpe_ew':'SR'} if weight_scheme=='EW' else {'vw_mean_pred':'Pred','vw_mean_ret':'Avg','std_vw_ret':'SD','sharpe_vw':'SR'}\n",
    "        sub_df=summary_df[[k for k in cols_map.keys() if k in summary_df.columns]].rename(columns=cols_map)\n",
    "        for col in ['Pred','Avg','SD']:\n",
    "            if col in sub_df.columns: sub_df[col]=sub_df[col]*100\n",
    "        def map_index(x):\n",
    "            if x==0: return 'Low (L)';\n",
    "            if x==9: return 'High (H)';\n",
    "            if x=='H-L': return 'H-L'\n",
    "            try: return str(int(x)+1)\n",
    "            except: return str(x)\n",
    "        sub_df.index=sub_df.index.map(map_index); desired_order=['Low (L)','2','3','4','5','6','7','8','9','High (H)','H-L']; sub_df=sub_df.reindex([idx for idx in desired_order if idx in sub_df.index])\n",
    "        return sub_df[[col for col in ['Pred','Avg','SD','SR'] if col in sub_df.columns]]\n",
    "\n",
    "    ew_table_unform = format_decile_table(results_summary_df, 'EW'); vw_table_unform = format_decile_table(results_summary_df, 'VW')\n",
    "    ew_table = ew_table_unform.copy(); vw_table = vw_table_unform.copy()\n",
    "    for df_tbl in [ew_table, vw_table]:\n",
    "        for col in ['Pred', 'Avg', 'SD']:\n",
    "             if col in df_tbl.columns: df_tbl[col] = df_tbl[col].map('{:.2f}'.format).replace('nan','N/A')\n",
    "        if 'SR' in df_tbl.columns: df_tbl['SR'] = df_tbl['SR'].map('{:.2f}'.format).replace('nan','N/A')\n",
    "\n",
    "    print(f\"\\n--- Ytelsestabell ({model_name_label} Desiler) - EW ---\"); print(ew_table);\n",
    "    print(f\"\\n--- Ytelsestabell ({model_name_label} Desiler) - VW ---\"); print(vw_table)\n",
    "\n",
    "    print(\"\\n--- Analyse av H-L Portefølje (Turnover, Drawdown, Risk-Adj. Perf.) ---\")\n",
    "    turnover_ew, turnover_vw, maxDD_ew, maxDD_vw, max_loss_ew, max_loss_vw = (np.nan,) * 6; hl_metrics_calculated = False\n",
    "    if hl_calculated and not zeronet_monthly.empty and monthly_data_dict:\n",
    "        all_months_weights = pd.concat(monthly_data_dict.values(), ignore_index=True)\n",
    "        if 0 in all_months_weights['DecileRank'].unique() and 9 in all_months_weights['DecileRank'].unique():\n",
    "             long_weights = all_months_weights[all_months_weights['DecileRank']==9][['MonthYear','Instrument','eq_weights','me_weights']]; short_weights = all_months_weights[all_months_weights['DecileRank']==0][['MonthYear','Instrument','eq_weights','me_weights']]; short_weights[['eq_weights','me_weights']]*=-1\n",
    "             hl_weights = pd.concat([long_weights,short_weights]).sort_values(['Instrument','MonthYear']); hl_weights['eq_weights_lead1'] = hl_weights.groupby('Instrument')['eq_weights'].shift(-1).fillna(0); hl_weights['me_weights_lead1'] = hl_weights.groupby('Instrument')['me_weights'].shift(-1).fillna(0); hl_weights['trade_ew'] = abs(hl_weights['eq_weights_lead1']-hl_weights['eq_weights']); hl_weights['trade_vw'] = abs(hl_weights['me_weights_lead1']-hl_weights['me_weights']); last_month_hl = hl_weights['MonthYear'].max(); monthly_turnover = hl_weights[hl_weights['MonthYear'] != last_month_hl].groupby('MonthYear').agg(sum_trade_ew=('trade_ew','sum'),sum_trade_vw=('trade_vw','sum'))\n",
    "             if not monthly_turnover.empty: turnover_ew=monthly_turnover['sum_trade_ew'].mean()/2; turnover_vw=monthly_turnover['sum_trade_vw'].mean()/2; print(f\"  Avg Turnover (H-L): EW={turnover_ew*100:.2f}%, VW={turnover_vw*100:.2f}%\")\n",
    "        maxDD_ew=MDD(zeronet_monthly['excess_return_portfolio_ew_HL']); maxDD_vw=MDD(zeronet_monthly['excess_return_portfolio_vw_HL']); print(f\"  Max Drawdown (H-L, Excess Ret): EW={abs(maxDD_ew)*100:.2f}%, VW={abs(maxDD_vw)*100:.2f}%\")\n",
    "        max_loss_ew=zeronet_monthly['excess_return_portfolio_ew_HL'].min(); max_loss_vw=zeronet_monthly['excess_return_portfolio_vw_HL'].min(); print(f\"  Max 1M Loss (H-L, Excess Ret): EW={max_loss_ew*100:.2f}%, VW={max_loss_vw*100:.2f}%\")\n",
    "        hl_metrics_calculated = True\n",
    "    else: print(\"  Kan ikke utføre H-L turnover/drawdown analyse.\");\n",
    "\n",
    "    benchmark_name = \"OSEBX\"; print(f\"\\n--- Sammenligning med Benchmark ({benchmark_name}) og Faktor Modell ---\")\n",
    "    alpha_ew, t_alpha_ew, r2_reg_ew = np.nan, np.nan, np.nan; alpha_vw, t_alpha_vw, r2_reg_vw = np.nan, np.nan, np.nan; factor_data_loaded = False; factors = None; bench_data_loaded = False; bench_zeronet = pd.DataFrame(); mean_ret_bench_pct, std_bench_raw_pct, sr_bench, mdd_bench_pct = (np.nan,) * 4\n",
    "\n",
    "    # --- Load Factor Data (COMMENTED OUT) ---\n",
    "    # ... (factor loading code - unchanged) ...\n",
    "    # --- Load Benchmark Data (COMMENTED OUT) ---\n",
    "    # ... (benchmark loading code - unchanged) ...\n",
    "    # --- Factor Regression for H-L Portfolio (COMMENTED OUT) ---\n",
    "    # ... (factor regression code - unchanged) ...\n",
    "\n",
    "    if hl_metrics_calculated:\n",
    "        hl_res = results_summary_df.loc['H-L'] if 'H-L' in results_summary_df.index else pd.Series(dtype=float)\n",
    "        index_perf_hl = [\"Mean Excess Return [%]\", 'Std Dev (Raw) [%]', \"Ann. Sharpe Ratio\", \"Max Drawdown [%]\", \"Avg Monthly Turnover [%]\", \"FF5+Mom Alpha [%]\", \"t(Alpha)\", \"FF5+Mom Adj R2\"]\n",
    "        ew_chart_hl_data = {f'{model_name_label} H-L': [hl_res.get('ew_mean_ret', np.nan) * 100, hl_res.get('std_ew_ret', np.nan) * 100, hl_res.get('sharpe_ew', np.nan), abs(maxDD_ew) * 100 if pd.notna(maxDD_ew) else np.nan, turnover_ew * 100 if pd.notna(turnover_ew) else np.nan, alpha_ew, t_alpha_ew, r2_reg_ew]}\n",
    "        vw_chart_hl_data = {f'{model_name_label} H-L': [hl_res.get('vw_mean_ret', np.nan) * 100, hl_res.get('std_vw_ret', np.nan) * 100, hl_res.get('sharpe_vw', np.nan), abs(maxDD_vw) * 100 if pd.notna(maxDD_vw) else np.nan, turnover_vw * 100 if pd.notna(turnover_vw) else np.nan, alpha_vw, t_alpha_vw, r2_reg_vw]}\n",
    "        if bench_data_loaded:\n",
    "            bench_col_data = [mean_ret_bench_pct, std_bench_raw_pct, sr_bench, mdd_bench_pct, 0, np.nan, np.nan, np.nan]; ew_chart_hl_data[benchmark_name] = bench_col_data; vw_chart_hl_data[benchmark_name] = bench_col_data\n",
    "            ew_chart_hl = pd.DataFrame(ew_chart_hl_data, index=index_perf_hl)[[benchmark_name, f'{model_name_label} H-L']]; vw_chart_hl = pd.DataFrame(vw_chart_hl_data, index=index_perf_hl)[[benchmark_name, f'{model_name_label} H-L']]\n",
    "        else: ew_chart_hl = pd.DataFrame(ew_chart_hl_data, index=index_perf_hl); vw_chart_hl = pd.DataFrame(vw_chart_hl_data, index=index_perf_hl)\n",
    "        print(f\"\\n--- Risk-Adjusted Performance (H-L vs {benchmark_name}, EW) ---\"); print(ew_chart_hl.round(3)); print(f\"\\n--- Risk-Adjusted Performance (H-L vs {benchmark_name}, VW) ---\"); print(vw_chart_hl.round(3))\n",
    "\n",
    "    print(\"\\n--- Analyse av Long-Only (Topp Desil) Portefølje ---\")\n",
    "    rank_9_df = decile_portfolios.get(9, pd.DataFrame()); turnover_ew_long, turnover_vw_long, maxDD_ew_long, maxDD_vw_long, max_loss_ew_long, max_loss_vw_long = (np.nan,) * 6; long_metrics_calculated = False; alpha_long_ew, t_alpha_long_ew, r2_reg_long_ew = np.nan, np.nan, np.nan; alpha_long_vw, t_alpha_long_vw, r2_reg_long_vw = np.nan, np.nan, np.nan\n",
    "    if not rank_9_df.empty:\n",
    "        long_weights = rank_9_df[['MonthYear','Instrument','eq_weights','me_weights']].copy().sort_values(['Instrument','MonthYear']); long_weights['eq_weights_lead1'] = long_weights.groupby('Instrument')['eq_weights'].shift(-1).fillna(0); long_weights['me_weights_lead1'] = long_weights.groupby('Instrument')['me_weights'].shift(-1).fillna(0); long_weights['trade_ew'] = abs(long_weights['eq_weights_lead1']-long_weights['eq_weights']); long_weights['trade_vw'] = abs(long_weights['me_weights_lead1']-long_weights['me_weights']); last_month_long = long_weights['MonthYear'].max(); monthly_turnover_long=long_weights[long_weights['MonthYear']!=last_month_long].groupby('MonthYear').agg(sum_trade_ew=('trade_ew','sum'),sum_trade_vw=('trade_vw','sum'))\n",
    "        if not monthly_turnover_long.empty: turnover_ew_long=monthly_turnover_long['sum_trade_ew'].mean()/2; turnover_vw_long=monthly_turnover_long['sum_trade_vw'].mean()/2; print(f\"  Avg Turnover (Long Only): EW={turnover_ew_long*100:.2f}%, VW={turnover_vw_long*100:.2f}%\")\n",
    "        long_monthly_agg = monthly_agg_data.get(9, pd.DataFrame())\n",
    "        if not long_monthly_agg.empty:\n",
    "            maxDD_ew_long=MDD(long_monthly_agg[\"excess_return_portfolio_ew\"]); maxDD_vw_long=MDD(long_monthly_agg[\"excess_return_portfolio_vw\"]); print(f\"  Max Drawdown (Long Only, Excess Ret): EW={abs(maxDD_ew_long)*100:.2f}%, VW={abs(maxDD_vw_long)*100:.2f}%\")\n",
    "            max_loss_ew_long=long_monthly_agg[\"excess_return_portfolio_ew\"].min(); max_loss_vw_long=long_monthly_agg[\"excess_return_portfolio_vw\"].min(); print(f\"  Max 1M Loss (Long Only, Excess Ret): EW={max_loss_ew_long*100:.2f}%, VW={max_loss_vw_long*100:.2f}%\")\n",
    "            long_metrics_calculated = True\n",
    "            # --- Factor Regression for Long-Only Portfolio (COMMENTED OUT) ---\n",
    "            # ... (factor regression code for long-only - unchanged) ...\n",
    "            if long_metrics_calculated:\n",
    "                long_res = results_summary_df.loc[9] if 9 in results_summary_df.index else pd.Series(dtype=float)\n",
    "                index_perf_long = [\"Mean Excess Return [%]\", 'Std Dev (Raw) [%]', \"Ann. Sharpe Ratio\", \"Max Drawdown [%]\", \"Avg Monthly Turnover [%]\", \"FF5+Mom Alpha [%]\", \"t(Alpha)\", \"FF5+Mom Adj R2\"]\n",
    "                ew_chart_long_data = {f'{model_name_label} Long': [long_res.get('ew_mean_ret', np.nan) * 100, long_res.get('std_ew_ret', np.nan) * 100, long_res.get('sharpe_ew', np.nan), abs(maxDD_ew_long) * 100 if pd.notna(maxDD_ew_long) else np.nan, turnover_ew_long * 100 if pd.notna(turnover_ew_long) else np.nan, alpha_long_ew, t_alpha_long_ew, r2_reg_long_ew]}\n",
    "                vw_chart_long_data = {f'{model_name_label} Long': [long_res.get('vw_mean_ret', np.nan) * 100, long_res.get('std_vw_ret', np.nan) * 100, long_res.get('sharpe_vw', np.nan), abs(maxDD_vw_long) * 100 if pd.notna(maxDD_vw_long) else np.nan, turnover_vw_long * 100 if pd.notna(turnover_vw_long) else np.nan, alpha_long_vw, t_alpha_long_vw, r2_reg_long_vw]}\n",
    "                if bench_data_loaded:\n",
    "                    bench_col_data_long = [mean_ret_bench_pct, std_bench_raw_pct, sr_bench, mdd_bench_pct, 0, np.nan, np.nan, np.nan]; ew_chart_long_data[benchmark_name] = bench_col_data_long; vw_chart_long_data[benchmark_name] = bench_col_data_long\n",
    "                    ew_chart_long = pd.DataFrame(ew_chart_long_data, index=index_perf_long)[[benchmark_name, f'{model_name_label} Long']]; vw_chart_long = pd.DataFrame(vw_chart_long_data, index=index_perf_long)[[benchmark_name, f'{model_name_label} Long']]\n",
    "                else: ew_chart_long = pd.DataFrame(ew_chart_long_data, index=index_perf_long); vw_chart_long = pd.DataFrame(vw_chart_long_data, index=index_perf_long)\n",
    "                print(f\"\\n--- Risk-Adjusted Performance (Long Only vs {benchmark_name}, EW) ---\"); print(ew_chart_long.round(3)); print(f\"\\n--- Risk-Adjusted Performance (Long Only vs {benchmark_name}, VW) ---\"); print(vw_chart_long.round(3))\n",
    "    else: print(\"  Ingen data for Long-Only porteføljeanalyse.\")\n",
    "\n",
    "    print(\"\\n--- Genererer kumulative avkastningsplott (Excess Returns) ---\")\n",
    "    plot_data = {};\n",
    "    for j in [0, 9]:\n",
    "        if j in monthly_agg_data and not monthly_agg_data[j].empty:\n",
    "            df_agg=monthly_agg_data[j].set_index('MonthYear').sort_index();\n",
    "            if not df_agg.empty: df_agg[f'cum_ret_ew_{j}']=(1+df_agg['excess_return_portfolio_ew']).cumprod()-1; df_agg[f'cum_ret_vw_{j}']=(1+df_agg['excess_return_portfolio_vw']).cumprod()-1; plot_data[j]=df_agg\n",
    "    # if bench_data_loaded and not bench_zeronet.empty: bench_plot=bench_zeronet.set_index('MonthYear').sort_index(); bench_plot['cum_ret_bench']=(1+bench_plot['monthly_excess_return_bench']).cumprod()-1; plot_data['benchmark']=bench_plot\n",
    "    def plot_cumulative_returns(plot_data_dict, weight_scheme, model_label, benchmark_label):\n",
    "        fig, ax = plt.subplots(figsize=(15, 7)); legend_items=[]\n",
    "        if 9 in plot_data_dict: col_name=f'cum_ret_{weight_scheme.lower()}_9'; plot_data_dict[9][col_name].plot(ax=ax, label=f'{model_label} Long {weight_scheme.upper()}'); legend_items.append(f'{model_label} Long {weight_scheme.upper()}')\n",
    "        if 0 in plot_data_dict: col_name=f'cum_ret_{weight_scheme.lower()}_0'; plot_data_dict[0][col_name].plot(ax=ax, label=f'{model_label} Short {weight_scheme.upper()}', linestyle=':'); legend_items.append(f'{model_label} Short {weight_scheme.upper()}')\n",
    "        # if 'benchmark' in plot_data_dict: plot_data_dict['benchmark']['cum_ret_bench'].plot(ax=ax, label=benchmark_label, linestyle='--', color='grey'); legend_items.append(benchmark_label)\n",
    "        if legend_items: ax.set_title(f'Kumulativ Excess Avkastning ({weight_scheme.upper()} Vektet)'); ax.set_ylabel('Kumulativ Excess Avkastning'); ax.set_xlabel('Måned'); ax.legend(legend_items); ax.grid(True); fig.tight_layout(); plt.show()\n",
    "        else: plt.close(fig); print(f\"Ingen {weight_scheme.upper()} plottdata tilgjengelig.\")\n",
    "    plot_cumulative_returns(plot_data, 'EW', model_name_label, benchmark_name); plot_cumulative_returns(plot_data, 'VW', model_name_label, benchmark_name)\n",
    "\n",
    "    print(\"--- Detaljert Porteføljeanalyse Fullført ---\")\n",
    "    return ew_table, vw_table, ew_chart_hl, vw_chart_hl, ew_chart_long, vw_chart_long\n",
    "\n",
    "\n",
    "# Step 6.7: Variable Importance Function (Adapted for GBRT)\n",
    "def calculate_variable_importance_single_window(model_params, X_eval, y_eval, features, base_r2):\n",
    "    \"\"\" Calculates permutation importance for GBRT-H for ONE window by retraining. \"\"\"\n",
    "    importance_results = {}\n",
    "    ss_tot_eval = np.sum((y_eval - y_eval.mean())**2)\n",
    "    if ss_tot_eval < 1e-15: return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
    "\n",
    "    if not model_params: print(\"  ADVARSEL (VI): Modellparametre mangler.\"); return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
    "\n",
    "    # Ensure correct fixed settings for GBRT VI runs\n",
    "    current_model_params = model_params.copy()\n",
    "    current_model_params['loss'] = 'huber'\n",
    "    current_model_params['alpha'] = 0.999 # Fix alpha as per main model\n",
    "    current_model_params['random_state'] = 42 # Consistent state\n",
    "\n",
    "    for feature_idx, feature_name in enumerate(features):\n",
    "        X_eval_permuted = X_eval.copy(); X_eval_permuted[:, feature_idx] = 0\n",
    "        try:\n",
    "            # Re-train GBRT with zeroed feature using same optimal params\n",
    "            permuted_model = GradientBoostingRegressor(**current_model_params)\n",
    "            permuted_model.fit(X_eval_permuted, y_eval)\n",
    "            permuted_preds = permuted_model.predict(X_eval_permuted)\n",
    "            if not np.all(np.isfinite(permuted_preds)): permuted_r2 = np.nan\n",
    "            else: ss_res_permuted = np.sum((y_eval - permuted_preds)**2); permuted_r2 = 1 - (ss_res_permuted / ss_tot_eval)\n",
    "            r2_reduction = base_r2 - permuted_r2\n",
    "            importance_results[feature_name] = max(0, r2_reduction) if pd.notna(r2_reduction) else 0.0\n",
    "        except Exception as e: importance_results[feature_name] = 0.0\n",
    "\n",
    "    if not importance_results: return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
    "    imp_df = pd.DataFrame(importance_results.items(), columns=['Feature', 'R2_reduction'])\n",
    "    total_reduction = imp_df['R2_reduction'].sum()\n",
    "    imp_df['Importance'] = imp_df['R2_reduction'] / total_reduction if total_reduction > 1e-9 else 0.0\n",
    "    return imp_df[['Feature', 'Importance']]\n",
    "\n",
    "\n",
    "# Step 6.8 Plot Time-Varying Complexity (Adapted for Max Depth)\n",
    "def plot_time_varying_complexity(model_metrics, model_name='GBRT-H'):\n",
    "     \"\"\" Plots the optimal max_depth over time for GBRT-H \"\"\"\n",
    "     print(f\"\\n--- Plotter Tidsvarierende Modellkompleksitet (Optimal Max Depth for {model_name}) ---\")\n",
    "     complexity_param = 'optim_max_depth' # Parameter to plot\n",
    "\n",
    "     if model_name in model_metrics and complexity_param in model_metrics[model_name]:\n",
    "         values = model_metrics[model_name][complexity_param]\n",
    "         if values and not all(np.isnan(d) for d in values):\n",
    "             valid_values = [(i + 1, int(v)) for i, v in enumerate(values) if pd.notna(v)]\n",
    "             if valid_values:\n",
    "                 windows, plot_values = zip(*valid_values); data = pd.DataFrame({complexity_param: plot_values}, index=pd.Index(windows, name='Window'))\n",
    "                 print(f\"\\n--- Optimal {complexity_param} per Vindu Tabell ({model_name}) ---\"); print(data)\n",
    "                 plt.figure(figsize=(10, 5)); plt.plot(windows, plot_values, marker='o', linestyle='-')\n",
    "                 plot_title = f\"{model_name} Optimal Max Depth per Rullerende Vindu\"; y_label = f\"Optimal Max Depth\"\n",
    "                 plt.xlabel(\"Rullerende Vindu Nummer\"); plt.ylabel(y_label); plt.title(plot_title); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "             else: print(f\"  Ingen gyldige verdier funnet for '{complexity_param}' for {model_name}.\")\n",
    "         else: print(f\"  Ingen data funnet for '{complexity_param}' for {model_name}.\")\n",
    "     else: print(f\"  Metrikk '{complexity_param}' ikke funnet for {model_name}.\")\n",
    "\n",
    "\n",
    "# Step 7: Analyze Prespecified Portfolios (Placeholder)\n",
    "def analyze_prespecified_portfolios(results_df, original_df_subset, portfolio_definitions_file=None, model_name_label=\"GBRT-H\"): # Default Label Changed\n",
    "    \"\"\" Placeholder function for analyzing prespecified portfolios. Needs implementation. \"\"\"\n",
    "    print(\"\\n--- Starter Analyse av Prespesifiserte Porteføljer (Placeholder) ---\")\n",
    "    # ... (rest of placeholder logic - unchanged) ...\n",
    "    pred_col = f'yhat_{model_name_label.lower().replace(\"-\",\"_\").replace(\"+\",\"h\")}'\n",
    "    portfolio_r2_results = pd.DataFrame(); market_timing_results = pd.DataFrame()\n",
    "    if portfolio_definitions_file is None or not os.path.exists(portfolio_definitions_file):\n",
    "        print(\"  ADVARSEL: Fil med porteføljedefinisjoner mangler. Hopper over.\"); return portfolio_r2_results, market_timing_results\n",
    "    # --- IMPLEMENTATION NEEDED ---\n",
    "    print(\"--- Analyse av Prespesifiserte Porteføljer Fullført (Placeholder) ---\")\n",
    "    return portfolio_r2_results, market_timing_results\n",
    "\n",
    "\n",
    "# Step 8: Main Orchestration Function (Adapted for GBRT-H)\n",
    "INITIAL_TRAIN_YEARS_DEFAULT = 9\n",
    "VALIDATION_YEARS_DEFAULT = 6\n",
    "TEST_YEARS_PER_WINDOW_DEFAULT = 1\n",
    "\n",
    "def run_analysis_for_subset(file_path,\n",
    "                            data_subset='all',\n",
    "                            benchmark_file=None,\n",
    "                            ff_factor_file=None,\n",
    "                            portfolio_defs_file=None,\n",
    "                            filter_portfolio_construction=False,\n",
    "                            top_n=1000,\n",
    "                            bottom_n=1000,\n",
    "                            initial_train_years=INITIAL_TRAIN_YEARS_DEFAULT,\n",
    "                            val_years=VALIDATION_YEARS_DEFAULT,\n",
    "                            test_years=TEST_YEARS_PER_WINDOW_DEFAULT\n",
    "                           ):\n",
    "    \"\"\" Runs the full pipeline for one data subset using GBRT-H with YEARLY refitting. \"\"\"\n",
    "    run_label = data_subset.capitalize()\n",
    "    model_name = 'GBRT-H' # *** MODEL NAME SET ***\n",
    "    start_time = datetime.datetime.now(); print(f\"\\n{'='*20} Starter Kjøring: {model_name} for '{run_label}' Firms (ÅRLIG Refitting) {'='*20}\")\n",
    "    if data_subset == 'big': print(f\"(Definert som Topp {top_n} basert på MarketCap per måned)\")\n",
    "    if data_subset == 'small': print(f\"(Definert som Bunn {bottom_n} basert på MarketCap per måned)\")\n",
    "    print(f\"Starttid: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # --- Load & Subset ---\n",
    "    print(\"\\n--- Steg 1: Laster og Forbereder Rådata ---\"); df_raw = load_prepare_data(file_path)\n",
    "    if df_raw is None: return np.nan, None, None, (None,)*6, (None, None)\n",
    "    if 'MarketCap_orig' not in df_raw.columns: print(\"FEIL: MarketCap_orig mangler.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
    "    if 'Date' not in df_raw.columns: print(\"FEIL: Date mangler.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
    "\n",
    "    print(f\"\\n--- Steg 1.1: Lager Subset: {run_label} ---\"); df = pd.DataFrame(); df_raw_mc = df_raw.dropna(subset=['MarketCap_orig', 'Date'])\n",
    "    if data_subset == 'all': df = df_raw.copy()\n",
    "    elif data_subset == 'big': df_raw_mc['MonthYear'] = df_raw_mc['Date'].dt.to_period('M'); df = df_raw_mc.groupby('MonthYear', group_keys=False).apply(lambda x: x.nlargest(top_n, \"MarketCap_orig\")); df = df.drop(columns=['MonthYear'])\n",
    "    elif data_subset == 'small': df_raw_mc['MonthYear'] = df_raw_mc['Date'].dt.to_period('M'); df = df_raw_mc.groupby('MonthYear', group_keys=False).apply(lambda x: x.nsmallest(bottom_n, \"MarketCap_orig\")); df = df.drop(columns=['MonthYear'])\n",
    "    else: print(f\"FEIL: Ukjent subset '{data_subset}'.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
    "    if df.empty: print(f\"FEIL: Subset '{run_label}' er tomt.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
    "    print(f\"Subset '{run_label}' initiell form: {df.shape}\")\n",
    "\n",
    "    # --- Define Features, Standardize & Clean ---\n",
    "    print(\"\\n--- Steg 2 & 1.5: Definerer Features og Rank Standardiserer ---\"); gbrt_features = define_features(df)\n",
    "    if not gbrt_features: print(f\"FEIL: Ingen features funnet i subset '{run_label}'.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
    "    df = rank_standardize_features(df, gbrt_features)\n",
    "    print(\"\\n--- Steg 3: Renser Data (Missing/Inf/Filters) ---\"); essential_cols = ['TargetReturn_t', 'NextMonthlyReturn_t+1', 'MarketCap_orig', 'Date', 'Instrument']\n",
    "    df = clean_data(df, gbrt_features, essential_cols, target=\"TargetReturn_t\")\n",
    "    if df.empty: print(f\"FEIL: DataFrame er tom etter rensing for subset '{run_label}'.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
    "    gbrt_features = [f for f in gbrt_features if f in df.columns and df[f].nunique() > 1 and df[f].std() > 1e-9]; # Refresh features\n",
    "    if not gbrt_features: print(\"FEIL: Ingen features igjen etter datarensing.\"); return np.nan, None, None, (None,)*6, (None, None)\n",
    "    df = df.sort_values([\"Date\", \"Instrument\"]).reset_index(drop=True)\n",
    "\n",
    "    # --- Yearly Rolling Window Setup ---\n",
    "    print(f\"\\n--- Steg 4: Setter opp ÅRLIG Rullerende Vindu (InitTrain={initial_train_years}, Val={val_years}, Test={test_years}) ---\"); results_list = []; model_metrics = defaultdict(lambda: defaultdict(list)); variable_importance_scores_all_windows = []; yhat_col_name = f'yhat_{model_name.lower().replace(\"-\", \"_\").replace(\"+\",\"h\")}'\n",
    "    try: splits_generator = get_yearly_rolling_splits(df, initial_train_years, val_years, test_years); splits = list(splits_generator); num_windows = len(splits)\n",
    "    except ValueError as e: print(f\"FEIL ved generering av årlige vinduer: {e}\"); if 'Year' in df.columns: df.drop(columns=['Year'], inplace=True, errors='ignore'); return np.nan, None, model_metrics, (None,)*6, (None, None)\n",
    "    print(f\"Antall årlige rullerende vinduer som skal kjøres: {num_windows}\\n\");\n",
    "    if num_windows == 0: print(\"Ingen årlige vinduer å kjøre. Avslutter.\"); return np.nan, None, model_metrics, (None,)*6, (None, None)\n",
    "\n",
    "    # --- Rolling Window Loop ---\n",
    "    print(f\"--- Starter {model_name} ÅRLIG Rullerende Vindu Trening & Prediksjon ---\"); total_vi_time = 0\n",
    "    for window, (train_idx, val_idx, test_idx, train_dates, val_dates, test_dates) in enumerate(splits):\n",
    "        window_start_time = datetime.datetime.now(); window_num = window + 1; print(\"-\" * 60)\n",
    "        if test_idx.empty or val_idx.empty or train_idx.empty: print(f\"Vindu {window_num}: Tomt train/val/test. Hopper over.\"); model_metrics[model_name]['oos_r2'].append(np.nan); model_metrics[model_name]['optim_max_depth'].append(np.nan); model_metrics[model_name]['is_r2_train_val'].append(np.nan); continue\n",
    "        X_train=df.loc[train_idx, gbrt_features].values; y_train=df.loc[train_idx, \"TargetReturn_t\"].values; X_val=df.loc[val_idx, gbrt_features].values; y_val=df.loc[val_idx, \"TargetReturn_t\"].values; X_test=df.loc[test_idx, gbrt_features].values; y_test=df.loc[test_idx, \"TargetReturn_t\"].values; X_train_val=np.vstack((X_train, X_val)); y_train_val=np.concatenate((y_train, y_val))\n",
    "        if np.isnan(y_train).all() or np.isnan(y_val).all() or X_train.shape[0]<2 or X_val.shape[0]<2 or np.nanstd(y_train) < 1e-9: print(f\"Vindu {window_num}: Utilstrekkelig data/varians. Hopper over.\"); model_metrics[model_name]['oos_r2'].append(np.nan); model_metrics[model_name]['optim_max_depth'].append(np.nan); model_metrics[model_name]['is_r2_train_val'].append(np.nan); continue\n",
    "\n",
    "        # *** Define GBRT parameter grid for this window ***\n",
    "        gbrt_param_grid = { 'n_estimators': [100], 'learning_rate': [0.1, 0.01], 'max_depth': [1, 2], 'min_samples_split': [1000, 5000], 'min_samples_leaf': [500, 1000], 'max_features': ['sqrt'] }\n",
    "\n",
    "        # *** Call the GBRT-H function ***\n",
    "        trained_model, preds_oos, r2_oos, mse_oos, sharpe_oos, preds_is, r2_is, optim_depth, optim_params = run_gbrt_h_on_window( X_train, y_train, X_val, y_val, X_test, y_test, param_grid=gbrt_param_grid )\n",
    "\n",
    "        # --- Store Results ---\n",
    "        if test_idx.shape[0] > 0 and preds_oos is not None: window_predictions = {'Date': df.loc[test_idx, 'Date'].values, 'Instrument': df.loc[test_idx, 'Instrument'].values, 'TargetReturn_t': y_test, yhat_col_name: preds_oos}; results_list.append(pd.DataFrame(window_predictions))\n",
    "        model_metrics[model_name]['oos_r2'].append(r2_oos); model_metrics[model_name]['optim_max_depth'].append(optim_depth); model_metrics[model_name]['is_r2_train_val'].append(r2_is)\n",
    "\n",
    "        # --- Variable Importance ---\n",
    "        if trained_model is not None and optim_params is not None:\n",
    "            print(f\"  Beregner VI for Vindu {window_num} (GBRT)...\") # Keep less verbose\n",
    "            vi_start_time = time.time(); window_vi_df = calculate_variable_importance_single_window(optim_params, X_train_val, y_train_val, gbrt_features, r2_is);\n",
    "            if window_vi_df is not None and not window_vi_df.empty: variable_importance_scores_all_windows.append(window_vi_df); total_vi_time += time.time() - vi_start_time; print(f\"    VI beregnet på {time.time() - vi_start_time:.2f}s.\")\n",
    "        # elif trained_model is None: print(\"    ADVARSEL: Hovedmodell feilet, hopper over VI.\")\n",
    "        # elif optim_params is None: print(\"    ADVARSEL: Optimal parametre ikke funnet, hopper over VI.\") # Less verbose\n",
    "\n",
    "        print(f\"  Vindu {window_num} fullført på {(datetime.datetime.now() - window_start_time).total_seconds():.1f}s. Opt Depth: {optim_depth}, Win OOS R2: {r2_oos:.4f}\")\n",
    "\n",
    "    # --- Aggregate Results & Overall Analysis ---\n",
    "    if not results_list: print(f\"\\nFEIL: Ingen OOS resultater for {run_label}.\"); return np.nan, None, model_metrics, (None,)*6, (None, None)\n",
    "    results_df = pd.concat(results_list).reset_index(drop=True); print(f\"\\n--- Samlet Resultatanalyse ({run_label}, {model_name}, Årlig Refit) ---\")\n",
    "    y_true_all = results_df['TargetReturn_t']; y_pred_all = results_df[yhat_col_name]; valid_idx_all = y_true_all.notna() & y_pred_all.notna() & np.isfinite(y_true_all) & np.isfinite(y_pred_all); y_t_valid_all = y_true_all[valid_idx_all]; y_p_valid_all = y_pred_all[valid_idx_all]; R2OOS_overall = np.nan\n",
    "    if len(y_t_valid_all) > 1: ss_res_all = np.sum((y_t_valid_all - y_p_valid_all)**2); ss_true_sq_all = np.sum(y_t_valid_all**2);\n",
    "    if ss_true_sq_all > 1e-15: R2OOS_overall = 1 - (ss_res_all / ss_true_sq_all)\n",
    "    avg_yearly_window_r2 = np.nanmean(model_metrics[model_name]['oos_r2']); print(f\"Overall OOS R² ({run_label}, Gu et al. Def): {R2OOS_overall:.6f}\"); print(f\"Average Yearly Window OOS R² ({run_label}):  {avg_yearly_window_r2:.6f}\"); model_metrics[model_name]['oos_r2_overall_gu'] = R2OOS_overall\n",
    "\n",
    "    # --- Detailed Portfolio Analysis ---\n",
    "    portfolio_tables = perform_detailed_portfolio_analysis(results_df, df, benchmark_file=benchmark_file, ff_factor_file=ff_factor_file, filter_small_caps=filter_portfolio_construction, model_name_label=model_name)\n",
    "\n",
    "    # --- Analyze Prespecified Portfolios ---\n",
    "    prespec_r2_table, prespec_timing_table = analyze_prespecified_portfolios(results_df, df, portfolio_definitions_file=portfolio_defs_file, model_name_label=model_name)\n",
    "\n",
    "    # --- Aggregate and Plot Variable Importance ---\n",
    "    averaged_vi_df = None\n",
    "    if variable_importance_scores_all_windows:\n",
    "         print(f\"\\n--- Aggregerer Variabel Viktighet over {len(variable_importance_scores_all_windows)} ÅRLIGE vinduer --- (Total VI tid: {total_vi_time:.1f}s)\")\n",
    "         all_vi_df = pd.concat(variable_importance_scores_all_windows); averaged_vi_df = all_vi_df.groupby('Feature')['Importance'].mean().reset_index(); total_avg_importance = averaged_vi_df['Importance'].sum(); averaged_vi_df['Importance'] = averaged_vi_df['Importance'] / total_avg_importance if total_avg_importance > 1e-9 else 0.0; averaged_vi_df = averaged_vi_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "         print(f\"\\n--- Gjennomsnittlig Variabel Viktighet ({model_name}, Top 10) ---\"); print(averaged_vi_df[['Feature', 'Importance']].head(10).round(4))\n",
    "         plt.figure(figsize=(12, 8)); top_n_plot = 20; plot_df = averaged_vi_df.head(top_n_plot).sort_values(by='Importance', ascending=True); plt.barh(plot_df['Feature'], plot_df['Importance']); plt.xlabel(\"Gj.snitt Relativ Viktighet\"); plt.ylabel(\"Feature\"); plt.title(f'{model_name} Gj.snitt Variabel Viktighet (Top {top_n_plot})'); plt.tight_layout(); plt.show()\n",
    "    else: print(\"\\nIngen VI data samlet.\")\n",
    "\n",
    "    # --- Plot Time-Varying Complexity (Max Depth) ---\n",
    "    plot_time_varying_complexity(model_metrics, model_name)\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    end_time = datetime.datetime.now(); print(f\"\\n--- Kjøring ({run_label}, {model_name}, Årlig Refit) fullført ---\"); print(f\"Sluttid: {end_time.strftime('%Y-%m-%d %H:%M:%S')} (Total tid: {(end_time - start_time)})\"); print(f\"{'='*70}\")\n",
    "    return R2OOS_overall, averaged_vi_df, model_metrics, portfolio_tables, (prespec_r2_table, prespec_timing_table)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Main Execution Block (MODIFIED for GBRT-H)\n",
    "# --------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- CONFIGURATION ---\n",
    "    data_file = \"Cleaned_OSEFX_Market_Macro_Data.csv\" # <--- SET YOUR INPUT DATA FILE\n",
    "\n",
    "    # --- Yearly Split Parameters ---\n",
    "    INITIAL_TRAIN_YEARS = 9\n",
    "    VALIDATION_YEARS = 6\n",
    "    TEST_YEARS_PER_WINDOW = 1\n",
    "\n",
    "    # --- Optional External Data Files ---\n",
    "    benchmark_csv_file = None\n",
    "    ff_factor_csv_file = None\n",
    "    portfolio_defs_csv_file = None\n",
    "\n",
    "    # --- Analysis Settings ---\n",
    "    filter_small_caps_portfolio = False\n",
    "    TOP_N_FIRMS = 1000\n",
    "    BOTTOM_N_FIRMS = 1000\n",
    "\n",
    "    # --- Output Directory ---\n",
    "    output_dir = \"GBRT_H_Results_YearlyRefit\" # *** CHANGED Directory Name ***\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir); print(f\"Opprettet mappe: {output_dir}\")\n",
    "\n",
    "    # --- Check File Existence ---\n",
    "    if not os.path.exists(data_file): print(f\"FEIL: Input datafil ikke funnet på '{data_file}'. Avslutter.\"); exit()\n",
    "    if benchmark_csv_file and not os.path.exists(benchmark_csv_file): print(f\"ADVARSEL: Benchmark fil ikke funnet. Benchmark-sammenligning deaktivert.\"); benchmark_csv_file = None\n",
    "    if ff_factor_csv_file and not os.path.exists(ff_factor_csv_file): print(f\"ADVARSEL: FF+Mom faktorfil ikke funnet. Faktorregresjoner deaktivert.\"); ff_factor_csv_file = None\n",
    "    if portfolio_defs_csv_file and not os.path.exists(portfolio_defs_csv_file): print(f\"ADVARSEL: Porteføljedefinisjonsfil ikke funnet. Prespesifisert analyse deaktivert.\"); portfolio_defs_csv_file = None\n",
    "\n",
    "    # --- RUN ANALYSIS FOR EACH SUBSET ---\n",
    "    results_r2_summary = {}; overall_variable_importance_df = None; all_subset_metrics = {}; all_portfolio_results_tables = {}; all_prespecified_results = {}\n",
    "    subsets_to_run = ['all', 'big', 'small']\n",
    "    model_run_name = 'GBRT-H' # *** Set model name for output ***\n",
    "\n",
    "    for subset in subsets_to_run:\n",
    "        r2, vi_df_avg_run, metrics_run, port_tables_run, prespec_tables_run = run_analysis_for_subset(\n",
    "            file_path=data_file, data_subset=subset, benchmark_file=benchmark_csv_file, ff_factor_file=ff_factor_csv_file, portfolio_defs_file=portfolio_defs_csv_file,\n",
    "            filter_portfolio_construction=filter_small_caps_portfolio, top_n=TOP_N_FIRMS, bottom_n=BOTTOM_N_FIRMS,\n",
    "            initial_train_years=INITIAL_TRAIN_YEARS, val_years=VALIDATION_YEARS, test_years=TEST_YEARS_PER_WINDOW\n",
    "        )\n",
    "        results_r2_summary[subset] = r2; all_subset_metrics[subset] = metrics_run; all_portfolio_results_tables[subset] = port_tables_run; all_prespecified_results[subset] = prespec_tables_run\n",
    "        if subset == 'all' and vi_df_avg_run is not None: overall_variable_importance_df = vi_df_avg_run\n",
    "\n",
    "    # --- Print Final R2 Summary Table ---\n",
    "    print(\"\\n\\n\" + \"=\"*30 + f\" Final OOS R2 Summary ({model_run_name} - Yearly Refit) \" + \"=\"*30)\n",
    "    summary_r2_data = { \"Full Sample (all)\": [results_r2_summary.get('all', np.nan)], f\"Large Firms (Top {TOP_N_FIRMS})\": [results_r2_summary.get('big', np.nan)], f\"Small Firms (Bottom {BOTTOM_N_FIRMS})\": [results_r2_summary.get('small', np.nan)] }\n",
    "    r2_summary_df = pd.DataFrame.from_dict(summary_r2_data, orient='index', columns=[f'{model_run_name} R2oos (%)']); r2_summary_df[f'{model_run_name} R2oos (%)'] *= 100\n",
    "    print(r2_summary_df.round(4))\n",
    "    try: r2_summary_filename = os.path.join(output_dir, f\"{model_run_name.lower().replace('+','h')}_R2oos_summary_subsets_yearly.csv\"); r2_summary_df.to_csv(r2_summary_filename); print(f\" -> R2 Sammendrag lagret til {r2_summary_filename}\")\n",
    "    except Exception as e: print(f\"  FEIL ved lagring av R2 Sammendrag: {e}\")\n",
    "    print(\"=\"*78)\n",
    "\n",
    "    # --- Save Averaged VI results ---\n",
    "    if overall_variable_importance_df is not None:\n",
    "         print(\"\\nLagrer Gjennomsnittlig Variabel Viktighet resultater...\")\n",
    "         try: vi_filename = os.path.join(output_dir, f\"{model_run_name.lower().replace('+','h')}_variable_importance_averaged_yearly.csv\"); overall_variable_importance_df.to_csv(vi_filename, index=False); print(f\" -> Gjennomsnittlig Variabel Viktighet lagret til {vi_filename}\")\n",
    "         except Exception as e: print(f\"  FEIL ved lagring av VI: {e}\")\n",
    "\n",
    "    # --- Save Portfolio Decile Tables and Risk Tables ---\n",
    "    print(\"\\nLagrer Portefølje Desil Tabeller...\")\n",
    "    for subset, tables in all_portfolio_results_tables.items():\n",
    "         if tables and len(tables) == 6:\n",
    "              names = ['decile_ew', 'decile_vw', 'hl_risk_ew', 'hl_risk_vw', 'long_risk_ew', 'long_risk_vw']\n",
    "              for i, table_df in enumerate(tables):\n",
    "                   if table_df is not None and not table_df.empty:\n",
    "                        filename = os.path.join(output_dir, f\"{model_run_name.lower().replace('+','h')}_portfolio_{subset}_{names[i]}_yearly.csv\")\n",
    "                        try: table_df.to_csv(filename, float_format='%.4f'); print(f\" -> Porteføljetabell lagret til {filename}\")\n",
    "                        except Exception as e: print(f\"  FEIL ved lagring av porteføljetabell {filename}: {e}\")\n",
    "\n",
    "    # --- Save Prespecified Portfolio Results (Optional) ---\n",
    "    # print(\"\\nLagrer Prespesifiserte Portefølje Resultater...\")\n",
    "    # ...\n",
    "\n",
    "    print(f\"\\nFull {model_run_name} analyse (Årlig Refitting) fullført.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
