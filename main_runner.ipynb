{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b989582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from config.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 16:00:25.218943: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start: 2025-04-02 16:00:29 ---\n",
      "\n",
      "--- 1. Laster & Forbereder Data ---\n",
      "Laster data fra: Cleaned_OSEFX_Market_Macro_Data.csv\n",
      "Data lastet inn. Form: (34476, 34)\n",
      "Kolonnenavn standardisert.\n",
      "FEIL: Date/Instrument mangler.\n",
      "\n",
      "--- 2. Definerer Features ---\n",
      " FEIL: DataFrame tom.\n",
      "\n",
      "--- 3. Rank Standardiserer Features ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 290\u001b[0m\n\u001b[1;32m    288\u001b[0m all_num, ols3_f, all_needed \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdefine_features(df_prep, config\u001b[38;5;241m.\u001b[39mOLS3_FEATURE_NAMES, [config\u001b[38;5;241m.\u001b[39mTARGET_VARIABLE, config\u001b[38;5;241m.\u001b[39mNEXT_RETURN_VARIABLE, config\u001b[38;5;241m.\u001b[39mMARKET_CAP_ORIG_VARIABLE, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstrument\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonthlyReturn_t\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonthlyRiskFreeRate_t\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_needed: exit()\n\u001b[0;32m--> 290\u001b[0m df_std \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank_standardize_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_prep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_needed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mclean_data(df_std, all_needed, config\u001b[38;5;241m.\u001b[39mESSENTIAL_COLS_FOR_DROPNA, config\u001b[38;5;241m.\u001b[39mMARKET_CAP_ORIG_VARIABLE)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_clean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m df_clean\u001b[38;5;241m.\u001b[39mempty: exit()\n",
      "File \u001b[0;32m~/Desktop/Masteroppgave/Structure/pipeline_utils.py:117\u001b[0m, in \u001b[0;36mrank_standardize_features\u001b[0;34m(df, features_to_standardize)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrank_standardize_features\u001b[39m(df, features_to_standardize):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- 3. Rank Standardiserer Features ---\u001b[39m\u001b[38;5;124m\"\u001b[39m);\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFEIL: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m mangler.\u001b[39m\u001b[38;5;124m\"\u001b[39m); \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m    118\u001b[0m     features\u001b[38;5;241m=\u001b[39m[f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features_to_standardize \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns];\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m features: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Ingen features funnet.\u001b[39m\u001b[38;5;124m\"\u001b[39m); \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# --- main_runner.py ---\n",
    "# Main orchestration script for running the ML asset pricing pipeline.\n",
    "# Imports config and utils, defines model training logic, runs the pipeline loops.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# --- Import Configuration & Utilities ---\n",
    "import config\n",
    "import pipeline_utils as utils\n",
    "\n",
    "# --- Import Model Specific Libraries ---\n",
    "from sklearn.linear_model import LinearRegression, ElasticNetCV, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor # Use this for GBRT_H\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid, KFold\n",
    "from sklearn.metrics import mean_squared_error # Added explicitly\n",
    "\n",
    "try: import statsmodels.api as sm; STATSMODELS_AVAILABLE = True\n",
    "except ImportError: STATSMODELS_AVAILABLE = False; print(\"Statsmodels not found, OLS3H will be skipped.\")\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, regularizers, callbacks, backend as K\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    os.environ['PYTHONHASHSEED']=str(config.TF_SEED); os.environ['TF_CUDNN_DETERMINISTIC']='1'\n",
    "    random.seed(config.TF_SEED); np.random.seed(config.TF_SEED); tf.random.set_seed(config.TF_SEED)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in gpus]; print(f\"GPUs found ({len(gpus)}), memory growth enabled.\")\n",
    "        except RuntimeError as e: print(e)\n",
    "except ImportError: TENSORFLOW_AVAILABLE = False; print(\"TensorFlow/Keras not found, NN models will be skipped.\")\n",
    "\n",
    "# ==========================================================================\n",
    "# --- MODEL TRAINING/EVALUATION FUNCTIONS (Per Window) ---\n",
    "# ==========================================================================\n",
    "\n",
    "# --- Linear Models ---\n",
    "def train_evaluate_ols(X_train_val, y_train_val, X_test, model_params):\n",
    "    try:\n",
    "        model = LinearRegression(fit_intercept=True).fit(X_train_val, y_train_val)\n",
    "        preds_oos = model.predict(X_test) if X_test.shape[0] > 0 else np.array([])\n",
    "        preds_is = model.predict(X_train_val)\n",
    "        return model, preds_oos, preds_is, {}\n",
    "    except Exception as e:\n",
    "        print(f\" FEIL OLS: {e}\")\n",
    "        return None, np.array([]), np.array([]), {}\n",
    "\n",
    "def train_evaluate_ols3h(X_train_val, y_train_val, X_test, model_params):\n",
    "    if not STATSMODELS_AVAILABLE: return None, np.array([]), np.array([]), {}\n",
    "    try:\n",
    "        X_tv_c=sm.add_constant(X_train_val)\n",
    "        X_te_c=sm.add_constant(X_test) if X_test.shape[0]>0 else None\n",
    "        fitted=sm.RLM(y_train_val, X_tv_c, M=sm.robust.norms.HuberT()).fit(**model_params)\n",
    "        preds_oos = fitted.predict(X_te_c) if X_te_c is not None else np.array([])\n",
    "        preds_is = fitted.predict(X_tv_c)\n",
    "        return fitted, preds_oos, preds_is, {'M':'HuberT'}\n",
    "    except Exception as e:\n",
    "        print(f\" FEIL OLS3H: {e}\")\n",
    "        return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# --- Dimension Reduction ---\n",
    "def _tune_simple_model(ModelClass, X_train, y_train, X_val, y_val, param_grid_dict):\n",
    "    best_mse, best_p = np.inf, None\n",
    "    p_name=list(param_grid_dict.keys())[0]; p_vals=param_grid_dict[p_name]\n",
    "    max_c=min(p_vals[-1], X_train.shape[0], X_train.shape[1]); grid=[p for p in p_vals if 0<p<=max_c]; grid=grid if grid else [1]\n",
    "    for p_val in grid:\n",
    "        try:\n",
    "            m_val=Pipeline([('pca',PCA(n_components=p_val)),('lr',LinearRegression())]) if ModelClass==Pipeline else ModelClass(**{p_name:p_val,'scale':False})\n",
    "            m_val.fit(X_train,y_train)\n",
    "            y_pred=m_val.predict(X_val).flatten();\n",
    "            if not np.all(np.isfinite(y_pred)): continue\n",
    "            mse=mean_squared_error(y_val,y_pred);\n",
    "            if not np.isnan(mse) and mse<best_mse: best_mse=mse; best_p=p_val\n",
    "        except Exception: continue # Catch errors for specific param combo\n",
    "    return best_p\n",
    "\n",
    "def train_evaluate_pls(X_train, y_train, X_val, y_val, X_test, model_params):\n",
    "    optimal_params = {}\n",
    "    try:\n",
    "        # *** CORRECTED BLOCK ***\n",
    "        best_n = _tune_simple_model(PLSRegression, X_train, y_train, X_val, y_val, {'n_components': model_params['n_components_grid']})\n",
    "        if best_n is None:\n",
    "            raise ValueError(\"PLS tuning failed\")\n",
    "        optimal_params = {'n_components': best_n}\n",
    "        model = PLSRegression(n_components=best_n, scale=False)\n",
    "        X_tv = np.vstack((X_train, X_val))\n",
    "        y_tv = np.concatenate((y_train, y_val))\n",
    "        model.fit(X_tv, y_tv)\n",
    "        preds_oos = model.predict(X_test).flatten() if X_test.shape[0] > 0 else np.array([])\n",
    "        preds_is = model.predict(X_tv).flatten()\n",
    "        return model, preds_oos, preds_is, optimal_params\n",
    "        # *** END CORRECTION ***\n",
    "    except Exception as e:\n",
    "        print(f\" FEIL PLS: {e}\")\n",
    "        return None, np.array([]), np.array([]), {}\n",
    "\n",
    "def train_evaluate_pcr(X_train, y_train, X_val, y_val, X_test, model_params):\n",
    "    optimal_params = {}\n",
    "    try:\n",
    "        # *** CORRECTED BLOCK ***\n",
    "        best_n = _tune_simple_model(Pipeline, X_train, y_train, X_val, y_val, {'n_components': model_params['n_components_grid']})\n",
    "        if best_n is None:\n",
    "            raise ValueError(\"PCR tuning failed\")\n",
    "        optimal_params = {'n_components': best_n}\n",
    "        model = Pipeline([('pca', PCA(n_components=best_n)), ('lr', LinearRegression())])\n",
    "        X_tv = np.vstack((X_train, X_val))\n",
    "        y_tv = np.concatenate((y_train, y_val))\n",
    "        model.fit(X_tv, y_tv)\n",
    "        preds_oos = model.predict(X_test) if X_test.shape[0] > 0 else np.array([])\n",
    "        preds_is = model.predict(X_tv) # Predict on Train+Val\n",
    "        return model, preds_oos, preds_is, optimal_params\n",
    "        # *** END CORRECTION ***\n",
    "    except Exception as e:\n",
    "        print(f\" FEIL PCR: {e}\")\n",
    "        return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# --- Penalized Linear ---\n",
    "def train_evaluate_enet(X_train, y_train, X_test, model_params):\n",
    "    optimal_params = {}\n",
    "    try:\n",
    "        cv_strategy = KFold(n_splits=model_params['cv_folds'], shuffle=True, random_state=config.GENERAL_SEED)\n",
    "        model = ElasticNetCV(alphas=model_params['alphas'], l1_ratio=model_params['l1_ratio'],\n",
    "                             fit_intercept=True, cv=cv_strategy, n_jobs=model_params.get('n_jobs',-1),\n",
    "                             max_iter=model_params.get('max_iter',1000), tol=model_params.get('tol',0.001),\n",
    "                             random_state=config.GENERAL_SEED)\n",
    "        model.fit(X_train, y_train)\n",
    "        optimal_params = {'alpha': model.alpha_, 'l1_ratio': model.l1_ratio_}\n",
    "        preds_oos = model.predict(X_test) if X_test.shape[0] > 0 else np.array([])\n",
    "        preds_is = model.predict(X_train) # IS Prediction only on Training data for CV models\n",
    "        return model, preds_oos, preds_is, optimal_params\n",
    "    except Exception as e:\n",
    "        print(f\" FEIL ENET: {e}\")\n",
    "        return None, np.array([]), np.array([]), {}\n",
    "\n",
    "def train_evaluate_glm_h(X_train, y_train, X_val, y_val, X_test, model_params):\n",
    "    optimal_params = {}; best_mse = np.inf; optim_found = None\n",
    "    grid = list(ParameterGrid(model_params['param_grid'])); max_iter=model_params.get('max_iter',300)\n",
    "    for params in grid: # Tune\n",
    "        try:\n",
    "            m_v=HuberRegressor(fit_intercept=True,**params,max_iter=max_iter).fit(X_train,y_train)\n",
    "            y_pred_v=m_v.predict(X_val)\n",
    "            if not np.all(np.isfinite(y_pred_v)): continue\n",
    "            mse=mean_squared_error(y_val,y_pred_v)\n",
    "            if not np.isnan(mse) and mse<best_mse: best_mse=mse; optim_found=params\n",
    "        except Exception: continue\n",
    "    if optim_found is None: print(\" FEIL GLM_H Tuning\"); return None,np.array([]),np.array([]),{}\n",
    "    optimal_params=optim_found.copy()\n",
    "    try: # Final fit\n",
    "        # *** CORRECTED BLOCK ***\n",
    "        X_tv=np.vstack((X_train,X_val)); y_tv=np.concatenate((y_train,y_val))\n",
    "        model=HuberRegressor(fit_intercept=True,**optimal_params,max_iter=max_iter).fit(X_tv,y_tv)\n",
    "        preds_oos=model.predict(X_test) if X_test.shape[0]>0 else np.array([])\n",
    "        preds_is=model.predict(X_tv) # Predict on Train+Val\n",
    "        return model,preds_oos,preds_is,optimal_params\n",
    "        # *** END CORRECTION ***\n",
    "    except Exception as e:\n",
    "        print(f\" FEIL GLM_H Final: {e}\")\n",
    "        return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# --- Tree Models ---\n",
    "def _tune_tree_model(ModelClass, X_train, y_train, X_val, y_val, model_params):\n",
    "    best_mse, best_params = np.inf, None; param_grid=list(ParameterGrid(model_params['param_grid'])); base_params={k:v for k,v in model_params.items() if k!='param_grid'}\n",
    "    for params in param_grid:\n",
    "        try:\n",
    "            current_params={**base_params, **params}\n",
    "            # Handle n_jobs for RandomForest specifically\n",
    "            if isinstance(ModelClass, RandomForestRegressor) and 'n_jobs' not in current_params: current_params['n_jobs'] = -1\n",
    "            model_val=ModelClass(**current_params).fit(X_train, y_train)\n",
    "            y_pred_val=model_val.predict(X_val)\n",
    "            if not np.all(np.isfinite(y_pred_val)): continue\n",
    "            mse=mean_squared_error(y_val,y_pred_val)\n",
    "            if not np.isnan(mse) and mse<best_mse: best_mse=mse; best_params=params\n",
    "        except Exception: continue\n",
    "    return best_params\n",
    "\n",
    "def train_evaluate_rf(X_train, y_train, X_val, y_val, X_test, model_params):\n",
    "    optimal_params = {}\n",
    "    try:\n",
    "        # *** CORRECTED BLOCK ***\n",
    "        best_p = _tune_tree_model(RandomForestRegressor, X_train, y_train, X_val, y_val, model_params)\n",
    "        if best_p is None:\n",
    "            raise ValueError(\"RF tuning failed\")\n",
    "        optimal_params = best_p.copy()\n",
    "        final_params = {**{k:v for k,v in model_params.items() if k!='param_grid'}, **optimal_params}\n",
    "        # Ensure n_jobs and random_state are correctly passed if not in tuned params\n",
    "        if 'n_jobs' not in final_params: final_params['n_jobs'] = model_params.get('n_jobs', -1)\n",
    "        if 'random_state' not in final_params: final_params['random_state'] = model_params.get('random_state', 42)\n",
    "\n",
    "        X_tv=np.vstack((X_train,X_val)); y_tv=np.concatenate((y_train,y_val))\n",
    "        model=RandomForestRegressor(**final_params).fit(X_tv, y_tv)\n",
    "        preds_oos = model.predict(X_test) if X_test.shape[0]>0 else np.array([])\n",
    "        preds_is = model.predict(X_tv) # Predict on Train+Val\n",
    "        return model, preds_oos, preds_is, optimal_params\n",
    "        # *** END CORRECTION ***\n",
    "    except Exception as e:\n",
    "        print(f\" FEIL RF: {e}\")\n",
    "        return None, np.array([]), np.array([]), {}\n",
    "\n",
    "def train_evaluate_gbrt_h(X_train, y_train, X_val, y_val, X_test, model_params):\n",
    "    optimal_params = {}\n",
    "    try:\n",
    "        # *** CORRECTED BLOCK ***\n",
    "        best_p = _tune_tree_model(GradientBoostingRegressor, X_train, y_train, X_val, y_val, model_params)\n",
    "        if best_p is None:\n",
    "            raise ValueError(\"GBRT tuning failed\")\n",
    "        optimal_params = best_p.copy()\n",
    "        final_params = {**{k:v for k,v in model_params.items() if k!='param_grid'}, **optimal_params}\n",
    "        final_params['loss']='huber' # Ensure Huber loss is set\n",
    "        if 'random_state' not in final_params: final_params['random_state'] = model_params.get('random_state', 42)\n",
    "\n",
    "        X_tv=np.vstack((X_train,X_val)); y_tv=np.concatenate((y_train,y_val))\n",
    "        model=GradientBoostingRegressor(**final_params).fit(X_tv, y_tv)\n",
    "        preds_oos = model.predict(X_test) if X_test.shape[0]>0 else np.array([])\n",
    "        preds_is = model.predict(X_tv) # Predict on Train+Val\n",
    "        return model, preds_oos, preds_is, optimal_params\n",
    "        # *** END CORRECTION ***\n",
    "    except Exception as e:\n",
    "        print(f\" FEIL GBRT_H: {e}\")\n",
    "        return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# --- Neural Networks ---\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    def build_nn_model(input_shape, nn_config, lambda1):\n",
    "        model=keras.Sequential(name=nn_config['name']); model.add(layers.Input(shape=(input_shape,)))\n",
    "        for u in nn_config['hidden_units']: model.add(layers.Dense(u,activation='relu',kernel_regularizer=regularizers.l1(lambda1)))\n",
    "        model.add(layers.Dense(1,activation='linear')); return model\n",
    "\n",
    "    def train_evaluate_nn(X_train, y_train, X_val, y_val, X_test, model_params, nn_specific_config):\n",
    "        opt_p={}; best_mse=np.inf; optim_found=None; shape=X_train.shape[1]; shared=model_params['NN_SHARED']; grid=list(ParameterGrid(shared['param_grid'])); epochs=shared['epochs']; batch=shared['batch_size']; patience=shared['patience']; ens=shared['ensemble_size']; seed=shared['random_seed_base']; name=nn_specific_config['name']; cb_stop=callbacks.EarlyStopping(monitor='val_loss',patience=patience,restore_best_weights=True,verbose=0); cb_nan=callbacks.TerminateOnNaN()\n",
    "        for params in grid: # Tune\n",
    "            l1=params['lambda1']; lr=params['learning_rate']; val_preds_ens=[];\n",
    "            try:\n",
    "                for i in range(ens): K.clear_session(); tf.random.set_seed(seed+i); m=build_nn_model(shape,nn_specific_config,l1); m.compile(optimizer=Adam(learning_rate=lr),loss='mse'); hist=m.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=epochs,batch_size=batch,callbacks=[cb_stop,cb_nan],verbose=0);\n",
    "                if not np.isnan(hist.history['val_loss']).any(): val_preds_ens.append(m.predict(X_val,batch_size=batch).flatten())\n",
    "                else: val_preds_ens=[]; break\n",
    "                if not val_preds_ens: continue; avg_v_preds=np.mean(np.array(val_preds_ens),axis=0); mse=mean_squared_error(y_val[np.isfinite(avg_v_preds)],avg_v_preds[np.isfinite(avg_v_preds)]);\n",
    "                if not np.isnan(mse) and mse<best_mse: best_mse=mse; optim_found=params\n",
    "            except Exception: continue\n",
    "        if optim_found is None: print(f\" FEIL {name} Tuning\"); return None,np.array([]),np.array([]),{}\n",
    "        opt_p=optim_found.copy(); opt_l1=opt_p['lambda1']; opt_lr=opt_p['learning_rate']\n",
    "        # Final Train\n",
    "        final_m=None; test_preds_ens, is_preds_ens=[],[]; X_tv=np.vstack((X_train,X_val)); y_tv=np.concatenate((y_train,y_val))\n",
    "        try:\n",
    "            # *** CORRECTED BLOCK ***\n",
    "            for i in range(ens):\n",
    "                K.clear_session(); tf.random.set_seed(seed+i+ens)\n",
    "                m=build_nn_model(shape,nn_specific_config,opt_l1)\n",
    "                m.compile(optimizer=Adam(learning_rate=opt_lr),loss='mse') # Use learning_rate here\n",
    "                hist_f=m.fit(X_tv,y_tv,epochs=epochs,batch_size=batch,callbacks=[cb_nan],verbose=0);\n",
    "                if not np.isnan(hist_f.history['loss']).any():\n",
    "                    if X_test.shape[0]>0: test_preds_ens.append(m.predict(X_test,batch_size=batch).flatten())\n",
    "                    is_preds_ens.append(m.predict(X_tv,batch_size=batch).flatten()); # IS on Train+Val\n",
    "                    if i==0: final_m=m # Store one instance\n",
    "                else: test_preds_ens=[]; is_preds_ens=[]; break # Fail fast\n",
    "\n",
    "            if X_test.shape[0]>0 and not test_preds_ens: raise ValueError(f\"{name}: OOS pred failed\")\n",
    "            if not is_preds_ens: raise ValueError(f\"{name}: IS pred failed\")\n",
    "\n",
    "            p_oos=np.mean(np.array(test_preds_ens),axis=0) if X_test.shape[0]>0 else np.array([])\n",
    "            p_is=np.mean(np.array(is_preds_ens),axis=0);\n",
    "            return final_m,p_oos,p_is,opt_p\n",
    "            # *** END CORRECTION ***\n",
    "        except Exception as e:\n",
    "             print(f\" FEIL {name} Final: {e}\");\n",
    "             return None, np.array([]), np.array([]), {}\n",
    "\n",
    "# ==========================================================================\n",
    "# --- MAIN EXECUTION SCRIPT ---\n",
    "# ==========================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    overall_start_time = datetime.datetime.now()\n",
    "    print(f\"--- Start: {overall_start_time:%Y-%m-%d %H:%M:%S} ---\")\n",
    "\n",
    "    # === 1-4: Load, Prep, Features, Standardize, Clean ===\n",
    "    df_prep = utils.load_prepare_data(config.DATA_FILE, config.COLUMN_CONFIG, config.VARS_TO_LOG, config.WINSORIZE_LIMITS, config.TARGET_VARIABLE, config.NEXT_RETURN_VARIABLE, config.MARKET_CAP_ORIG_VARIABLE)\n",
    "    if df_prep is None: exit()\n",
    "    all_num, ols3_f, all_needed = utils.define_features(df_prep, config.OLS3_FEATURE_NAMES, [config.TARGET_VARIABLE, config.NEXT_RETURN_VARIABLE, config.MARKET_CAP_ORIG_VARIABLE, 'Instrument', 'Date', 'MonthlyReturn_t', 'MonthlyRiskFreeRate_t'])\n",
    "    if not all_needed: exit()\n",
    "    df_std = utils.rank_standardize_features(df_prep, all_needed)\n",
    "    df_clean = utils.clean_data(df_std, all_needed, config.ESSENTIAL_COLS_FOR_DROPNA, config.MARKET_CAP_ORIG_VARIABLE)\n",
    "    if df_clean is None or df_clean.empty: exit()\n",
    "    all_num, ols3_f, _ = utils.define_features(df_clean, config.OLS3_FEATURE_NAMES, [config.TARGET_VARIABLE, config.NEXT_RETURN_VARIABLE, config.MARKET_CAP_ORIG_VARIABLE, 'Instrument', 'Date', 'MonthlyReturn_t', 'MonthlyRiskFreeRate_t'])\n",
    "    if not ols3_f: config.RUN_MODELS['OLS3H'] = False; print(\"ADVARSEL: OLS3H deaktivert.\")\n",
    "    if not all_num: print(\"FEIL: Ingen numeriske features igjen.\"); exit()\n",
    "\n",
    "    # === Initialize Results Storage ===\n",
    "    all_metrics = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    all_vi_avg = defaultdict(dict)\n",
    "    all_portfolios = defaultdict(dict)\n",
    "    all_summaries = {}\n",
    "\n",
    "    # === 5: Outer Loop: Subsets ===\n",
    "    for subset in config.SUBSETS_TO_RUN:\n",
    "        subset_start = datetime.datetime.now()\n",
    "        print(f\"\\n{'='*25} Starter Subset: {subset.upper()} {'='*25}\")\n",
    "        # --- Create Subset (Using PERCENTILES) ---\n",
    "        df_sub = pd.DataFrame(); df_mc = df_clean.dropna(subset=[config.MARKET_CAP_ORIG_VARIABLE, 'Date']).copy()\n",
    "        if df_mc.empty: print(f\"FEIL: Ingen data for subsetting i {subset}.\"); continue\n",
    "        if subset=='all': df_sub = df_clean.copy()\n",
    "        else:\n",
    "            df_mc['MonthYear'] = df_mc['Date'].dt.to_period('M')\n",
    "            if subset=='big': cutoff_perc=1-(config.BIG_FIRM_TOP_PERCENT/100.0); size_cutoffs=df_mc.groupby('MonthYear')[config.MARKET_CAP_ORIG_VARIABLE].quantile(cutoff_perc); print(f\"(Definert som Topp {config.BIG_FIRM_TOP_PERCENT}%)\")\n",
    "            elif subset=='small': cutoff_perc=config.SMALL_FIRM_BOTTOM_PERCENT/100.0; size_cutoffs=df_mc.groupby('MonthYear')[config.MARKET_CAP_ORIG_VARIABLE].quantile(cutoff_perc); print(f\"(Definert som Bunn {config.SMALL_FIRM_BOTTOM_PERCENT}%)\")\n",
    "            else: print(\"FEIL: Ukjent subset type\"); continue\n",
    "            df_sub = df_mc.join(size_cutoffs.rename('cutoff'), on='MonthYear')\n",
    "            df_sub = df_sub[df_sub[config.MARKET_CAP_ORIG_VARIABLE] >= df_sub['cutoff']] if subset == 'big' else df_sub[df_sub[config.MARKET_CAP_ORIG_VARIABLE] <= df_sub['cutoff']]\n",
    "            df_sub = df_sub.drop(columns=['MonthYear', 'cutoff'], errors='ignore')\n",
    "        if df_sub.empty: print(f\"FEIL: Tomt subset {subset}.\"); continue\n",
    "        df_sub = df_sub.sort_values([\"Date\", \"Instrument\"]).reset_index(drop=True); print(f\"Subset form: {df_sub.shape}\")\n",
    "\n",
    "        # === 6: Inner Loop: Rolling Windows ===\n",
    "        try: splits = list(utils.get_yearly_rolling_splits(df_sub, config.INITIAL_TRAIN_YEARS, config.VALIDATION_YEARS, config.TEST_YEARS_PER_WINDOW))\n",
    "        except ValueError as e: print(f\"FEIL splits {subset}: {e}\"); continue\n",
    "        if not splits: print(f\"Ingen vinduer for {subset}.\"); continue\n",
    "        window_preds_list = []; last_train_idx, last_val_idx, last_models = None, None, {}\n",
    "        all_vi_window = defaultdict(list)\n",
    "\n",
    "        for window, (train_idx, val_idx, test_idx, _, _, _) in enumerate(splits):\n",
    "            win_num = window + 1; win_start = time.time(); print(f\"-- Vindu {win_num}/{len(splits)} --\")\n",
    "            if test_idx.empty or val_idx.empty or train_idx.empty: print(\" Tomt sett.\"); continue\n",
    "            y_train=df_sub.loc[train_idx,config.TARGET_VARIABLE].values; y_val=df_sub.loc[val_idx,config.TARGET_VARIABLE].values; y_test=df_sub.loc[test_idx,config.TARGET_VARIABLE].values; y_train_val=np.concatenate((y_train,y_val))\n",
    "            window_preds={'Date':df_sub.loc[test_idx,'Date'].values,'Instrument':df_sub.loc[test_idx,'Instrument'].values,config.TARGET_VARIABLE:y_test}\n",
    "            window_models_fit={}\n",
    "\n",
    "            # === 7: Innermost Loop: Models ===\n",
    "            for model_name, do_run in config.RUN_MODELS.items():\n",
    "                if not do_run: continue\n",
    "                print(f\"  -> {model_name}...\")\n",
    "                m_start=time.time(); fitted_model,p_oos,p_is,opt_p = None,np.array([]),np.array([]),{}\n",
    "                f_key=config.MODEL_FEATURE_MAP.get(model_name); current_f=ols3_f if f_key=='ols3_features' else all_num\n",
    "                m_params=config.MODEL_PARAMS.get(model_name,{});\n",
    "                if not current_f: print(f\"    FEIL: Mangler features.\"); continue\n",
    "                X_train=df_sub.loc[train_idx,current_f].values; X_val=df_sub.loc[val_idx,current_f].values; X_test=df_sub.loc[test_idx,current_f].values; X_train_val=np.vstack((X_train,X_val))\n",
    "                min_obs_train=max(2,X_train.shape[1]+1 if model_name=='OLS3H' else 2)\n",
    "                if np.isnan(y_train).all() or X_train.shape[0]<min_obs_train: print(f\"    Utilstrekkelig train data.\"); continue\n",
    "                if np.isnan(y_val).all() or X_val.shape[0]<2:\n",
    "                    if model_name not in ['OLS','OLS3H','ENET']: print(f\"    Utilstrekkelig val data.\"); continue\n",
    "\n",
    "                try: # Call training function\n",
    "                    train_func_name = f\"train_evaluate_{model_name.lower().replace('-','').replace('+','_')}\"\n",
    "                    train_func = locals().get(train_func_name)\n",
    "                    if train_func:\n",
    "                        if model_name in ['OLS','OLS3H']: fitted_model,p_oos,p_is,opt_p = train_func(X_train_val,y_train_val,X_test,m_params); y_is_target=y_train_val\n",
    "                        elif model_name == 'ENET': fitted_model,p_oos,p_is,opt_p = train_func(X_train,y_train,X_test,m_params); y_is_target=y_train\n",
    "                        elif model_name.startswith('NN'):\n",
    "                             if TENSORFLOW_AVAILABLE: fitted_model,p_oos,p_is,opt_p = train_func(X_train,y_train,X_val,y_val,X_test,m_params,config.MODEL_PARAMS[model_name]); y_is_target=y_train_val\n",
    "                             else: continue\n",
    "                        else: fitted_model,p_oos,p_is,opt_p = train_func(X_train,y_train,X_val,y_val,X_test,m_params); y_is_target=y_train_val\n",
    "                    else: print(f\"    FEIL: Fant ikke {train_func_name}.\"); continue\n",
    "                    # Metrics & Storage\n",
    "                    r2_oos=utils.calculate_oos_r2(y_test,p_oos); r2_is=utils.calculate_oos_r2(y_is_target,p_is); sharpe_oos=utils.calculate_sharpe_of_predictions(p_oos)\n",
    "                    all_metrics[subset][model_name]['oos_r2'].append(r2_oos); all_metrics[subset][model_name]['is_r2_train_val'].append(r2_is); all_metrics[subset][model_name]['oos_sharpe'].append(sharpe_oos)\n",
    "                    for pname,pval in opt_p.items(): all_metrics[subset][model_name][f'optim_{pname}'].append(pval)\n",
    "                    window_preds[f'yhat_{model_name.lower()}']=p_oos if p_oos is not None else np.nan; window_models_fit[model_name]=fitted_model\n",
    "                    print(f\"    {model_name}: R2={r2_oos:.4f} ({time.time()-m_start:.1f}s)\")\n",
    "                    # Per-window VI\n",
    "                    if config.CALCULATE_VI and config.MODEL_VI_STRATEGY.get(model_name)=='per_window':\n",
    "                         vi_start=time.time(); vi_df=utils.calculate_variable_importance(model_name,fitted_model,X_train_val,y_train_val,current_f,r2_is,config.VI_METHOD,opt_p)\n",
    "                         if vi_df is not None and not vi_df.empty: all_vi_window[model_name].append(vi_df)\n",
    "                except Exception as e: print(f\"    !!! FEIL {model_name}: {e}\"); traceback.print_exc(); all_metrics[subset][model_name]['oos_r2'].append(np.nan); window_preds[f'yhat_{model_name.lower()}']=np.nan\n",
    "\n",
    "            window_preds_list.append(pd.DataFrame(window_preds))\n",
    "            if window==len(splits)-1: last_train_idx,last_val_idx,last_models = train_idx.copy(),val_idx.copy(),window_models_fit.copy()\n",
    "            print(f\"-- Vindu {win_num} ferdig ({time.time()-win_start:.1f}s) --\")\n",
    "        # End window loop\n",
    "\n",
    "        # Aggregate per-window VI\n",
    "        if config.CALCULATE_VI:\n",
    "            for model_name, vi_list in all_vi_window.items():\n",
    "                 if vi_list: all_vi[subset][model_name].extend(vi_list)\n",
    "\n",
    "        # === 8-10: Post-Window Analysis ===\n",
    "        if not window_preds_list: print(f\"Ingen resultater for {subset}.\"); continue\n",
    "        results_df_sub=pd.concat(window_preds_list).reset_index(drop=True); pred_cols_sub=[c for c in results_df_sub.columns if c.startswith('yhat_')]\n",
    "        if not pred_cols_sub: continue\n",
    "\n",
    "        # Overall OOS R2\n",
    "        print(f\"\\n--- Overall OOS R2 ({subset}) ---\")\n",
    "        y_true_s=results_df_sub[config.TARGET_VARIABLE].dropna(); ss_tot_s=np.sum(y_true_s**2)\n",
    "        if len(y_true_s)>1 and ss_tot_s>1e-15:\n",
    "            for pc in pred_cols_sub: mn=pc.replace('yhat_','').upper(); y_pred_s=results_df_sub[pc]; r2_o=utils.calculate_oos_r2(y_true_s,y_pred_s); all_metrics[subset][mn]['oos_r2_overall_gu']=r2_o; print(f\"  {mn}: {r2_o:.6f}\")\n",
    "\n",
    "        # Portfolio Analysis\n",
    "        decile_t, hl_risk_t, long_risk_t = utils.perform_detailed_portfolio_analysis(results_df_sub, df_clean, pred_cols_sub, config.MARKET_CAP_ORIG_VARIABLE, config.NEXT_RETURN_VARIABLE, 'MonthlyRiskFreeRate_t', config.FILTER_SMALL_CAPS_PORTFOLIO, config.ANNUALIZATION_FACTOR, config.BENCHMARK_FILE, config.FF_FACTOR_FILE)\n",
    "        all_portfolios[subset]={'decile_tables':decile_t, 'hl_risk_tables':hl_risk_t, 'long_risk_tables':long_risk_t}\n",
    "\n",
    "        # Variable Importance (Final)\n",
    "        if config.CALCULATE_VI:\n",
    "            print(f\"\\n--- Variabel Viktighet ({subset}) ---\")\n",
    "            for model_name, do_run in config.RUN_MODELS.items():\n",
    "                if not do_run: continue\n",
    "                vi_strat=config.MODEL_VI_STRATEGY.get(model_name); f_key=config.MODEL_FEATURE_MAP.get(model_name); current_f=ols3_f if f_key=='ols3_features' else all_num\n",
    "                if not current_f: continue\n",
    "                if vi_strat=='per_window':\n",
    "                    vi_list=all_vi[subset].get(model_name,[])\n",
    "                    if vi_list: avg_vi=pd.concat(vi_list).groupby('Feature')['Importance'].mean().reset_index(); tot_avg=avg_vi['Importance'].sum(); avg_vi['Importance']=avg_vi['Importance']/tot_avg if tot_avg>1e-9 else 0.0; all_vi_avg[subset][model_name]=avg_vi.sort_values('Importance',ascending=False).reset_index(drop=True); print(f\"  VI (Avg) for {model_name} beregnet.\")\n",
    "                elif vi_strat=='last_window':\n",
    "                    if last_train_idx is None or model_name not in last_models: print(f\"  Skipping last_window VI for {model_name}.\"); continue\n",
    "                    print(f\"  Beregner last_window VI for {model_name}...\"); vi_start=time.time(); last_model=last_models[model_name]\n",
    "                    last_is_r2=all_metrics[subset][model_name]['is_r2_train_val'][-1] if all_metrics[subset][model_name]['is_r2_train_val'] else np.nan\n",
    "                    X_tv_last=df_sub.loc[last_train_idx.union(last_val_idx),current_f].values; y_tv_last=df_sub.loc[last_train_idx.union(last_val_idx),config.TARGET_VARIABLE].values\n",
    "                    last_opt_params={k.replace('optim_',''):v[-1] for k,v in all_metrics[subset][model_name].items() if k.startswith('optim_') and v}\n",
    "                    vi_df=utils.calculate_variable_importance(model_name,last_model,X_tv_last,y_tv_last,current_f,last_is_r2,config.VI_METHOD,last_opt_params)\n",
    "                    if vi_df is not None and not vi_df.empty: all_vi_avg[subset][model_name]=vi_df.sort_values('Importance',ascending=False).reset_index(drop=True); print(f\"  VI ({model_name}) ferdig ({time.time()-vi_start:.1f}s)\")\n",
    "\n",
    "        # Summary Table & Plots\n",
    "        all_summaries[subset] = utils.create_summary_table(all_metrics[subset], config.ANNUALIZATION_FACTOR)\n",
    "        utils.plot_time_varying_complexity(all_metrics[subset], config.COMPLEXITY_PARAMS_TO_PLOT)\n",
    "        if config.CALCULATE_VI and all_vi_avg[subset]:\n",
    "             print(f\"\\n--- Plotter VI for Subset: {subset} ---\")\n",
    "             for model_name, vi_df in all_vi_avg[subset].items():\n",
    "                 plt.figure(figsize=(10,max(6, config.VI_PLOT_TOP_N*0.3))); plot_df=vi_df[vi_df['Importance']>1e-6].head(config.VI_PLOT_TOP_N).sort_values(by='Importance',ascending=True)\n",
    "                 if not plot_df.empty: plt.barh(plot_df['Feature'],plot_df['Importance']); plt.xlabel(\"Relativ Viktighet\"); plt.title(f\"{model_name} VI ({subset} - Top {config.VI_PLOT_TOP_N})\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "        # Save Results\n",
    "        results_to_save={'summary_metrics':all_summaries[subset],'portfolio_deciles':all_portfolios[subset].get('decile_tables',{}),'portfolio_hl_risk':all_portfolios[subset].get('hl_risk_tables',{}),'portfolio_long_risk':all_portfolios[subset].get('long_risk_tables',{}),'variable_importance_avg':all_vi_avg[subset]}\n",
    "        utils.save_results(config.OUTPUT_DIR, subset, results_to_save)\n",
    "        print(f\"\\n{'='*25} Subset Fullført: {subset.upper()} (Tid: {datetime.datetime.now() - subset_start}) {'='*25}\")\n",
    "    # End subset loop\n",
    "\n",
    "    # === Final Reporting ===\n",
    "    print(\"\\n\\n\" + \"=\"*30 + \" SLUTTSAMMENDRAG \" + \"=\"*30)\n",
    "    r2_final = defaultdict(dict)\n",
    "    for sub in config.SUBSETS_TO_RUN:\n",
    "        for model, mets in all_metrics[sub].items(): r2_final[sub][model] = mets.get('oos_r2_overall_gu', np.nan) * 100\n",
    "    r2_summary_final = pd.DataFrame.from_dict(r2_final, orient='index')\n",
    "    model_order=['OLS','OLS3H','PLS','PCR','ENET','GLM_H','RF','GBRT_H','NN1','NN2','NN3','NN4','NN5']\n",
    "    cols_ordered=[m for m in model_order if m in r2_summary_final.columns]+[m for m in r2_summary_final.columns if m not in model_order]\n",
    "    r2_summary_final = r2_summary_final[cols_ordered]; r2_summary_final.index.name=\"Subset\"; r2_summary_final.columns.name=\"Model\"\n",
    "    print(\"--- Tabell 1 Stil: Overall Monthly OOS R2 (%) [Gu et al. Def] ---\"); print(r2_summary_final.round(4))\n",
    "    utils.save_results(config.OUTPUT_DIR, \"consolidated\", {\"R2_summary_table1_style\": r2_summary_final})\n",
    "\n",
    "    print(f\"\\n--- Pipeline Fullført --- ({datetime.datetime.now():%Y-%m-%d %H:%M:%S})\")\n",
    "    print(f\"Total kjøretid: {datetime.datetime.now() - overall_start_time}\")\n",
    "    print(f\"Resultater lagret i: {config.OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
