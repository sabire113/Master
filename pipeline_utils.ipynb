{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4537de04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from config.py\n"
     ]
    }
   ],
   "source": [
    "# --- pipeline_utils.py ---\n",
    "# Shared utility functions for the ML Asset Pricing Pipeline.\n",
    "# Contains functions for data loading/prep, feature definition,\n",
    "# standardization, cleaning, splitting, portfolio analysis, VI, plotting, saving.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from scipy.stats.mstats import winsorize # No longer needed here\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "# Import config AFTER it's defined\n",
    "import config\n",
    "\n",
    "# --- Suppress specific warnings ---\n",
    "# (Keep suppression settings as they are)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Mean of empty slice\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"invalid value encountered in log\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Maximum number of iterations reached.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"divide by zero encountered.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"invalid value encountered.*divide\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Set general random seed\n",
    "random.seed(config.GENERAL_SEED)\n",
    "np.random.seed(config.GENERAL_SEED)\n",
    "\n",
    "\n",
    "# === Stage 1: Data Loading and Preparation (SIMPLIFIED) ===\n",
    "def find_col(df, potential_names, default=None):\n",
    "    \"\"\"Helper to find the first matching column name from a list.\"\"\"\n",
    "    for name in potential_names:\n",
    "        if name in df.columns: return name\n",
    "    return default\n",
    "\n",
    "def load_prepare_data(file_path, column_config, target_var_name, next_ret_var_name, mkt_cap_orig_var_name):\n",
    "    \"\"\"\n",
    "    Loads the PREPROCESSED data file.\n",
    "    Performs minimal checks: date conversion, finds essential columns (Date, ID, Target, NextReturn, MktCapOrig).\n",
    "    Does NOT calculate returns, targets, log transforms, etc. as this is assumed done externally.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 1. Laster Forhåndsbehandlet Data ---\")\n",
    "    print(f\"Laster data fra: {file_path}\")\n",
    "    try:\n",
    "        # Load data, ensuring Date is parsed\n",
    "        df = pd.read_csv(file_path, parse_dates=['Date']) # Assume 'Date' is the date column\n",
    "        print(f\"Forhåndsbehandlet data lastet inn. Form: {df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FEIL: Fil '{file_path}' ikke funnet.\"); return None\n",
    "    except Exception as e:\n",
    "        print(f\"FEIL under lasting av forhåndsbehandlet data: {e}\"); return None\n",
    "\n",
    "    # --- 1. Find and Standardize Essential Column Names ---\n",
    "    std_names_map = {\n",
    "        'date': 'Date',\n",
    "        'id': 'Instrument',\n",
    "        # Add other potential mappings from config if needed, but keep it minimal\n",
    "    }\n",
    "    rename_dict = {}\n",
    "    found_cols = {}\n",
    "\n",
    "    # Find Date column\n",
    "    date_col_found = find_col(df, column_config.get('date', ['Date', 'date']))\n",
    "    if not date_col_found: print(\"FEIL: Datokolonne ikke funnet.\"); return None\n",
    "    if date_col_found != 'Date': rename_dict[date_col_found] = 'Date'\n",
    "    found_cols['date'] = 'Date'\n",
    "\n",
    "    # Find ID column\n",
    "    id_col_found = find_col(df, column_config.get('id', ['Instrument', 'instrument']))\n",
    "    if not id_col_found: print(\"FEIL: Instrument ID kolonne ikke funnet.\"); return None\n",
    "    if id_col_found != 'Instrument': rename_dict[id_col_found] = 'Instrument'\n",
    "    found_cols['id'] = 'Instrument'\n",
    "\n",
    "    # Apply renames if necessary\n",
    "    if rename_dict:\n",
    "        df = df.rename(columns=rename_dict)\n",
    "        print(f\"Standardiserte essensielle kolonner: {list(rename_dict.values())}\")\n",
    "\n",
    "    # --- 2. Verify Essential Columns Exist ---\n",
    "    essential_cols = ['Date', 'Instrument', target_var_name, next_ret_var_name, mkt_cap_orig_var_name]\n",
    "    # Check if sector column exists IF it's needed later (e.g., for dummies, although dummies should be pre-created now)\n",
    "    if 'EconomicSector' in column_config:\n",
    "         sector_col_cand = find_col(df, column_config['EconomicSector'])\n",
    "         if sector_col_cand: essential_cols.append(sector_col_cand)\n",
    "\n",
    "    missing_essential = [col for col in essential_cols if col not in df.columns]\n",
    "    if missing_essential:\n",
    "        print(f\"FEIL: Essensielle kolonner mangler i forhåndsbehandlet fil: {missing_essential}\")\n",
    "        print(f\"Tilgjengelige kolonner: {df.columns.tolist()}\")\n",
    "        return None\n",
    "    print(\"Essensielle kolonner funnet.\")\n",
    "\n",
    "    # --- 3. Ensure Date is Datetime and Sort ---\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
    "        try:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "        except Exception as e:\n",
    "            print(f\"FEIL: Kunne ikke konvertere 'Date' kolonne til datetime: {e}\"); return None\n",
    "    df = df.sort_values(by=['Instrument', 'Date']).reset_index(drop=True)\n",
    "    print(\"Data sortert etter Instrument og Dato.\")\n",
    "\n",
    "    # --- 4. Optional: Create Sector Dummies (if not already done in preprocessing) ---\n",
    "    # Check if sector column exists and if dummy columns DON'T already exist\n",
    "    sector_col_std = find_col(df, column_config.get('EconomicSector', []))\n",
    "    if sector_col_std and not any(col.startswith(\"Sector_\") for col in df.columns):\n",
    "         print(f\"  INFO: Oppretter Sektor dummy-variabler fra '{sector_col_std}'...\")\n",
    "         df = pd.get_dummies(df, columns=[sector_col_std], prefix=\"Sector\", dtype=int)\n",
    "         print(\"  Sektor dummy-variabler opprettet.\")\n",
    "\n",
    "    print(f\"Lasting og grunnleggende sjekk fullført. Form: {df.shape}\")\n",
    "    # print(f\"Final Columns: {df.columns.tolist()}\") # Uncomment for detailed debug\n",
    "    return df\n",
    "\n",
    "\n",
    "# === Stage 2: Feature Definition ===\n",
    "def define_features(df, ols3_feature_names, base_exclusions):\n",
    "    \"\"\"\n",
    "    Identifies numeric features from the PREPROCESSED data,\n",
    "    excluding specified base columns (target, IDs, intermediate calcs that might remain).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 2. Definerer Features (fra forhåndsbehandlet data) ---\")\n",
    "    if df is None or df.empty: print(\" FEIL: DataFrame tom.\"); return [], [], []\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    print(f\"  Funnet {len(numeric_cols)} num. kolonner.\")\n",
    "\n",
    "    # Define columns to exclude from features\n",
    "    cols_to_exclude = set()\n",
    "\n",
    "    # Add columns explicitly passed for exclusion (target, next_ret, mkt_cap_orig, etc.)\n",
    "    base_exclusions_present = [col for col in base_exclusions if col in df.columns]\n",
    "    cols_to_exclude.update(base_exclusions_present)\n",
    "\n",
    "    # Add standard identifiers and intermediate calculation columns that might *still* be present\n",
    "    # (AdjustedReturn_t is likely still there from the preprocessor script)\n",
    "    date_col = 'Date' # Should be standardized now\n",
    "    id_col = 'Instrument' # Should be standardized now\n",
    "    std_exclusions = [date_col, id_col, 'level_0','index','Year','MonthYear',\n",
    "                      'AdjustedReturn_t', # Keep this exclusion\n",
    "                      'MonthlyReturn_t', 'MonthlyRiskFreeRate_t'] # Keep these\n",
    "    cols_to_exclude.update([col for col in std_exclusions if col in df.columns])\n",
    "\n",
    "    # Add portfolio helper columns if they might exist (unlikely now)\n",
    "    pf_cols=['Rank','DecileRank','Decile','ew_weights','vw_weights','rank']\n",
    "    cols_to_exclude.update([c for c in pf_cols if c in df.columns])\n",
    "\n",
    "    # Exclude original versions of logged variables IF the log version exists\n",
    "    # (This check remains useful, e.g., excludes MarketCap if log_MarketCap exists)\n",
    "    log_cols = {c for c in numeric_cols if c.startswith('log_')}\n",
    "    # List potential raw names based on how log names are created (log_VARNAME)\n",
    "    raw_names_from_logs = {c.replace('log_', '') for c in log_cols}\n",
    "    for raw_name in raw_names_from_logs:\n",
    "        if raw_name in df.columns and f\"log_{raw_name}\" in log_cols:\n",
    "            cols_to_exclude.add(raw_name)\n",
    "            # Specific exclusions if MarketCap or ClosePrice were logged\n",
    "            if raw_name in ['MarketCap', 'ClosePrice'] and 'ClosePrice' in df.columns:\n",
    "                 cols_to_exclude.add('ClosePrice') # Exclude if log exists\n",
    "            if raw_name == 'MarketCap' and 'CommonSharesOutstanding' in df.columns:\n",
    "                 cols_to_exclude.add('CommonSharesOutstanding') # Exclude if log exists\n",
    "\n",
    "    # Exclude raw NorgesBank10Y, NIBOR3M if TermSpread exists\n",
    "    if 'TermSpread' in df.columns:\n",
    "         if 'NorgesBank10Y' in df.columns: cols_to_exclude.add('NorgesBank10Y')\n",
    "         if 'NIBOR3M' in df.columns: cols_to_exclude.add('NIBOR3M')\n",
    "\n",
    "    # Identify final numeric features\n",
    "    potential_features = [c for c in numeric_cols if c not in cols_to_exclude]\n",
    "    final_features = []\n",
    "    for col in potential_features:\n",
    "        if col not in df.columns: continue\n",
    "        valid = df[col].dropna()\n",
    "        # Check for variance and multiple unique values\n",
    "        # Use a slightly larger tolerance for std dev check with float32\n",
    "        if len(valid) > 1 and valid.nunique() > 1 and valid.std() > 1e-7:\n",
    "            final_features.append(col)\n",
    "        # else: print(f\"  -> Dropping potential feature '{col}' due to no variance or <=1 unique value.\")\n",
    "\n",
    "    final_features = sorted(list(set(final_features)))\n",
    "    print(f\"  Identifisert {len(final_features)} features totalt etter ekskludering og validitetssjekk.\")\n",
    "    print(f\"  Features sample: {final_features[:5]}...{final_features[-5:]}\") # Show sample\n",
    "    # print(f\"  Ekskluderte kolonner: {sorted(list(cols_to_exclude.intersection(df.columns)))}\") # Optional Debug\n",
    "\n",
    "    # Check OLS3 features against the *final* feature list\n",
    "    # Use the names specified in config.OLS3_FEATURE_NAMES directly\n",
    "    ols3_features_final = [f for f in ols3_feature_names if f in final_features]\n",
    "    missing_ols3 = [f for f in ols3_feature_names if f not in ols3_features_final]\n",
    "\n",
    "    if missing_ols3: print(f\"  ADVARSEL: OLS3 mangler features fra config: {missing_ols3}\")\n",
    "    if not ols3_features_final: print(\"  ADVARSEL: Ingen av de spesifiserte OLS3 features er gyldige endelige features.\")\n",
    "    elif len(ols3_features_final) < len(ols3_feature_names): print(f\"  ADVARSEL: Kunne finne deler av OLS3 features: {ols3_features_final}\")\n",
    "    else: print(f\"  Valide OLS3 features funnet: {ols3_features_final}\")\n",
    "\n",
    "    # Return all valid features, valid OLS3 features, and all valid features again\n",
    "    all_needed_final = sorted(list(set(final_features)))\n",
    "    return all_needed_final, ols3_features_final, all_needed_final\n",
    "\n",
    "\n",
    "# === Stage 3: Standardization ===\n",
    "# (Keep rank_standardize_features as is - it operates on the defined features)\n",
    "def rank_standardize_features(df, features_to_standardize):\n",
    "    print(\"\\n--- 3. Rank Standardiserer Features ---\");\n",
    "    date_col = 'Date' # Assumes Date column exists and is named 'Date'\n",
    "    if date_col not in df.columns: print(f\"FEIL: Datokolonne ('{date_col}') mangler for standardisering.\"); return df\n",
    "    features=[f for f in features_to_standardize if f in df.columns];\n",
    "    if not features: print(\"  Ingen features funnet å standardisere.\"); return df\n",
    "    print(f\"Standardiserer {len(features)} features...\")\n",
    "    def rank_transform(x):\n",
    "        x_num=pd.to_numeric(x,errors='coerce')\n",
    "        if x_num.isnull().all(): return x_num # Return NaNs if all are NaN\n",
    "        # Rank, converting ranks to [-1, 1] range\n",
    "        r=x_num.rank(pct=True, na_option='keep')\n",
    "        # Fill remaining NaNs (e.g., from single non-NaN value) with 0 AFTER scaling\n",
    "        return (r * 2 - 1).fillna(0)\n",
    "\n",
    "    try:\n",
    "        # Group by Date and apply the rank transform to each feature column\n",
    "        # Using transform should be efficient\n",
    "        df[features] = df.groupby(date_col)[features].transform(rank_transform)\n",
    "    except Exception as e:\n",
    "        print(f\" ADVARSEL under transform (prøver apply): {e}. Standardisering kan være ufullstendig.\");\n",
    "        # Fallback might be needed for complex cases, but transform is preferred\n",
    "        try:\n",
    "            df_s = df.set_index(date_col)\n",
    "            for col in features:\n",
    "                 df_s[col] = df_s.groupby(level=0)[col].apply(rank_transform)\n",
    "            df = df_s.reset_index() # Bring Date back as a column\n",
    "        except Exception as e2:\n",
    "             print(f\" FEIL under apply: {e2}. Standardisering kan være ufullstendig.\"); return df # Return potentially partially processed df\n",
    "\n",
    "    print(\"Rank standardisering fullført.\"); return df\n",
    "\n",
    "\n",
    "# === Stage 4: Data Cleaning (Post-Standardization) ===\n",
    "def clean_data(df, numeric_features_to_impute, essential_cols_for_dropna, mkt_cap_orig_var):\n",
    "    \"\"\"\n",
    "    Cleans data AFTER standardization.\n",
    "    1. Replaces inf with NaN in features.\n",
    "    2. Imputes NaN in FEATURE columns using the overall median.\n",
    "    3. Drops rows with NaN in ESSENTIAL columns (target, IDs, next_ret, mkt_cap_orig).\n",
    "    4. Drops rows with non-positive original market cap.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 4. Renser Data (Post-Standardisering) ---\"); initial_rows=len(df)\n",
    "    features=[f for f in numeric_features_to_impute if f in df.columns];\n",
    "\n",
    "    if features:\n",
    "        # Replace inf values first within feature columns\n",
    "        inf_mask = df[features].isin([np.inf, -np.inf])\n",
    "        if inf_mask.any().any():\n",
    "            inf_cols = df[features].columns[inf_mask.any(axis=0)].tolist()\n",
    "            print(f\"  Erstatter +/-inf med NaN i features: {inf_cols}...\")\n",
    "            df[features] = df[features].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # Impute NaNs in FEATURES with OVERALL MEDIAN (robust to outliers after standardization)\n",
    "        # Calculate medians ONLY for the feature columns that have NaNs\n",
    "        cols_with_nan = df[features].isnull().any()\n",
    "        features_to_impute_now = cols_with_nan[cols_with_nan].index.tolist()\n",
    "\n",
    "        if features_to_impute_now:\n",
    "            print(f\"  Imputerer NaNs i {len(features_to_impute_now)} features med overall median...\")\n",
    "            medians = df[features_to_impute_now].median(skipna=True) # Calculate median for each feature column\n",
    "\n",
    "            # Fill NaNs using the calculated medians\n",
    "            df[features_to_impute_now] = df[features_to_impute_now].fillna(medians)\n",
    "\n",
    "            # If median itself is NaN (e.g., all NaNs in a column), fill remaining NaNs with 0\n",
    "            if medians.isnull().any():\n",
    "                cols_nan_median = medians[medians.isnull()].index.tolist()\n",
    "                print(f\"  ADVARSEL: Median var NaN for features: {cols_nan_median}. Fyller resterende NaNs i disse kolonnene med 0.\")\n",
    "                df[cols_nan_median] = df[cols_nan_median].fillna(0)\n",
    "            print(f\"  NaNs i features imputert.\")\n",
    "        else:\n",
    "            print(\"  Ingen NaNs funnet i features som trenger imputering.\")\n",
    "\n",
    "    # Drop rows with NaNs in ESSENTIAL columns (target, IDs, next return, original market cap)\n",
    "    # These columns should have been created by the preprocessing script.\n",
    "    essentials_present = [c for c in essential_cols_for_dropna if c in df.columns]\n",
    "    if essentials_present:\n",
    "        rows0 = len(df)\n",
    "        df = df.dropna(subset=essentials_present)\n",
    "        dropped_count = rows0 - len(df)\n",
    "        if dropped_count > 0:\n",
    "            print(f\"  Fjernet {dropped_count} rader pga NaN i essensielle kolonner: {essentials_present}.\")\n",
    "        else:\n",
    "            print(f\"  Ingen rader fjernet pga NaN i essensielle kolonner ({essentials_present}).\")\n",
    "    else:\n",
    "         print(f\"  ADVARSEL: Kunne ikke sjekke NaN i essensielle kolonner (mangler): {[c for c in essential_cols_for_dropna if c not in df.columns]}\")\n",
    "\n",
    "\n",
    "    # Drop rows where original market cap is non-positive (already done in preprocessor, but good safety check)\n",
    "    if mkt_cap_orig_var in df.columns:\n",
    "        rows0 = len(df)\n",
    "        df = df[df[mkt_cap_orig_var] > 0]\n",
    "        dropped_count = rows0 - len(df)\n",
    "        if dropped_count > 0:\n",
    "            print(f\"  Fjernet {dropped_count} rader der '{mkt_cap_orig_var}' <= 0 (sikkerhetssjekk).\")\n",
    "    else:\n",
    "        print(f\"ADVARSEL: Kolonne '{mkt_cap_orig_var}' ikke funnet for sjekk av positiv verdi.\")\n",
    "\n",
    "    print(f\"Datarensing ferdig. Form: {df.shape}. Totalt fjernet {initial_rows-len(df)} rader i dette steget.\");\n",
    "    if df.empty: print(\"FEIL: DataFrame er tom etter rensing.\"); return None\n",
    "    return df\n",
    "\n",
    "\n",
    "# === Stage 5: Data Splitting ===\n",
    "# (Keep get_yearly_rolling_splits as is)\n",
    "def get_yearly_rolling_splits(df, initial_train_years, val_years, test_years):\n",
    "    print(\"\\n--- 5. Setter opp Årlige Rullerende Vinduer ---\")\n",
    "    date_col = 'Date' # Assumes standard name\n",
    "    if date_col not in df.columns: raise ValueError(f\"'{date_col}' kolonnen mangler for splitting.\")\n",
    "\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
    "         try: df[date_col] = pd.to_datetime(df[date_col])\n",
    "         except Exception as e: raise ValueError(f\"Kunne ikke konvertere '{date_col}' til datetime: {e}\")\n",
    "\n",
    "    df['Year']=df[date_col].dt.year; unique_years=sorted(df[\"Year\"].unique()); n_years=len(unique_years)\n",
    "    print(f\"Funnet {n_years} unike år i data ({unique_years[0]}-{unique_years[-1]})\")\n",
    "\n",
    "    min_years_needed = initial_train_years + val_years + test_years\n",
    "    if n_years < min_years_needed:\n",
    "        df.drop(columns=['Year'],inplace=True,errors='ignore'); # Clean up temp column\n",
    "        raise ValueError(f\"Ikke nok år ({n_years}) for den spesifiserte splitten (trenger minst {min_years_needed}).\")\n",
    "\n",
    "    first_test_year_idx = initial_train_years + val_years\n",
    "    if first_test_year_idx >= n_years:\n",
    "        df.drop(columns=['Year'],inplace=True,errors='ignore');\n",
    "        raise ValueError(f\"Kombinasjonen av initial_train ({initial_train_years}) og validation ({val_years}) dekker alle ({n_years}) eller flere år. Ingen testår igjen.\")\n",
    "\n",
    "    first_test_year = unique_years[first_test_year_idx]\n",
    "    last_test_start_year = unique_years[n_years - test_years]\n",
    "    num_windows = last_test_start_year - first_test_year + 1\n",
    "\n",
    "    if num_windows <= 0:\n",
    "        df.drop(columns=['Year'],inplace=True,errors='ignore');\n",
    "        raise ValueError(f\"Negativt eller null antall vinduer beregnet ({num_windows}). Sjekk årskonfigurasjon. First test year: {first_test_year}, Last possible test start year: {last_test_start_year}\")\n",
    "\n",
    "    print(f\"Genererer {num_windows} rullerende vinduer.\")\n",
    "    print(f\"  Første vindu testår: {first_test_year} (slutter {first_test_year+test_years-1})\")\n",
    "    print(f\"  Siste vindu testår: {last_test_start_year} (slutter {last_test_start_year+test_years-1})\")\n",
    "\n",
    "    splits_info=[] # Store tuples of (train_idx, val_idx, test_idx, train_dates, val_dates, test_dates)\n",
    "    for i in range(num_windows):\n",
    "        test_start_year = first_test_year + i\n",
    "        test_end_year = test_start_year + test_years - 1\n",
    "        val_end_year = test_start_year - 1\n",
    "        val_start_year = val_end_year - val_years + 1\n",
    "        train_end_year = val_start_year - 1\n",
    "        train_start_year = unique_years[0] # Train from the beginning\n",
    "\n",
    "        train_indices = df[(df['Year'] >= train_start_year) & (df['Year'] <= train_end_year)].index\n",
    "        val_indices = df[(df['Year'] >= val_start_year) & (df['Year'] <= val_end_year)].index\n",
    "        test_indices = df[(df['Year'] >= test_start_year) & (df['Year'] <= test_end_year)].index\n",
    "\n",
    "        train_dates = df.loc[train_indices, date_col].agg(['min','max']) if not train_indices.empty else None\n",
    "        val_dates = df.loc[val_indices, date_col].agg(['min','max']) if not val_indices.empty else None\n",
    "        test_dates = df.loc[test_indices, date_col].agg(['min','max']) if not test_indices.empty else None\n",
    "\n",
    "        splits_info.append((\n",
    "            train_indices, val_indices, test_indices,\n",
    "            train_dates, val_dates, test_dates,\n",
    "            train_start_year, train_end_year, val_start_year, val_end_year, test_start_year, test_end_year\n",
    "        ))\n",
    "\n",
    "    print(\"\\n--- Split Detaljer per Vindu ---\")\n",
    "    for i,split_data in enumerate(splits_info):\n",
    "        tr_idx, v_idx, te_idx, tr_d, v_d, t_d, tr_s, tr_e, v_s, v_e, t_s, t_e = split_data\n",
    "        print(f\"  Vindu {i+1}/{num_windows}:\")\n",
    "        print(f\"    Train: {tr_s}-{tr_e} ({len(tr_idx)} obs) [{tr_d['min'].date() if tr_d is not None else 'N/A'} -> {tr_d['max'].date() if tr_d is not None else 'N/A'}]\")\n",
    "        print(f\"    Val:   {v_s}-{v_e} ({len(v_idx)} obs) [{v_d['min'].date() if v_d is not None else 'N/A'} -> {v_d['max'].date() if v_d is not None else 'N/A'}]\")\n",
    "        print(f\"    Test:  {t_s}-{t_e} ({len(te_idx)} obs) [{t_d['min'].date() if t_d is not None else 'N/A'} -> {t_d['max'].date() if t_d is not None else 'N/A'}]\")\n",
    "        yield tr_idx, v_idx, te_idx, tr_d, v_d, t_d # Yield indices and date ranges\n",
    "\n",
    "    df.drop(columns=['Year'],inplace=True,errors='ignore')\n",
    "\n",
    "\n",
    "# === Stage 6: Model Evaluation Metrics ===\n",
    "# (Keep calculate_oos_r2 and calculate_sharpe_of_predictions as is)\n",
    "def calculate_oos_r2(y_true, y_pred):\n",
    "    \"\"\" Calculates OOS R2 based on Gu, Kelly, Xiu (2020) definition: 1 - SSR/SST0. \"\"\"\n",
    "    if y_true is None or y_pred is None: return np.nan\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    if len(y_true) < 2 or len(y_pred) < 2 or len(y_true) != len(y_pred): return np.nan\n",
    "\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    y_t = y_true[mask]; y_p = y_pred[mask]\n",
    "\n",
    "    if len(y_t) < 2: return np.nan\n",
    "    ss_res = np.sum((y_t - y_p)**2)\n",
    "    ss_tot = np.sum(y_t**2) # SST0\n",
    "\n",
    "    if ss_tot < 1e-15:\n",
    "        return 1.0 if ss_res < 1e-15 else np.nan\n",
    "    return 1.0 - (ss_res / ss_tot)\n",
    "\n",
    "def calculate_sharpe_of_predictions(y_pred, annualization_factor=12):\n",
    "    \"\"\" Calculates annualized Sharpe ratio of the *predictions* themselves. \"\"\"\n",
    "    if y_pred is None: return np.nan\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if len(y_pred) < 2: return np.nan\n",
    "\n",
    "    mask = np.isfinite(y_pred)\n",
    "    y_p = y_pred[mask]\n",
    "    if len(y_p) < 2: return np.nan\n",
    "    mean_pred = np.mean(y_p)\n",
    "    std_pred = np.std(y_p)\n",
    "    if std_pred < 1e-9: return np.nan\n",
    "    return (mean_pred / std_pred) * np.sqrt(annualization_factor)\n",
    "\n",
    "\n",
    "# === Stage 7: Portfolio Analysis ===\n",
    "# (Keep perform_detailed_portfolio_analysis mostly as is, but ensure column names passed are correct)\n",
    "# --> Key change: The `original_df_subset` argument will now be the main `df_clean` DataFrame\n",
    "#     loaded by the pipeline, which contains the preprocessed data including\n",
    "#     NEXT_RETURN_VARIABLE and MARKET_CAP_ORIG_VARIABLE created by `preprocess_data.py`.\n",
    "def MDD(returns):\n",
    "    \"\"\" Calculates Maximum Drawdown from a pandas Series of returns. \"\"\"\n",
    "    returns = pd.Series(returns).fillna(0) # Ensure it's a series and fill NaNs with 0\n",
    "    if returns.empty or len(returns) < 2: return np.nan\n",
    "\n",
    "    cumulative_returns = (1 + returns).cumprod()\n",
    "    peak = cumulative_returns.cummax()\n",
    "    drawdown = (cumulative_returns / peak) - 1\n",
    "    max_drawdown = drawdown.min() # MDD is the minimum value in the drawdown series\n",
    "\n",
    "    return max_drawdown * 100 if pd.notna(max_drawdown) else np.nan\n",
    "\n",
    "def perform_detailed_portfolio_analysis(results_df, # Contains Date, Instrument, yhat_*\n",
    "                                        full_preprocessed_df, # Contains Date, Instrument, Target, NextRawReturn, MktCqpOrig etc.\n",
    "                                        prediction_cols,\n",
    "                                        mkt_cap_orig_var, # Name of the original market cap column\n",
    "                                        next_ret_var,     # Name of the NEXT month's RAW return column\n",
    "                                        # monthly_rf_var, # Not strictly needed if using excess returns from results_df\n",
    "                                        filter_small_caps=False, annualization_factor=12,\n",
    "                                        benchmark_file=None, ff_factor_file=None):\n",
    "    print(\"\\n--- 7. Detaljert Porteføljeanalyse (Desiler) ---\")\n",
    "    if filter_small_caps: print(\">>> Filtrering av små selskaper (basert på market cap ved t) er AKTIVERT for porteføljedanning <<<\")\n",
    "\n",
    "    # --- Input Data Validation ---\n",
    "    date_col = 'Date' # Standard name\n",
    "    id_col = 'Instrument' # Standard name\n",
    "    target_var = config.TARGET_VARIABLE # From config, should match results_df\n",
    "\n",
    "    required_orig = [date_col, id_col, mkt_cap_orig_var, next_ret_var] # Core needs from preprocessed data\n",
    "    missing_orig = [c for c in required_orig if c not in full_preprocessed_df.columns]\n",
    "    if missing_orig: print(f\"FEIL: Mangler påkrevde kolonner i full_preprocessed_df: {missing_orig}.\"); return {},{},{}\n",
    "\n",
    "    required_res = [date_col, id_col, target_var] + prediction_cols\n",
    "    missing_res = [c for c in required_res if c not in results_df.columns]\n",
    "    if missing_res: print(f\"FEIL: Mangler påkrevde kolonner i results_df: {missing_res}.\"); return {},{},{}\n",
    "\n",
    "    # --- Data Preparation & Merging ---\n",
    "    print(\"Forbereder data for porteføljeanalyse...\")\n",
    "    # Ensure correct dtypes and standard columns before merge\n",
    "    results_df[date_col] = pd.to_datetime(results_df[date_col])\n",
    "    results_df[id_col] = results_df[id_col].astype(str)\n",
    "    full_preprocessed_df[date_col] = pd.to_datetime(full_preprocessed_df[date_col])\n",
    "    full_preprocessed_df[id_col] = full_preprocessed_df[id_col].astype(str)\n",
    "\n",
    "    # Select necessary columns from the full preprocessed data\n",
    "    # We need Date, Instrument, the original market cap (t), and the next raw return (t+1)\n",
    "    df_orig_sub = full_preprocessed_df[required_orig].drop_duplicates(subset=[date_col, id_col], keep='first')\n",
    "\n",
    "    # Select necessary columns from the results (predictions at t for portfolio formed at t)\n",
    "    results_sub = results_df[[date_col, id_col] + prediction_cols + [target_var]].drop_duplicates(subset=[date_col, id_col], keep='first')\n",
    "\n",
    "    # Merge predictions (at time t) with market cap (at t) and NEXT month's return (t+1)\n",
    "    portfolio_data = pd.merge(results_sub, df_orig_sub, on=[date_col, id_col], how='inner')\n",
    "    print(f\"Data for analyse etter merge: {portfolio_data.shape}\")\n",
    "\n",
    "    # Rename columns for clarity in portfolio context\n",
    "    # me = market equity (market cap) at time t\n",
    "    # ret_t+1 = raw return realized in month t+1 (obtained from preprocessed data)\n",
    "    # target = target variable (excess return t+1, used for sorting checks maybe, but ret_t+1 used for perf)\n",
    "    portfolio_data = portfolio_data.rename(columns={\n",
    "        mkt_cap_orig_var: 'me',\n",
    "        next_ret_var: 'ret_t+1',\n",
    "        target_var: 'target_ret_t+1' # Rename target for clarity\n",
    "    })\n",
    "\n",
    "    # --- Calculate Excess Returns (t+1) for analysis ---\n",
    "    # We need the risk-free rate corresponding to the ret_t+1 period.\n",
    "    # The easiest way is to get it from the target_ret_t+1 calculation:\n",
    "    # target_ret_t+1 = ret_t+1 - rf_t+1  =>  rf_t+1 = ret_t+1 - target_ret_t+1\n",
    "    if 'ret_t+1' in portfolio_data.columns and 'target_ret_t+1' in portfolio_data.columns:\n",
    "         portfolio_data['rf_t+1'] = portfolio_data['ret_t+1'] - portfolio_data['target_ret_t+1']\n",
    "         # Calculate excess return for t+1 (should be very close to target_ret_t+1, good check)\n",
    "         portfolio_data['excess_ret_t+1'] = portfolio_data['ret_t+1'] - portfolio_data['rf_t+1']\n",
    "         print(\"  Beregnet excess_ret_t+1 for porteføljeanalyse.\")\n",
    "    else:\n",
    "         print(\"FEIL: Kan ikke beregne excess_ret_t+1 - mangler ret_t+1 eller target_ret_t+1.\")\n",
    "         return {}, {}, {}\n",
    "\n",
    "\n",
    "    # --- Data Cleaning Post-Merge ---\n",
    "    # Critical columns needed for decile sorts and return calculations\n",
    "    crit_cols = prediction_cols + ['excess_ret_t+1', 'ret_t+1', 'me'] # rf_t+1 not strictly needed if excess_ret exists\n",
    "    initial_rows = len(portfolio_data)\n",
    "    portfolio_data = portfolio_data.dropna(subset=crit_cols)\n",
    "    # Ensure valid market cap for weighting and potential filtering\n",
    "    portfolio_data = portfolio_data[portfolio_data['me'] > 0]\n",
    "    rows_removed = initial_rows - len(portfolio_data)\n",
    "    if rows_removed > 0:\n",
    "        print(f\"  Fjernet {rows_removed} rader pga NaNs i kritiske kolonner ({crit_cols}) eller me <= 0.\")\n",
    "    if portfolio_data.empty: print(\"FEIL: Ingen gyldige data igjen etter sammenslåing og rensing.\"); return {},{},{}\n",
    "\n",
    "    # Use Month-Year Period for grouping\n",
    "    portfolio_data['MonthYear'] = portfolio_data[date_col].dt.to_period('M')\n",
    "\n",
    "    # --- Decile Sorting and Monthly Returns Calculation ---\n",
    "    # (The rest of this function remains largely the same, as it operates on the prepared portfolio_data)\n",
    "    all_monthly_results = []\n",
    "    monthly_weights_all = [] # To store weights for turnover calculation\n",
    "    model_names_processed = [] # Track models actually processed\n",
    "    hl_monthly_dfs_plotting = {} # Store H-L returns for plotting\n",
    "    long_monthly_dfs_plotting = {} # Store Long-only (D10) returns for plotting\n",
    "\n",
    "    unique_months = sorted(portfolio_data['MonthYear'].unique())\n",
    "    print(f\"Itererer gjennom {len(unique_months)} måneder for desil-sortering...\")\n",
    "\n",
    "    for month in unique_months:\n",
    "        monthly_data_full = portfolio_data[portfolio_data['MonthYear'] == month].copy()\n",
    "\n",
    "        if filter_small_caps:\n",
    "            mc_cutoff = monthly_data_full['me'].quantile(config.SMALL_FIRM_BOTTOM_PERCENT / 100.0)\n",
    "            monthly_data_filtered = monthly_data_full[monthly_data_full['me'] > mc_cutoff].copy()\n",
    "            if monthly_data_filtered.empty and not monthly_data_full.empty: continue\n",
    "            elif len(monthly_data_filtered) < 10: continue\n",
    "            else: monthly_data = monthly_data_filtered\n",
    "        else:\n",
    "            monthly_data = monthly_data_full\n",
    "\n",
    "        if len(monthly_data) < 10: continue\n",
    "\n",
    "        for model_pred_col in prediction_cols:\n",
    "            model_name = model_pred_col.replace('yhat_', '').upper().replace('_', '-')\n",
    "            if model_name not in model_names_processed: model_names_processed.append(model_name)\n",
    "\n",
    "            monthly_data_model = monthly_data.dropna(subset=[model_pred_col]).copy()\n",
    "            if len(monthly_data_model) < 10: continue\n",
    "\n",
    "            monthly_data_model['Rank'] = monthly_data_model[model_pred_col].rank(method='first')\n",
    "            try:\n",
    "                monthly_data_model['Decile'] = pd.qcut(monthly_data_model['Rank'], 10, labels=False, duplicates='drop') + 1\n",
    "            except ValueError: continue\n",
    "\n",
    "            if monthly_data_model['Decile'].nunique() < 2: continue\n",
    "\n",
    "            monthly_data_model['ew_weights'] = 1 / monthly_data_model.groupby('Decile')[id_col].transform('size')\n",
    "            mc_sum_decile = monthly_data_model.groupby('Decile')['me'].transform('sum')\n",
    "            monthly_data_model['vw_weights'] = np.where(mc_sum_decile > 1e-9, monthly_data_model['me'] / mc_sum_decile, 0)\n",
    "\n",
    "            # Use 'excess_ret_t+1' and 'ret_t+1' directly now\n",
    "            monthly_data_model['ew_excess_ret'] = monthly_data_model['excess_ret_t+1'] * monthly_data_model['ew_weights']\n",
    "            monthly_data_model['vw_excess_ret'] = monthly_data_model['excess_ret_t+1'] * monthly_data_model['vw_weights']\n",
    "            monthly_data_model['ew_raw_ret'] = monthly_data_model['ret_t+1'] * monthly_data_model['ew_weights']\n",
    "            monthly_data_model['vw_raw_ret'] = monthly_data_model['ret_t+1'] * monthly_data_model['vw_weights']\n",
    "            monthly_data_model['ew_pred_ret'] = monthly_data_model[model_pred_col] * monthly_data_model['ew_weights']\n",
    "            monthly_data_model['vw_pred_ret'] = monthly_data_model[model_pred_col] * monthly_data_model['vw_weights']\n",
    "\n",
    "            weights_m = monthly_data_model[[id_col, 'Decile', 'ew_weights', 'vw_weights']].copy()\n",
    "            weights_m['Model'] = model_name\n",
    "            weights_m['MonthYear'] = month\n",
    "            monthly_weights_all.append(weights_m)\n",
    "\n",
    "            agg_results = monthly_data_model.groupby('Decile').agg(\n",
    "                ew_excess_ret = ('ew_excess_ret', 'sum'),\n",
    "                vw_excess_ret = ('vw_excess_ret', 'sum'),\n",
    "                ew_raw_ret = ('ew_raw_ret', 'sum'),\n",
    "                vw_raw_ret = ('vw_raw_ret', 'sum'),\n",
    "                ew_pred_ret = ('ew_pred_ret', 'sum'),\n",
    "                vw_pred_ret = ('vw_pred_ret', 'sum'),\n",
    "                n_stocks = (id_col, 'size')\n",
    "            ).reset_index()\n",
    "\n",
    "            agg_results['MonthYear'] = month\n",
    "            agg_results['Model'] = model_name\n",
    "            all_monthly_results.append(agg_results)\n",
    "\n",
    "    if not all_monthly_results: print(\"FEIL: Ingen månedsresultater ble generert.\"); return {},{},{}\n",
    "\n",
    "    # --- Combine Monthly Results and Calculate Turnover ---\n",
    "    # (Turnover calculation logic remains the same)\n",
    "    combined_results_df = pd.concat(all_monthly_results).reset_index(drop=True)\n",
    "    turnover_results = defaultdict(lambda: {'ew': np.nan, 'vw': np.nan, 'long_ew': np.nan, 'long_vw': np.nan})\n",
    "\n",
    "    if monthly_weights_all:\n",
    "        all_weights_df = pd.concat(monthly_weights_all).sort_values(['Model', 'MonthYear', id_col])\n",
    "        print(f\"\\nBeregner porteføljeomsetning (turnover) for {len(model_names_processed)} modeller...\")\n",
    "        for mn in model_names_processed:\n",
    "            model_weights = all_weights_df[all_weights_df['Model'] == mn].copy()\n",
    "            if model_weights.empty: continue\n",
    "\n",
    "            # H-L Turnover\n",
    "            long_weights = model_weights[model_weights['Decile'] == 10]\n",
    "            short_weights = model_weights[model_weights['Decile'] == 1].assign(ew_weights=lambda x: -x.ew_weights, vw_weights=lambda x: -x.vw_weights)\n",
    "            hl_weights = pd.concat([long_weights, short_weights]).sort_values([id_col, 'MonthYear'])\n",
    "            hl_weights['ew_w_next'] = hl_weights.groupby(id_col)['ew_weights'].shift(-1).fillna(0)\n",
    "            hl_weights['vw_w_next'] = hl_weights.groupby(id_col)['vw_weights'].shift(-1).fillna(0)\n",
    "            hl_weights['trade_ew'] = abs(hl_weights['ew_w_next'] - hl_weights['ew_weights'])\n",
    "            hl_weights['trade_vw'] = abs(hl_weights['vw_w_next'] - hl_weights['vw_weights'])\n",
    "            last_month_hl = hl_weights['MonthYear'].max()\n",
    "            monthly_turnover_hl = hl_weights[hl_weights['MonthYear'] != last_month_hl].groupby('MonthYear').agg(sum_trade_ew=('trade_ew', 'sum'), sum_trade_vw=('trade_vw', 'sum'))\n",
    "            if not monthly_turnover_hl.empty:\n",
    "                turnover_results[mn]['ew'] = monthly_turnover_hl['sum_trade_ew'].mean() / 2\n",
    "                turnover_results[mn]['vw'] = monthly_turnover_hl['sum_trade_vw'].mean() / 2\n",
    "\n",
    "            # Long-Only Turnover\n",
    "            long_only_weights = model_weights[model_weights['Decile'] == 10].sort_values([id_col, 'MonthYear'])\n",
    "            if not long_only_weights.empty:\n",
    "                 long_only_weights['ew_w_next'] = long_only_weights.groupby(id_col)['ew_weights'].shift(-1).fillna(0)\n",
    "                 long_only_weights['vw_w_next'] = long_only_weights.groupby(id_col)['vw_weights'].shift(-1).fillna(0)\n",
    "                 long_only_weights['trade_ew'] = abs(long_only_weights['ew_w_next'] - long_only_weights['ew_weights'])\n",
    "                 long_only_weights['trade_vw'] = abs(long_only_weights['vw_w_next'] - long_only_weights['vw_weights'])\n",
    "                 last_month_lo = long_only_weights['MonthYear'].max()\n",
    "                 monthly_turnover_lo = long_only_weights[long_only_weights['MonthYear'] != last_month_lo].groupby('MonthYear').agg(sum_trade_ew=('trade_ew', 'sum'), sum_trade_vw=('trade_vw', 'sum'))\n",
    "                 if not monthly_turnover_lo.empty:\n",
    "                     turnover_results[mn]['long_ew'] = monthly_turnover_lo['sum_trade_ew'].mean() / 2\n",
    "                     turnover_results[mn]['long_vw'] = monthly_turnover_lo['sum_trade_vw'].mean() / 2\n",
    "        print(\"Omsetningsberegning fullført.\")\n",
    "    else: print(\"Advarsel: Ingen vektdata funnet, kan ikke beregne omsetning.\")\n",
    "\n",
    "    # --- Aggregate Performance and Generate Tables/Plots ---\n",
    "    # (Calculation and formatting logic remains the same)\n",
    "    decile_tables = {}\n",
    "    hl_risk_tables = {}\n",
    "    long_risk_tables = {}\n",
    "    performance_summary_list = []\n",
    "\n",
    "    print(f\"\\nGenererer ytelsestabeller for {len(model_names_processed)} modeller...\")\n",
    "    for model_name in model_names_processed:\n",
    "        model_results = combined_results_df[combined_results_df['Model'] == model_name].copy()\n",
    "        if model_results.empty: continue\n",
    "\n",
    "        # Decile Performance\n",
    "        decile_perf = model_results.groupby('Decile').agg(\n",
    "            ew_pred_mean=('ew_pred_ret', 'mean'), vw_pred_mean=('vw_pred_ret', 'mean'),\n",
    "            ew_excess_mean=('ew_excess_ret', 'mean'), vw_excess_mean=('vw_excess_ret', 'mean'),\n",
    "            ew_raw_std=('ew_raw_ret', 'std'), vw_raw_std=('vw_raw_ret', 'std'), # Use raw std for SR\n",
    "            n_months=('MonthYear', 'nunique'), avg_stocks=('n_stocks','mean')\n",
    "        ).reset_index()\n",
    "        decile_perf['ew_sharpe'] = (decile_perf['ew_excess_mean'] / decile_perf['ew_raw_std']) * np.sqrt(annualization_factor)\n",
    "        decile_perf['vw_sharpe'] = (decile_perf['vw_excess_mean'] / decile_perf['vw_raw_std']) * np.sqrt(annualization_factor)\n",
    "\n",
    "        # H-L Performance\n",
    "        hl_stats_df = pd.DataFrame(); hl_monthly = pd.DataFrame()\n",
    "        if 1 in model_results['Decile'].values and 10 in model_results['Decile'].values:\n",
    "            long_monthly = model_results[model_results['Decile'] == 10].set_index('MonthYear')\n",
    "            short_monthly = model_results[model_results['Decile'] == 1].set_index('MonthYear')\n",
    "            common_index = long_monthly.index.intersection(short_monthly.index)\n",
    "            if not common_index.empty:\n",
    "                hl_monthly = pd.DataFrame({\n",
    "                    'ew_excess_ret_HL': long_monthly.loc[common_index, 'ew_excess_ret'].sub(short_monthly.loc[common_index, 'ew_excess_ret'], fill_value=0),\n",
    "                    'vw_excess_ret_HL': long_monthly.loc[common_index, 'vw_excess_ret'].sub(short_monthly.loc[common_index, 'vw_excess_ret'], fill_value=0),\n",
    "                    'ew_raw_ret_HL': long_monthly.loc[common_index, 'ew_raw_ret'].sub(short_monthly.loc[common_index, 'ew_raw_ret'], fill_value=0),\n",
    "                    'vw_raw_ret_HL': long_monthly.loc[common_index, 'vw_raw_ret'].sub(short_monthly.loc[common_index, 'vw_raw_ret'], fill_value=0),\n",
    "                    'ew_pred_ret_HL': long_monthly.loc[common_index, 'ew_pred_ret'].sub(short_monthly.loc[common_index, 'ew_pred_ret'], fill_value=0),\n",
    "                    'vw_pred_ret_HL': long_monthly.loc[common_index, 'vw_pred_ret'].sub(short_monthly.loc[common_index, 'vw_pred_ret'], fill_value=0)\n",
    "                }).reset_index()\n",
    "                hl_monthly_dfs_plotting[model_name] = hl_monthly.copy()\n",
    "\n",
    "                ew_excess_mean_hl = hl_monthly['ew_excess_ret_HL'].mean(); vw_excess_mean_hl = hl_monthly['vw_excess_ret_HL'].mean()\n",
    "                ew_raw_std_hl = hl_monthly['ew_raw_ret_HL'].std(); vw_raw_std_hl = hl_monthly['vw_raw_ret_HL'].std()\n",
    "                ew_sharpe_hl = (ew_excess_mean_hl / ew_raw_std_hl) * np.sqrt(annualization_factor) if ew_raw_std_hl > 1e-9 else np.nan\n",
    "                vw_sharpe_hl = (vw_excess_mean_hl / vw_raw_std_hl) * np.sqrt(annualization_factor) if vw_raw_std_hl > 1e-9 else np.nan\n",
    "                mdd_ew_hl = MDD(hl_monthly['ew_excess_ret_HL']); mdd_vw_hl = MDD(hl_monthly['vw_excess_ret_HL'])\n",
    "                # Factor model placeholders\n",
    "                alpha_ew_hl, tstat_ew_hl, r2_ew_hl = np.nan, np.nan, np.nan\n",
    "                alpha_vw_hl, tstat_vw_hl, r2_vw_hl = np.nan, np.nan, np.nan\n",
    "\n",
    "                hl_stats_df = pd.DataFrame({\n",
    "                    'ew_pred_mean': [hl_monthly['ew_pred_ret_HL'].mean()],'vw_pred_mean': [hl_monthly['vw_pred_ret_HL'].mean()],\n",
    "                    'ew_excess_mean': [ew_excess_mean_hl],'vw_excess_mean': [vw_excess_mean_hl],\n",
    "                    'ew_raw_std': [ew_raw_std_hl],'vw_raw_std': [vw_raw_std_hl],\n",
    "                    'n_months': [len(hl_monthly)], 'ew_sharpe': [ew_sharpe_hl],'vw_sharpe': [vw_sharpe_hl],\n",
    "                    'avg_stocks': [np.nan], 'Decile': ['H-L']\n",
    "                })\n",
    "\n",
    "        model_summary = pd.concat([decile_perf, hl_stats_df], ignore_index=True)\n",
    "        performance_summary_list.append(model_summary)\n",
    "\n",
    "        # Formatting Functions (keep as is)\n",
    "        def format_decile_table(summary_df, weight_scheme):\n",
    "            prefix = 'ew_' if weight_scheme == 'EW' else 'vw_'\n",
    "            cols_map = {f'{prefix}pred_mean': 'Pred', f'{prefix}excess_mean': 'Avg Ex Ret', f'{prefix}raw_std': 'SD (Raw Ret)', f'{prefix}sharpe': 'Ann SR', 'avg_stocks': 'Avg N'}\n",
    "            relevant_cols = [c for c in cols_map if c in summary_df.columns]\n",
    "            if not relevant_cols or 'Decile' not in summary_df.columns: return pd.DataFrame()\n",
    "            sub_df = summary_df[['Decile'] + relevant_cols].rename(columns=cols_map).copy().set_index('Decile')\n",
    "            for col in ['Pred', 'Avg Ex Ret', 'SD (Raw Ret)']:\n",
    "                 if col in sub_df.columns: sub_df[col] = pd.to_numeric(sub_df[col], errors='coerce') * 100\n",
    "            if 'Ann SR' in sub_df.columns: sub_df['Ann SR'] = pd.to_numeric(sub_df['Ann SR'], errors='coerce')\n",
    "            if 'Avg N' in sub_df.columns: sub_df['Avg N'] = pd.to_numeric(sub_df['Avg N'], errors='coerce')\n",
    "            def map_idx(x): return 'Low (L)' if str(x) == '1' else ('High (H)' if str(x) == '10' else str(x))\n",
    "            sub_df.index = sub_df.index.map(map_idx)\n",
    "            desired_order = ['Low (L)','2','3','4','5','6','7','8','9','High (H)','H-L']\n",
    "            sub_df = sub_df.reindex([i for i in desired_order if i in sub_df.index])\n",
    "            final_cols = [c for c in ['Pred', 'Avg Ex Ret', 'SD (Raw Ret)', 'Ann SR', 'Avg N'] if c in sub_df.columns]\n",
    "            sub_df_formatted = sub_df[final_cols].copy()\n",
    "            for col in ['Pred', 'Avg Ex Ret', 'SD (Raw Ret)']:\n",
    "                 if col in sub_df_formatted.columns: sub_df_formatted[col] = sub_df_formatted[col].map('{:.2f}%'.format).replace('nan%','N/A')\n",
    "            if 'Ann SR' in sub_df_formatted.columns: sub_df_formatted['Ann SR'] = sub_df_formatted['Ann SR'].map('{:.2f}'.format).replace('nan','N/A')\n",
    "            if 'Avg N' in sub_df_formatted.columns: sub_df_formatted['Avg N'] = sub_df_formatted['Avg N'].map('{:.0f}'.format).replace('nan','N/A')\n",
    "            return sub_df_formatted[final_cols]\n",
    "\n",
    "        def format_risk_table(data_dict, table_index):\n",
    "             df_risk = pd.DataFrame(data_dict, index=table_index)\n",
    "             for idx in df_risk.index:\n",
    "                  is_percent = '%' in idx; num_decimals = 2 if is_percent else 3; suffix = '%' if is_percent else ''\n",
    "                  try: df_risk.loc[idx] = df_risk.loc[idx].map(f'{{:.{num_decimals}f}}'.format).astype(str) + suffix\n",
    "                  except (ValueError, TypeError): df_risk.loc[idx] = df_risk.loc[idx].apply(lambda x: f'{float(x):.{num_decimals}f}{suffix}' if pd.notna(x) and isinstance(x,(int,float)) else 'N/A')\n",
    "                  df_risk.loc[idx] = df_risk.loc[idx].replace(['nan%', 'nan', ''], 'N/A', regex=False)\n",
    "             return df_risk\n",
    "\n",
    "        # Generate and Print Decile Tables\n",
    "        ew_table = format_decile_table(model_summary, 'EW'); decile_tables[f'{model_name}_EW'] = ew_table\n",
    "        vw_table = format_decile_table(model_summary, 'VW'); decile_tables[f'{model_name}_VW'] = vw_table\n",
    "        print(f\"\\n--- Ytelsestabell (Desiler): {model_name} - EW ---\"); print(ew_table)\n",
    "        print(f\"\\n--- Ytelsestabell (Desiler): {model_name} - VW ---\"); print(vw_table)\n",
    "\n",
    "        # Generate and Print H-L Risk/Performance Table\n",
    "        if not hl_stats_df.empty and not hl_monthly.empty:\n",
    "            hl_res = hl_stats_df.iloc[0]\n",
    "            turnover_ew_hl = turnover_results.get(model_name, {}).get('ew', np.nan)\n",
    "            turnover_vw_hl = turnover_results.get(model_name, {}).get('vw', np.nan)\n",
    "            max_loss_1m_ew_hl = hl_monthly['ew_excess_ret_HL'].min() * 100 if not hl_monthly.empty else np.nan\n",
    "            max_loss_1m_vw_hl = hl_monthly['vw_excess_ret_HL'].min() * 100 if not hl_monthly.empty else np.nan\n",
    "            risk_idx_hl = [\"Mean Excess Return [%]\", 'Std Dev (Raw) [%]', \"Ann. Sharpe Ratio\", \"Max Drawdown (Excess) [%]\", \"Max 1M Loss (Excess) [%]\", \"Avg Monthly Turnover [%]\", \"Factor Model Alpha [%]\", \"t(Alpha)\", \"Factor Model Adj R2\", \"Info Ratio\"]\n",
    "            ew_data_hl = {f'{model_name} H-L EW': [hl_res.get('ew_excess_mean', np.nan) * 100, hl_res.get('ew_raw_std', np.nan) * 100, hl_res.get('ew_sharpe', np.nan), abs(mdd_ew_hl), max_loss_1m_ew_hl, turnover_ew_hl * 100, alpha_ew_hl, tstat_ew_hl, r2_ew_hl, np.nan]}\n",
    "            vw_data_hl = {f'{model_name} H-L VW': [hl_res.get('vw_excess_mean', np.nan) * 100, hl_res.get('vw_raw_std', np.nan) * 100, hl_res.get('vw_sharpe', np.nan), abs(mdd_vw_hl), max_loss_1m_vw_hl, turnover_vw_hl * 100, alpha_vw_hl, tstat_vw_hl, r2_vw_hl, np.nan]}\n",
    "            ew_chart_hl = format_risk_table(ew_data_hl, risk_idx_hl); hl_risk_tables[f'{model_name}_EW'] = ew_chart_hl\n",
    "            vw_chart_hl = format_risk_table(vw_data_hl, risk_idx_hl); hl_risk_tables[f'{model_name}_VW'] = vw_chart_hl\n",
    "            print(f\"\\n--- H-L Portefølje Risk/Performance ({model_name} EW) ---\"); print(ew_chart_hl)\n",
    "            print(f\"\\n--- H-L Portefølje Risk/Performance ({model_name} VW) ---\"); print(vw_chart_hl)\n",
    "\n",
    "        # Generate and Print Long-Only (Decile 10) Risk/Performance Table\n",
    "        long_res_row = decile_perf[decile_perf['Decile'] == 10]\n",
    "        if not long_res_row.empty:\n",
    "            long_res = long_res_row.iloc[0]\n",
    "            long_monthly = model_results[model_results['Decile'] == 10].set_index('MonthYear')\n",
    "            if not long_monthly.empty: long_monthly_dfs_plotting[model_name] = long_monthly.reset_index()\n",
    "            mdd_ew_long = MDD(long_monthly['ew_excess_ret']) if not long_monthly.empty else np.nan\n",
    "            mdd_vw_long = MDD(long_monthly['vw_excess_ret']) if not long_monthly.empty else np.nan\n",
    "            max_loss_1m_ew_long = long_monthly['ew_excess_ret'].min() * 100 if not long_monthly.empty else np.nan\n",
    "            max_loss_1m_vw_long = long_monthly['vw_excess_ret'].min() * 100 if not long_monthly.empty else np.nan\n",
    "            turnover_ew_long = turnover_results.get(model_name, {}).get('long_ew', np.nan)\n",
    "            turnover_vw_long = turnover_results.get(model_name, {}).get('long_vw', np.nan)\n",
    "            alpha_long_ew, tstat_long_ew, r2_long_ew = np.nan, np.nan, np.nan # Factor placeholders\n",
    "            alpha_long_vw, tstat_long_vw, r2_long_vw = np.nan, np.nan, np.nan # Factor placeholders\n",
    "            risk_idx_long = [\"Mean Excess Return [%]\", 'Std Dev (Raw) [%]', \"Ann. Sharpe Ratio\", \"Max Drawdown (Excess) [%]\", \"Max 1M Loss (Excess) [%]\", \"Avg Monthly Turnover [%]\", \"Factor Model Alpha [%]\", \"t(Alpha)\", \"Factor Model Adj R2\", \"Info Ratio\"]\n",
    "            ew_data_long = {f'{model_name} Long EW': [long_res.get('ew_excess_mean', np.nan) * 100, long_res.get('ew_raw_std', np.nan) * 100, long_res.get('ew_sharpe', np.nan), abs(mdd_ew_long), max_loss_1m_ew_long, turnover_ew_long * 100, alpha_long_ew, tstat_long_ew, r2_long_ew, np.nan]}\n",
    "            vw_data_long = {f'{model_name} Long VW': [long_res.get('vw_excess_mean', np.nan) * 100, long_res.get('vw_raw_std', np.nan) * 100, long_res.get('vw_sharpe', np.nan), abs(mdd_vw_long), max_loss_1m_vw_long, turnover_vw_long * 100, alpha_long_vw, tstat_long_vw, r2_long_vw, np.nan]}\n",
    "            ew_chart_long = format_risk_table(ew_data_long, risk_idx_long); long_risk_tables[f'{model_name}_EW'] = ew_chart_long\n",
    "            vw_chart_long = format_risk_table(vw_data_long, risk_idx_long); long_risk_tables[f'{model_name}_VW'] = vw_chart_long\n",
    "            print(f\"\\n--- Long-Only (D10) Risk/Performance ({model_name} EW) ---\"); print(ew_chart_long)\n",
    "            print(f\"\\n--- Long-Only (D10) Risk/Performance ({model_name} VW) ---\"); print(vw_chart_long)\n",
    "        else: print(f\"  Advarsel: Ingen data for Desil 10 funnet for modell {model_name}.\")\n",
    "\n",
    "    # Plotting Cumulative Returns (keep as is)\n",
    "    fig_hl, ax_hl = plt.subplots(figsize=(14, 7)); plotted_hl = 0\n",
    "    sorted_models_hl = sorted(hl_monthly_dfs_plotting.keys())\n",
    "    for model_name in sorted_models_hl:\n",
    "        df_hl = hl_monthly_dfs_plotting[model_name]\n",
    "        if 'MonthYear' in df_hl.columns and not df_hl.empty:\n",
    "             df_hl['PlotDate'] = df_hl['MonthYear'].dt.to_timestamp() if pd.api.types.is_period_dtype(df_hl['MonthYear']) else pd.to_datetime(df_hl['MonthYear'])\n",
    "             df_hl = df_hl.set_index('PlotDate').sort_index()\n",
    "             if 'ew_excess_ret_HL' in df_hl.columns:\n",
    "                  ret_ew = df_hl['ew_excess_ret_HL'].dropna()\n",
    "                  if not ret_ew.empty: (1 + ret_ew).cumprod().plot(ax=ax_hl, label=f'{model_name} H-L EW'); plotted_hl += 1\n",
    "             if 'vw_excess_ret_HL' in df_hl.columns:\n",
    "                  ret_vw = df_hl['vw_excess_ret_HL'].dropna()\n",
    "                  if not ret_vw.empty: (1 + ret_vw).cumprod().plot(ax=ax_hl, label=f'{model_name} H-L VW', linestyle='--'); plotted_hl += 1\n",
    "    if plotted_hl > 0:\n",
    "        ax_hl.set_title('Kumulativ Excess Avkastning (H-L Portefølje, t+1)'); ax_hl.set_ylabel('Kumulativ Verdi (Log Skala)'); ax_hl.set_xlabel('Dato'); ax_hl.set_yscale('log'); ax_hl.legend(loc='center left', bbox_to_anchor=(1, 0.5)); ax_hl.grid(True, which='both', linestyle='--', linewidth=0.5); fig_hl.tight_layout(rect=[0, 0, 0.85, 1]); plt.show()\n",
    "    else: plt.close(fig_hl)\n",
    "\n",
    "    fig_long, ax_long = plt.subplots(figsize=(14, 7)); plotted_long = 0\n",
    "    sorted_models_long = sorted(long_monthly_dfs_plotting.keys())\n",
    "    for model_name in sorted_models_long:\n",
    "        df_long = long_monthly_dfs_plotting[model_name]\n",
    "        if 'MonthYear' in df_long.columns and not df_long.empty:\n",
    "             df_long['PlotDate'] = df_long['MonthYear'].dt.to_timestamp() if pd.api.types.is_period_dtype(df_long['MonthYear']) else pd.to_datetime(df_long['MonthYear'])\n",
    "             df_long = df_long.set_index('PlotDate').sort_index()\n",
    "             if 'ew_excess_ret' in df_long.columns:\n",
    "                  ret_ew = df_long['ew_excess_ret'].dropna()\n",
    "                  if not ret_ew.empty: (1 + ret_ew).cumprod().plot(ax=ax_long, label=f'{model_name} Long EW'); plotted_long += 1\n",
    "             if 'vw_excess_ret' in df_long.columns:\n",
    "                  ret_vw = df_long['vw_excess_ret'].dropna()\n",
    "                  if not ret_vw.empty: (1 + ret_vw).cumprod().plot(ax=ax_long, label=f'{model_name} Long VW', linestyle='--'); plotted_long += 1\n",
    "    if plotted_long > 0:\n",
    "        ax_long.set_title('Kumulativ Excess Avkastning (Long-Only Portefølje [D10], t+1)'); ax_long.set_ylabel('Kumulativ Verdi (Log Skala)'); ax_long.set_xlabel('Dato'); ax_long.set_yscale('log'); ax_long.legend(loc='center left', bbox_to_anchor=(1, 0.5)); ax_long.grid(True, which='both', linestyle='--', linewidth=0.5); fig_long.tight_layout(rect=[0, 0, 0.85, 1]); plt.show()\n",
    "    else: plt.close(fig_long)\n",
    "\n",
    "    print(\"--- Detaljert Porteføljeanalyse Fullført ---\")\n",
    "    return decile_tables, hl_risk_tables, long_risk_tables\n",
    "\n",
    "\n",
    "# === Stage 8: Variable Importance ===\n",
    "# (Keep calculate_variable_importance as is)\n",
    "def calculate_variable_importance(model_name, fitted_model, X_eval, y_eval, features, base_r2_is, vi_method='permutation_zero', model_params=None):\n",
    "    start_vi = time.time()\n",
    "    if vi_method != 'permutation_zero': print(f\"    FEIL: VI metode '{vi_method}' støttes ikke.\"); return pd.DataFrame()\n",
    "    if fitted_model is None: print(f\"    FEIL: Ingen modell for VI.\"); return pd.DataFrame()\n",
    "    if pd.isna(base_r2_is): print(f\"    ADVARSEL: Basis IS R2 NaN.\"); return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
    "    if len(features)==0 or X_eval.shape[0]==0 or y_eval.shape[0]==0 or X_eval.shape[1]!=len(features): print(f\"    FEIL: Ugyldige VI data dim.\"); return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
    "\n",
    "    importance_results = {}\n",
    "    y_eval_finite = y_eval[np.isfinite(y_eval)]\n",
    "    ss_tot_zero = np.sum(y_eval_finite**2)\n",
    "    if ss_tot_zero < 1e-15: return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
    "\n",
    "    params_retrain = model_params if model_params else {}\n",
    "    if model_name == 'ENET' and hasattr(fitted_model, 'alpha_'): params_retrain = {'alpha': fitted_model.alpha_, 'l1_ratio': fitted_model.l1_ratio_, **config.MODEL_PARAMS.get('ENET', {})}\n",
    "    elif model_name == 'PLS' and hasattr(fitted_model, 'n_components'): params_retrain = {'n_components': fitted_model.n_components, 'scale': False}\n",
    "    elif model_name == 'PCR' and hasattr(fitted_model, 'named_steps'):\n",
    "        try: params_retrain = {'n_components': fitted_model.named_steps['pca'].n_components_}\n",
    "        except KeyError: params_retrain = {'n_components': 1}\n",
    "    elif model_name in ['GLM_H', 'RF', 'GBRT_H'] and hasattr(fitted_model, 'get_params'):\n",
    "        params_retrain = fitted_model.get_params()\n",
    "        if model_params: params_retrain.update(model_params) # Use optimal params if provided\n",
    "    elif model_name == 'OLS3H': params_retrain = {k: v for k, v in config.MODEL_PARAMS.get('OLS3H', {}).items() if k != 'M'}\n",
    "\n",
    "    for idx, feat_name in enumerate(features):\n",
    "        X_permuted = X_eval.copy(); X_permuted[:, idx] = 0\n",
    "        permuted_model = None; permuted_preds = None; permuted_r2 = -np.inf\n",
    "        try:\n",
    "            if model_name == 'OLS': permuted_model = LinearRegression(fit_intercept=True).fit(X_permuted, y_eval)\n",
    "            elif model_name == 'OLS3H' and sm:\n",
    "                 X_perm_c = sm.add_constant(X_permuted)\n",
    "                 permuted_model_rlm = sm.RLM(y_eval, X_perm_c, M=sm.robust.norms.HuberT())\n",
    "                 permuted_model = permuted_model_rlm.fit(**params_retrain)\n",
    "                 permuted_preds = permuted_model.predict(X_perm_c)\n",
    "            elif model_name == 'PLS': permuted_model = PLSRegression(**params_retrain).fit(X_permuted, y_eval)\n",
    "            elif model_name == 'PCR': permuted_model = Pipeline([('pca', PCA(n_components=params_retrain.get('n_components', 1))), ('lr', LinearRegression())]).fit(X_permuted, y_eval)\n",
    "            elif model_name == 'ENET': permuted_model = ElasticNet(**params_retrain, fit_intercept=True).fit(X_permuted, y_eval)\n",
    "            elif model_name == 'GLM_H': permuted_model = HuberRegressor(**params_retrain, fit_intercept=True).fit(X_permuted, y_eval)\n",
    "            elif model_name == 'RF': permuted_model = RandomForestRegressor(**params_retrain).fit(X_permuted, y_eval)\n",
    "            elif model_name == 'GBRT_H': permuted_model = GradientBoostingRegressor(**params_retrain).fit(X_permuted, y_eval)\n",
    "\n",
    "            if permuted_model and model_name != 'OLS3H': permuted_preds = permuted_model.predict(X_permuted).flatten()\n",
    "            if permuted_preds is not None:\n",
    "                preds_finite = permuted_preds[np.isfinite(y_eval)]\n",
    "                if len(preds_finite) == len(y_eval_finite) and np.all(np.isfinite(preds_finite)):\n",
    "                    ss_res_permuted = np.sum((y_eval_finite - preds_finite)**2)\n",
    "                    permuted_r2 = 1.0 - (ss_res_permuted / ss_tot_zero)\n",
    "        except Exception as e: print(f\"    ADVARSEL: Unntak VI '{feat_name}' i {model_name}: {e}\")\n",
    "        reduction = base_r2_is - permuted_r2\n",
    "        importance_results[feat_name] = max(0, reduction) if pd.notna(reduction) else 0.0\n",
    "\n",
    "    if not importance_results: return pd.DataFrame({'Feature': features, 'Importance': 0.0})\n",
    "    importance_df = pd.DataFrame(importance_results.items(), columns=['Feature', 'R2_Reduction'])\n",
    "    total_reduction = importance_df['R2_Reduction'].sum()\n",
    "    importance_df['Importance'] = importance_df['R2_Reduction'] / total_reduction if total_reduction > 1e-9 else 0.0\n",
    "    return importance_df[['Feature', 'Importance']]\n",
    "\n",
    "\n",
    "# === Stage 9: Complexity Plotting ===\n",
    "# (Keep plot_time_varying_complexity as is)\n",
    "def plot_time_varying_complexity(model_metrics, complexity_params_to_plot):\n",
    "     print(\"\\n--- 9. Plotter Tidsvarierende Modellkompleksitet ---\")\n",
    "     plotted_any = False\n",
    "     for model_name, param_keys in complexity_params_to_plot.items():\n",
    "         if model_name not in model_metrics: continue\n",
    "         print(f\"\\n  --- Modell: {model_name} ---\")\n",
    "         model_data = model_metrics[model_name]\n",
    "         for param_key in param_keys:\n",
    "             if param_key in model_data:\n",
    "                 values = model_data[param_key]\n",
    "                 valid_data = [(i + 1, v) for i, v in enumerate(values) if v is not None and pd.notna(v)]\n",
    "                 if valid_data:\n",
    "                     plotted_any = True\n",
    "                     windows, param_values = zip(*valid_data)\n",
    "                     param_label = param_key.replace('optim_', '').replace('_', ' ').title()\n",
    "                     data_table = pd.DataFrame({param_label: param_values}, index=pd.Index(windows, name='Vindu Nr.'))\n",
    "                     print(f\"    Optimal {param_label} per Vindu:\"); print(data_table.round(4))\n",
    "                     plt.figure(figsize=(10, 5))\n",
    "                     plt.plot(windows, param_values, marker='o', linestyle='-')\n",
    "                     plot_title = f\"Tidsvariasjon i Optimal {param_label} for {model_name}\"\n",
    "                     y_axis_label = f\"Optimal {param_label}\"\n",
    "                     if 'alpha' in param_key.lower() or 'lambda' in param_key.lower():\n",
    "                          if all(v > 1e-9 for v in param_values): plt.yscale('log'); y_axis_label += \" (Log Skala)\"\n",
    "                          else: print(f\"    Advarsel: Kan ikke bruke log-skala for {param_label}.\")\n",
    "                     plt.xlabel(\"Vindu Nr.\"); plt.ylabel(y_axis_label); plt.title(plot_title); plt.grid(True, which='both', linestyle='--', linewidth=0.5); plt.tight_layout(); plt.show()\n",
    "                 else: print(f\"    Ingen gyldige verdier funnet for '{param_label}' for {model_name}.\")\n",
    "             else: print(f\"    Metrikk '{param_key}' ikke funnet for {model_name}.\")\n",
    "     if not plotted_any: print(\"  Ingen kompleksitetsparametere å plotte.\")\n",
    "\n",
    "\n",
    "# === Stage 10: Reporting & Saving ===\n",
    "# (Keep create_summary_table and save_results as is)\n",
    "def create_summary_table(model_metrics, annualization_factor=12):\n",
    "    print(\"\\n--- 10a. Lager Oppsummerende Resultattabell ---\")\n",
    "    summary_data = []\n",
    "    model_order = ['OLS','OLS3H','PLS','PCR','ENET','GLM_H','RF','GBRT_H','NN1','NN2','NN3','NN4','NN5']\n",
    "    models_in_results = list(model_metrics.keys())\n",
    "    models_sorted = [m for m in model_order if m in models_in_results] + \\\n",
    "                    [m for m in sorted(models_in_results) if m not in model_order]\n",
    "    if not models_sorted: print(\"Ingen modelldata funnet.\"); return pd.DataFrame()\n",
    "\n",
    "    for model_name in models_sorted:\n",
    "        metrics = model_metrics[model_name]\n",
    "        avg_is_r2 = np.nanmean(metrics.get('is_r2_train_val', [])) if metrics.get('is_r2_train_val') else np.nan\n",
    "        avg_oos_r2 = np.nanmean(metrics.get('oos_r2', [])) if metrics.get('oos_r2') else np.nan\n",
    "        avg_oos_sharpe = np.nanmean(metrics.get('oos_sharpe', [])) if metrics.get('oos_sharpe') else np.nan\n",
    "        overall_oos_r2_gu = metrics.get('oos_r2_overall_gu', np.nan)\n",
    "        avg_optim_params_str = \"\"\n",
    "        optim_parts = []\n",
    "        for k, v in metrics.items():\n",
    "            if k.startswith('optim_') and v:\n",
    "                 numeric_v = [item for item in v if isinstance(item, (int, float, np.number)) and pd.notna(item)]\n",
    "                 if numeric_v:\n",
    "                     try:\n",
    "                          mean_val = np.nanmean(numeric_v)\n",
    "                          if not np.isnan(mean_val): optim_parts.append(f\"{k.replace('optim_', '')}={mean_val:.2g}\")\n",
    "                     except Exception as e_mean: print(f\"  Advarsel: mean calc error for {k} {model_name}: {e_mean}.\")\n",
    "        avg_optim_params_str = \", \".join(optim_parts)\n",
    "\n",
    "        summary_data.append({\n",
    "            'Modell': model_name,\n",
    "            'Avg IS R² (%)': avg_is_r2 * 100 if pd.notna(avg_is_r2) else np.nan,\n",
    "            'Avg Window OOS R² (%)': avg_oos_r2 * 100 if pd.notna(avg_oos_r2) else np.nan,\n",
    "            'Overall OOS R² (%)': overall_oos_r2_gu * 100 if pd.notna(overall_oos_r2_gu) else np.nan,\n",
    "            'Avg Pred Sharpe (OOS)': avg_oos_sharpe if pd.notna(avg_oos_sharpe) else np.nan,\n",
    "            'Avg Optim Params': avg_optim_params_str\n",
    "        })\n",
    "    if not summary_data: print(\"Ingen data å inkludere.\"); return pd.DataFrame()\n",
    "    summary_df = pd.DataFrame(summary_data).set_index('Modell')\n",
    "    print(\"\\n--- Oppsummeringstabell ---\")\n",
    "    print(summary_df.to_string(float_format=lambda x: f\"{x:.3f}\" if pd.notna(x) else \"N/A\", na_rep=\"N/A\"))\n",
    "    return summary_df\n",
    "\n",
    "def save_results(output_dir, subset_label, results_dict):\n",
    "    print(f\"\\n--- 10b. Lagrer Resultater for Subset: {subset_label} ---\")\n",
    "    subset_dir = os.path.join(output_dir, subset_label)\n",
    "    try:\n",
    "        if not os.path.exists(subset_dir): os.makedirs(subset_dir); print(f\"  Opprettet mappe: {subset_dir}\")\n",
    "    except OSError as e: print(f\"  FEIL: Kunne ikke opprette mappe {subset_dir}: {e}\"); return\n",
    "\n",
    "    for name, data in results_dict.items():\n",
    "        base_filename = os.path.join(subset_dir, f\"{name}\")\n",
    "        try:\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                if not data.empty: filename = f\"{base_filename}.csv\"; data.to_csv(filename); print(f\"  -> Lagret DataFrame: {filename}\")\n",
    "                else: print(f\"  -> Hoppet over tom DataFrame: {name}\")\n",
    "            elif isinstance(data, dict):\n",
    "                saved_sub = False\n",
    "                for sub_name, sub_data in data.items():\n",
    "                     if isinstance(sub_data, pd.DataFrame):\n",
    "                         if not sub_data.empty: sub_filename = f\"{base_filename}_{sub_name}.csv\"; sub_data.to_csv(sub_filename); print(f\"  -> Lagret Dict->DataFrame: {sub_filename}\"); saved_sub = True\n",
    "                     elif isinstance(sub_data, dict): print(f\"  -> Hoppet over Dict->Dict: {name}_{sub_name}\")\n",
    "                     else: print(f\"  -> Hoppet over ukjent datatype i dict: {name}_{sub_name} (Type: {type(sub_data)})\")\n",
    "            else: print(f\"  -> Hoppet over ukjent datatype: {name} (Type: {type(data)})\")\n",
    "        except Exception as e: print(f\"  FEIL under lagring av '{name}' til '{subset_dir}': {e}\"); traceback.print_exc(limit=1)\n",
    "    print(f\"--- Lagring for {subset_label} fullført (eller forsøkt) ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
